{
"action": "write_file",
"path": "llm/adapters/llamacpp.py",
"content": "'''\nllamacpp.py\n\nPurpose:\nProvides a concrete implementation of the Engine interface that wraps the\nllama.cpp command-line tool (llama-cli).\n\nResponsibilities:\n - Construct valid llama-cli command-line arguments from GenOptions.\n - Execute the llama-cli binary as a subprocess.\n - Pass prompts to stdin and capture stdout/stderr.\n - Parse the generated text from the stdout stream.\n - Handle errors from the subprocess.\n'''\n\nimport subprocess\nimport logging\nfrom typing import List\n\nfrom ..llm.engine import Engine, CompletionRequest, GenOptions\n\n\nclass LlamaCppEngine(Engine):\n    '''\n    An Engine implementation that shells out to a llama.cpp binary.\n    '''\n\n    def __init__(self, llama_cli_path: str, model_path: str):\n        '''\n        Initializes the LlamaCppEngine.\n\n        Args:\n            llama_cli_path: The file path to the compiled `llama-cli` executable.\n            model_path: The file path to the .gguf model file.\n        '''\n        self.llama_cli_path = llama_cli_path\n        self.model_path = model_path\n        logging.info(f\"LlamaCppEngine initialized for model: {model_path}\")\n\n    def complete(self, req: CompletionRequest, gen: GenOptions) -> str:\n        '''\n        Generates a completion by calling the llama-cli executable.\n\n        Args:\n            req: The CompletionRequest containing system and user prompts.\n            gen: The GenOptions containing generation parameters.\n\n        Returns:\n            The generated text completion as a string.\n\n        Raises:\n            RuntimeError: If the llama-cli process returns a non-zero exit code.\n        '''\n        command: List[str] = [\n            self.llama_cli_path,\n            '-m', self.model_path,\n            '--ctx-size', str(gen.ctx),\n            '--n-predict', str(gen.max_tokens),\n            '--temp', str(gen.temp),\n            '--top-p', str(gen.top_p),\n            '--repeat-penalty', str(gen.repeat_penalty),\n            '--n-gpu-layers', '-1',  # Max GPU offload\n            '--main-gpu', '0',\n            '--no-display-prompt', # Suppress prompt from appearing in the output\n        ]\n        \n        if req.stop:\n            for stop_word in req.stop:\n                command.extend(['--stop', stop_word])\n\n        # Llama.cpp's server/main tools expect a specific prompt format.\n        # This can be adjusted based on the model's fine-tuning.\n        prompt = f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{req.system}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n{req.user}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n\n        try:\n            logging.debug(f\"Running llama-cli command: {' '.join(command)}\")\n            process = subprocess.run(\n                command,\n                input=prompt,\n                capture_output=True,\n                text=True,\n                check=True, # Will raise CalledProcessError on non-zero exit codes\n                timeout=120 # 2-minute timeout for generation\n            )\n            \n            completion = process.stdout.strip()\n            logging.debug(f\"llama-cli successful. Output length: {len(completion)}\")\n            return completion\n\n        except subprocess.CalledProcessError as e:\n            logging.error(f\"llama-cli failed with exit code {e.returncode}.\")\n            logging.error(f\"stderr: {e.stderr}\")\n            raise RuntimeError(f\"Llama.cpp execution failed: {e.stderr}\") from e\n        except subprocess.TimeoutExpired as e:\n            logging.error(\"llama-cli process timed out.\")\n            raise RuntimeError(\"Llama.cpp execution timed out.\") from e\n"
}
