{
"action": "write_file",
"path": "self_critic.py",
"content": """"\nSelf-critique scaffold for the AGI Modular Mind Loop.\n\nBuilder Prompt (for Codex/GPT):\nYou are builder. Implement a pure-Python self-critique module that inspects\n(task, plan, outputs, metrics) and emits a structured CritiqueReport.\nNo network, no I/O. Favor deterministic heuristics over LLM calls.\nProvide APIs:\n - SelfCritic.critique(task: dict, outputs: dict, plan: Plan|None=None,\n expectations: dict|None=None, metrics: dict|None=None) -> CritiqueReport\n - SelfCritic.generate_improvements(report: CritiqueReport) -> list[dict]\n - summarize(report: CritiqueReport) -> str # single-line memory snippet\nHeuristics: detect errors via common substrings, check plan step statuses,\ncompare metrics against thresholds in expectations, compute score in [0,1],\nverdict in {PASS, WARN, FAIL}. Keep small and testable.\n"""\nfrom future import annotations\n\nfrom dataclasses import dataclass, field, asdict\nfrom enum import Enum, auto\nfrom typing import Any, Dict, List, Optional\nfrom datetime import datetime\n\n# Optional type-only import to avoid hard dependency.\ntry: # pragma: no cover\n from planner import Plan, PlanStatus, StepStatus # type: ignore\nexcept Exception: # pragma: no cover\n Plan = Any\n PlanStatus = Any\n StepStatus = Any\n\n__all__ = [\n "Severity",\n "Verdict",\n "CritIssue",\n "CritiqueReport",\n "SelfCritic",\n "summarize",\n]\n\n\ndef _now_iso() -> str:\n return datetime.utcnow().isoformat()\n\n\nclass Severity(Enum):\n LOW = auto()\n MEDIUM = auto()\n HIGH = auto()\n\n\nclass Verdict(Enum):\n PASS = auto()\n WARN = auto()\n FAIL = auto()\n\n\n@dataclass(frozen=True)\nclass CritIssue:\n code: str\n message: str\n severity: Severity = Severity.MEDIUM\n meta: Dict[str, Any] = field(default_factory=dict)\n\n\n@dataclass(frozen=True)\nclass CritiqueReport:\n task_id: Optional[str]\n score: float # 0..1\n verdict: Verdict\n issues: List[CritIssue] = field(default_factory=list)\n suggestions: List[str] = field(default_factory=list)\n metrics: Dict[str, Any] = field(default_factory=dict)\n created_at: str = field(default_factory=now_iso)\n meta: Dict[str, Any] = field(default_factory=dict)\n\n def to_dict(self) -> Dict[str, Any]:\n d = asdict(self)\n d["verdict"] = self.verdict.name\n d["issues"] = [asdict(i) | {"severity": i.severity.name} for i in self.issues]\n return d\n\n\nclass SelfCritic:\n """Deterministic self-critique with simple heuristics.\n\n Inputs:\n task: free dict, should contain {id, title, expectation thresholds}\n outputs: dict with execution outputs {stdout, stderr, result}\n plan: optional Plan to read step statuses\n expectations: optional thresholds {latency_ms, accuracy>=, tokens<=, ...}\n metrics: optional actuals {latency_ms, accuracy, tokens, ...}\n """\n\n ERROR_KEYS = (\n "error",\n "exception",\n "traceback",\n "failed",\n "denied",\n "not found",\n "timeout",\n )\n\n def critique(\n self,\n task: Dict[str, Any],\n outputs: Dict[str, Any],\n plan: Optional[Plan] = None,\n expectations: Optional[Dict[str, Any]] = None,\n metrics: Optional[Dict[str, Any]] = None,\n ) -> CritiqueReport:\n issues: List[CritIssue] = []\n suggestions: List[str] = []\n score = 1.0\n\n # 1) Output error scan\n text_blob = "\n".join(\n str(outputs.get(k, "")) for k in ("stderr", "stdout", "result", "text")\n ).lower()\n for key in self.ERROR_KEYS:\n if key in text_blob:\n sev = Severity.HIGH if key in ("exception", "traceback", "failed") else Severity.MEDIUM\n issues.append(CritIssue(code=f"OUTPUT{key.upper()}", message=f"Detected '{key}' in outputs", severity=sev))\n if issues:\n score -= 0.4\n\n # 2) Plan status check\n if plan is not None:\n try:\n failed = [s for s in plan.steps if getattr(s, "status", None).name == "FAILED"]\n pending = [s for s in plan.steps if getattr(s, "status", None).name in {"PENDING", "READY"}]\n except Exception:\n failed, pending = [], []\n if failed:\n issues.append(CritIssue(code="PLAN_FAILED_STEPS", message=f"{len(failed)} step(s) failed", severity=Severity.HIGH, meta={"step_ids": [s.id for s in failed]}))\n score -= 0.4\n if pending:\n issues.append(CritIssue(code="PLAN_PENDING_STEPS", message=f"{len(pending)} step(s) still pending", severity=Severity.MEDIUM))\n score -= 0.1\n\n # 3) Expectations vs metrics\n expectations = expectations or {}\n metrics = metrics or {}\n for k, threshold in expectations.items():\n actual = metrics.get(k)\n if actual is None:\n continue\n # If expectation endswith max treat as <=, if min treat as >=, else numeric >= by default\n if k.endswith("max"):\n if actual > threshold:\n issues.append(CritIssue(code=f"EXP{k}", message=f"{k} {actual} > {threshold}", severity=Severity.MEDIUM))\n score -= 0.1\n elif k.endswith("min"):\n if actual < threshold:\n issues.append(CritIssue(code=f"EXP{k}", message=f"{k} {actual} < {threshold}", severity=Severity.MEDIUM))\n score -= 0.1\n else:\n # Default: accuracy-like, want >= threshold\n try:\n if float(actual) < float(threshold):\n issues.append(CritIssue(code=f"EXP{k}", message=f"{k} {actual} < {threshold}", severity=Severity.MEDIUM))\n score -= 0.1\n except Exception:\n # Non-numeric. Skip.\n pass\n\n # Clamp score\n score = max(0.0, min(1.0, score))\n\n # 4) Suggestions\n if any(i.severity is Severity.HIGH for i in issues):\n suggestions.append("add_test: reproduce failure with minimal inputs")\n suggestions.append("increase_guardrails: validate inputs and results")\n if any(i.code.startswith("OUTPUT") for i in issues):\n suggestions.append("inspect_logs: capture stderr and stack traces")\n if any(i.code == "PLAN_PENDING_STEPS" for i in issues):\n suggestions.append("reschedule_steps: unblock dependencies or reorder plan")\n if expectations:\n suggestions.append("tune_thresholds: align expectations with observed variance")\n if not suggestions and score < 1.0:\n suggestions.append("light_refactor: simplify control flow to reduce risk")\n\n verdict = (\n Verdict.FAIL if score < 0.45 else\n Verdict.WARN if score < 0.85 else\n Verdict.PASS\n )\n\n report = CritiqueReport(\n task_id=str(task.get("id")) if task and task.get("id") is not None else None,\n score=score,\n verdict=verdict,\n issues=issues,\n suggestions=suggestions,\n metrics=dict(metrics),\n meta={"title": task.get("title")} if task else {},\n )\n return report\n\n def generate_improvements(self, report: CritiqueReport) -> List[Dict[str, Any]]:\n """Map suggestions to structured actions. Pure mapping."""\n actions: List[Dict[str, Any]] = []\n for s in report.suggestions:\n if s.startswith("add_test"):\n actions.append({"type": "add_test", "priority": 1, "details": "Create failing unit test for reproduced error."})\n elif s.startswith("increase_guardrails"):\n actions.append({"type": "guardrails", "priority": 1, "details": "Add input/output schema checks."})\n elif s.startswith("inspect_logs"):\n actions.append({"type": "logging", "priority": 2, "details": "Persist stderr and tracebacks to debug log."})\n elif s.startswith("reschedule_steps"):\n actions.append({"type": "plan_adjust", "priority": 2, "details": "Reorder or split blocked steps."})\n elif s.startswith("tune_thresholds"):\n actions.append({"type": "tuning", "priority": 3, "details": "Update expectations to match empirical distribution."})\n elif s.startswith("light_refactor"):\n actions.append({"type": "refactor", "priority": 3, "details": "Simplify branching and reduce side effects."})\n # Sort by priority small->large\n actions.sort(key=lambda a: a.get("priority", 9))\n return actions\n\n\ndef summarize(report: CritiqueReport) -> str:\n """Single-line memory snippet for storage."""\n return (\n f"crit[{report.verdict.name}] score={report.score:.2f} "\n f"issues={len(report.issues)} title={report.meta.get('title','-')}"\n )\n\n\nif name == "main":\n # Smoke test\n critic = SelfCritic()\n task = {"id": "t1", "title": "Compile and run"}\n outputs = {"stdout": "ok", "stderr": "", "result": "done"}\n rep = critic.critique(task, outputs, expectations={"accuracy": 0.9}, metrics={"accuracy": 0.95})\n assert rep.verdict in {Verdict.PASS, Verdict.WARN, Verdict.FAIL}\n acts = critic.generate_improvements(rep)\n print("ok", rep.score >= 0.0, len(acts) >= 0)\n"
}
