{
"action": "write_file",
"path": "llm/adapters/gpt4all.py",
"content": "'''\ngpt4all.py\n\nPurpose:\nProvides a concrete implementation of the Engine interface that wraps the\ngpt4all Python library.\n\nResponsibilities:\n - Initialize the GPT4All model object.\n - Format prompts for the model.\n - Call the `generate` method with the correct parameters from GenOptions.\n - Handle errors during model generation.\n'''\n\nimport logging\nfrom pathlib import Path\n\n# The gpt4all library must be installed: pip install gpt4all\ntry:\n    from gpt4all import GPT4All\nexcept ImportError:\n    # This allows the module to be imported for type checking even if gpt4all is not installed.\n    GPT4All = None\n\nfrom ..llm.engine import Engine, CompletionRequest, GenOptions\n\n\nclass Gpt4AllEngine(Engine):\n    '''\n    An Engine implementation that uses the gpt4all library.\n    '''\n\n    def __init__(self, model_path: str):\n        '''\n        Initializes the Gpt4AllEngine.\n\n        Args:\n            model_path: The full file path to the .gguf model file.\n        '''\n        if GPT4All is None:\n            raise RuntimeError(\n                'The `gpt4all` package is not installed. ' \n                'Please install it with `pip install gpt4all` to use this engine.'\n            )\n\n        model_p = Path(model_path)\n        model_dir = str(model_p.parent)\n        model_name = model_p.name\n\n        try:\n            self.model = GPT4All(model_name=model_name, model_path=model_dir, allow_download=False)\n            logging.info(f\"Gpt4AllEngine initialized for model: {model_name}\")\n        except Exception as e:\n            logging.error(f\"Failed to initialize GPT4All model from path: {model_path}\")\n            logging.error(f\"Ensure the model file '{model_name}' is present in '{model_dir}'.\")\n            raise RuntimeError(f\"Failed to initialize GPT4All: {e}\") from e\n\n    def complete(self, req: CompletionRequest, gen: GenOptions) -> str:\n        '''\n        Generates a completion using the loaded GPT4All model.\n\n        Args:\n            req: The CompletionRequest containing system and user prompts.\n            gen: The GenOptions containing generation parameters.\n\n        Returns:\n            The generated text completion as a string.\n        '''\n        prompt = f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{req.system}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n{req.user}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n\n        try:\n            logging.debug(\"Calling Gpt4All model.generate().\")\n            with self.model.chat_session():\n                # Using chat_session helps manage context, though we are providing the full prompt each time.\n                completion = self.model.generate(\n                    prompt,\n                    max_tokens=gen.max_tokens,\n                    temp=gen.temp,\n                    top_p=gen.top_p,\n                    repeat_penalty=gen.repeat_penalty,\n                    n_batch=1024, # A sensible default\n                )\n            \n            logging.debug(f\"Gpt4All generation successful. Output length: {len(completion)}\")\n            return completion.strip()\n\n        except Exception as e:\n            logging.error(f\"Gpt4All model.generate() failed: {e}\")\n            raise RuntimeError(f\"GPT4All generation failed: {e}\") from e\n"
}
