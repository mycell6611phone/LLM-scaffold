{"type": "tool", "call": {"action": "tool", "tool": "fs_list", "args": {"pattern": "**/*"}}, "result": {"paths": ["/home/sentinel/Desktop/vm_share/workspace/config", "/home/sentinel/Desktop/vm_share/workspace/engine_openai.py", "/home/sentinel/Desktop/vm_share/workspace/goalmanger.md", "/home/sentinel/Desktop/vm_share/workspace/self_critic.py.md", "/home/sentinel/Desktop/vm_share/workspace/goal_manager.py", "/home/sentinel/Desktop/vm_share/workspace/self_critic.py", "/home/sentinel/Desktop/vm_share/workspace/llamacpp.py", "/home/sentinel/Desktop/vm_share/workspace/trainer.py", "/home/sentinel/Desktop/vm_share/workspace/apply_clean_patch.py", "/home/sentinel/Desktop/vm_share/workspace/memory_debate.py", "/home/sentinel/Desktop/vm_share/workspace/gpt4all.py", "/home/sentinel/Desktop/vm_share/workspace/memery.py", "/home/sentinel/Desktop/vm_share/workspace/main.py", "/home/sentinel/Desktop/vm_share/workspace/agent_personas.py.md", "/home/sentinel/Desktop/vm_share/workspace/core_loop.py", "/home/sentinel/Desktop/vm_share/workspace/interface.py", "/home/sentinel/Desktop/vm_share/workspace/requirements.txt", "/home/sentinel/Desktop/vm_share/workspace/planner.py.md", "/home/sentinel/Desktop/vm_share/workspace/planner.py", "/home/sentinel/Desktop/vm_share/workspace/emotion.py", "/home/sentinel/Desktop/vm_share/workspace/gtp5_personas.py", "/home/sentinel/Desktop/vm_share/workspace/experimenter.py", "/home/sentinel/Desktop/vm_share/workspace/memory2.py", "/home/sentinel/Desktop/vm_share/workspace/main1.py", "/home/sentinel/Desktop/vm_share/workspace/goal_manager2.py", "/home/sentinel/Desktop/vm_share/workspace/io", "/home/sentinel/Desktop/vm_share/workspace/dev_box.md", "/home/sentinel/Desktop/vm_share/workspace/__pycache__", "/home/sentinel/Desktop/vm_share/workspace/README.MD", "/home/sentinel/Desktop/vm_share/workspace/goal_manager.py.md", "/home/sentinel/Desktop/vm_share/workspace/config/interface_cli.py", "/home/sentinel/Desktop/vm_share/workspace/config/coreloop.py", "/home/sentinel/Desktop/vm_share/workspace/config/config.yaml.md~", "/home/sentinel/Desktop/vm_share/workspace/config/config.yaml", "/home/sentinel/Desktop/vm_share/workspace/config/coreloop.py~", "/home/sentinel/Desktop/vm_share/workspace/config/interface_cli.py~", "/home/sentinel/Desktop/vm_share/workspace/config/config.yaml.md", "/home/sentinel/Desktop/vm_share/workspace/io/self_critic.py~", "/home/sentinel/Desktop/vm_share/workspace/io/interface.py~", "/home/sentinel/Desktop/vm_share/workspace/io/agent_personas.py", "/home/sentinel/Desktop/vm_share/workspace/io/self_critic.py", "/home/sentinel/Desktop/vm_share/workspace/io/emotion.py~", "/home/sentinel/Desktop/vm_share/workspace/io/trainer.py", "/home/sentinel/Desktop/vm_share/workspace/io/goalmanager.py", "/home/sentinel/Desktop/vm_share/workspace/io/memory.py", "/home/sentinel/Desktop/vm_share/workspace/io/memory_prune.py~", "/home/sentinel/Desktop/vm_share/workspace/io/experimenter.py~", "/home/sentinel/Desktop/vm_share/workspace/io/coreloop.py", "/home/sentinel/Desktop/vm_share/workspace/io/interface.py", "/home/sentinel/Desktop/vm_share/workspace/io/interface.py~~", "/home/sentinel/Desktop/vm_share/workspace/io/goalmanager.py~", "/home/sentinel/Desktop/vm_share/workspace/io/planner.py", "/home/sentinel/Desktop/vm_share/workspace/io/emotion.py", "/home/sentinel/Desktop/vm_share/workspace/io/memoryloop.py~", "/home/sentinel/Desktop/vm_share/workspace/io/experimenter.py", "/home/sentinel/Desktop/vm_share/workspace/io/planner.py~", "/home/sentinel/Desktop/vm_share/workspace/io/agent_personas.py~", "/home/sentinel/Desktop/vm_share/workspace/io/trainer.py~", "/home/sentinel/Desktop/vm_share/workspace/io/memory_prune.py", "/home/sentinel/Desktop/vm_share/workspace/io/memoryloop.py", "/home/sentinel/Desktop/vm_share/workspace/io/memory.py~", "/home/sentinel/Desktop/vm_share/workspace/io/persona.py", "/home/sentinel/Desktop/vm_share/workspace/__pycache__/core_loop.cpython-313.pyc"]}}
{"type": "result", "step": "What programming languages and frameworks are used in the project?", "result": ""}
{"type": "tool", "call": {"action": "tool", "tool": "fs_list", "args": {"pattern": "**/*"}}, "result": {"paths": ["/home/sentinel/Desktop/vm_share/workspace/config", "/home/sentinel/Desktop/vm_share/workspace/engine_openai.py", "/home/sentinel/Desktop/vm_share/workspace/goalmanger.md", "/home/sentinel/Desktop/vm_share/workspace/self_critic.py.md", "/home/sentinel/Desktop/vm_share/workspace/goal_manager.py", "/home/sentinel/Desktop/vm_share/workspace/self_critic.py", "/home/sentinel/Desktop/vm_share/workspace/llamacpp.py", "/home/sentinel/Desktop/vm_share/workspace/trainer.py", "/home/sentinel/Desktop/vm_share/workspace/apply_clean_patch.py", "/home/sentinel/Desktop/vm_share/workspace/memory_debate.py", "/home/sentinel/Desktop/vm_share/workspace/gpt4all.py", "/home/sentinel/Desktop/vm_share/workspace/memery.py", "/home/sentinel/Desktop/vm_share/workspace/main.py", "/home/sentinel/Desktop/vm_share/workspace/agent_personas.py.md", "/home/sentinel/Desktop/vm_share/workspace/core_loop.py", "/home/sentinel/Desktop/vm_share/workspace/interface.py", "/home/sentinel/Desktop/vm_share/workspace/requirements.txt", "/home/sentinel/Desktop/vm_share/workspace/planner.py.md", "/home/sentinel/Desktop/vm_share/workspace/planner.py", "/home/sentinel/Desktop/vm_share/workspace/emotion.py", "/home/sentinel/Desktop/vm_share/workspace/gtp5_personas.py", "/home/sentinel/Desktop/vm_share/workspace/experimenter.py", "/home/sentinel/Desktop/vm_share/workspace/memory2.py", "/home/sentinel/Desktop/vm_share/workspace/main1.py", "/home/sentinel/Desktop/vm_share/workspace/goal_manager2.py", "/home/sentinel/Desktop/vm_share/workspace/io", "/home/sentinel/Desktop/vm_share/workspace/dev_box.md", "/home/sentinel/Desktop/vm_share/workspace/__pycache__", "/home/sentinel/Desktop/vm_share/workspace/README.MD", "/home/sentinel/Desktop/vm_share/workspace/goal_manager.py.md", "/home/sentinel/Desktop/vm_share/workspace/config/interface_cli.py", "/home/sentinel/Desktop/vm_share/workspace/config/coreloop.py", "/home/sentinel/Desktop/vm_share/workspace/config/config.yaml.md~", "/home/sentinel/Desktop/vm_share/workspace/config/config.yaml", "/home/sentinel/Desktop/vm_share/workspace/config/coreloop.py~", "/home/sentinel/Desktop/vm_share/workspace/config/interface_cli.py~", "/home/sentinel/Desktop/vm_share/workspace/config/config.yaml.md", "/home/sentinel/Desktop/vm_share/workspace/io/self_critic.py~", "/home/sentinel/Desktop/vm_share/workspace/io/interface.py~", "/home/sentinel/Desktop/vm_share/workspace/io/agent_personas.py", "/home/sentinel/Desktop/vm_share/workspace/io/self_critic.py", "/home/sentinel/Desktop/vm_share/workspace/io/emotion.py~", "/home/sentinel/Desktop/vm_share/workspace/io/trainer.py", "/home/sentinel/Desktop/vm_share/workspace/io/goalmanager.py", "/home/sentinel/Desktop/vm_share/workspace/io/memory.py", "/home/sentinel/Desktop/vm_share/workspace/io/memory_prune.py~", "/home/sentinel/Desktop/vm_share/workspace/io/experimenter.py~", "/home/sentinel/Desktop/vm_share/workspace/io/coreloop.py", "/home/sentinel/Desktop/vm_share/workspace/io/interface.py", "/home/sentinel/Desktop/vm_share/workspace/io/interface.py~~", "/home/sentinel/Desktop/vm_share/workspace/io/goalmanager.py~", "/home/sentinel/Desktop/vm_share/workspace/io/planner.py", "/home/sentinel/Desktop/vm_share/workspace/io/emotion.py", "/home/sentinel/Desktop/vm_share/workspace/io/memoryloop.py~", "/home/sentinel/Desktop/vm_share/workspace/io/experimenter.py", "/home/sentinel/Desktop/vm_share/workspace/io/planner.py~", "/home/sentinel/Desktop/vm_share/workspace/io/agent_personas.py~", "/home/sentinel/Desktop/vm_share/workspace/io/trainer.py~", "/home/sentinel/Desktop/vm_share/workspace/io/memory_prune.py", "/home/sentinel/Desktop/vm_share/workspace/io/memoryloop.py", "/home/sentinel/Desktop/vm_share/workspace/io/memory.py~", "/home/sentinel/Desktop/vm_share/workspace/io/persona.py", "/home/sentinel/Desktop/vm_share/workspace/__pycache__/core_loop.cpython-313.pyc"]}}
{"type": "tool", "call": {"action": "tool", "tool": "fs_list", "args": {"pattern": "**/*.py"}}, "result": {"paths": ["/home/sentinel/Desktop/vm_share/workspace/engine_openai.py", "/home/sentinel/Desktop/vm_share/workspace/goal_manager.py", "/home/sentinel/Desktop/vm_share/workspace/self_critic.py", "/home/sentinel/Desktop/vm_share/workspace/llamacpp.py", "/home/sentinel/Desktop/vm_share/workspace/trainer.py", "/home/sentinel/Desktop/vm_share/workspace/apply_clean_patch.py", "/home/sentinel/Desktop/vm_share/workspace/memory_debate.py", "/home/sentinel/Desktop/vm_share/workspace/gpt4all.py", "/home/sentinel/Desktop/vm_share/workspace/memery.py", "/home/sentinel/Desktop/vm_share/workspace/main.py", "/home/sentinel/Desktop/vm_share/workspace/core_loop.py", "/home/sentinel/Desktop/vm_share/workspace/interface.py", "/home/sentinel/Desktop/vm_share/workspace/planner.py", "/home/sentinel/Desktop/vm_share/workspace/emotion.py", "/home/sentinel/Desktop/vm_share/workspace/gtp5_personas.py", "/home/sentinel/Desktop/vm_share/workspace/experimenter.py", "/home/sentinel/Desktop/vm_share/workspace/memory2.py", "/home/sentinel/Desktop/vm_share/workspace/main1.py", "/home/sentinel/Desktop/vm_share/workspace/goal_manager2.py", "/home/sentinel/Desktop/vm_share/workspace/config/interface_cli.py", "/home/sentinel/Desktop/vm_share/workspace/config/coreloop.py", "/home/sentinel/Desktop/vm_share/workspace/io/agent_personas.py", "/home/sentinel/Desktop/vm_share/workspace/io/self_critic.py", "/home/sentinel/Desktop/vm_share/workspace/io/trainer.py", "/home/sentinel/Desktop/vm_share/workspace/io/goalmanager.py", "/home/sentinel/Desktop/vm_share/workspace/io/memory.py", "/home/sentinel/Desktop/vm_share/workspace/io/coreloop.py", "/home/sentinel/Desktop/vm_share/workspace/io/interface.py", "/home/sentinel/Desktop/vm_share/workspace/io/planner.py", "/home/sentinel/Desktop/vm_share/workspace/io/emotion.py", "/home/sentinel/Desktop/vm_share/workspace/io/experimenter.py", "/home/sentinel/Desktop/vm_share/workspace/io/memory_prune.py", "/home/sentinel/Desktop/vm_share/workspace/io/memoryloop.py", "/home/sentinel/Desktop/vm_share/workspace/io/persona.py"]}}
{"type": "tool", "call": {"action": "tool", "tool": "fs_list", "args": {"pattern": "**/*.py"}}, "result": {"paths": ["/home/sentinel/Desktop/vm_share/workspace/engine_openai.py", "/home/sentinel/Desktop/vm_share/workspace/goal_manager.py", "/home/sentinel/Desktop/vm_share/workspace/self_critic.py", "/home/sentinel/Desktop/vm_share/workspace/llamacpp.py", "/home/sentinel/Desktop/vm_share/workspace/trainer.py", "/home/sentinel/Desktop/vm_share/workspace/apply_clean_patch.py", "/home/sentinel/Desktop/vm_share/workspace/memory_debate.py", "/home/sentinel/Desktop/vm_share/workspace/gpt4all.py", "/home/sentinel/Desktop/vm_share/workspace/memery.py", "/home/sentinel/Desktop/vm_share/workspace/main.py", "/home/sentinel/Desktop/vm_share/workspace/core_loop.py", "/home/sentinel/Desktop/vm_share/workspace/interface.py", "/home/sentinel/Desktop/vm_share/workspace/planner.py", "/home/sentinel/Desktop/vm_share/workspace/emotion.py", "/home/sentinel/Desktop/vm_share/workspace/gtp5_personas.py", "/home/sentinel/Desktop/vm_share/workspace/experimenter.py", "/home/sentinel/Desktop/vm_share/workspace/memory2.py", "/home/sentinel/Desktop/vm_share/workspace/main1.py", "/home/sentinel/Desktop/vm_share/workspace/goal_manager2.py", "/home/sentinel/Desktop/vm_share/workspace/config/interface_cli.py", "/home/sentinel/Desktop/vm_share/workspace/config/coreloop.py", "/home/sentinel/Desktop/vm_share/workspace/io/agent_personas.py", "/home/sentinel/Desktop/vm_share/workspace/io/self_critic.py", "/home/sentinel/Desktop/vm_share/workspace/io/trainer.py", "/home/sentinel/Desktop/vm_share/workspace/io/goalmanager.py", "/home/sentinel/Desktop/vm_share/workspace/io/memory.py", "/home/sentinel/Desktop/vm_share/workspace/io/coreloop.py", "/home/sentinel/Desktop/vm_share/workspace/io/interface.py", "/home/sentinel/Desktop/vm_share/workspace/io/planner.py", "/home/sentinel/Desktop/vm_share/workspace/io/emotion.py", "/home/sentinel/Desktop/vm_share/workspace/io/experimenter.py", "/home/sentinel/Desktop/vm_share/workspace/io/memory_prune.py", "/home/sentinel/Desktop/vm_share/workspace/io/memoryloop.py", "/home/sentinel/Desktop/vm_share/workspace/io/persona.py"]}}
{"type": "result", "step": "List all files in ~/workspace/ to identify project structure", "result": ""}
{"type": "tool", "call": {"action": "tool", "tool": "fs_list", "args": {"pattern": "**/*.py"}}, "result": {"paths": ["/home/sentinel/Desktop/vm_share/workspace/engine_openai.py", "/home/sentinel/Desktop/vm_share/workspace/goal_manager.py", "/home/sentinel/Desktop/vm_share/workspace/self_critic.py", "/home/sentinel/Desktop/vm_share/workspace/llamacpp.py", "/home/sentinel/Desktop/vm_share/workspace/trainer.py", "/home/sentinel/Desktop/vm_share/workspace/apply_clean_patch.py", "/home/sentinel/Desktop/vm_share/workspace/memory_debate.py", "/home/sentinel/Desktop/vm_share/workspace/gpt4all.py", "/home/sentinel/Desktop/vm_share/workspace/memery.py", "/home/sentinel/Desktop/vm_share/workspace/main.py", "/home/sentinel/Desktop/vm_share/workspace/core_loop.py", "/home/sentinel/Desktop/vm_share/workspace/interface.py", "/home/sentinel/Desktop/vm_share/workspace/planner.py", "/home/sentinel/Desktop/vm_share/workspace/emotion.py", "/home/sentinel/Desktop/vm_share/workspace/gtp5_personas.py", "/home/sentinel/Desktop/vm_share/workspace/experimenter.py", "/home/sentinel/Desktop/vm_share/workspace/memory2.py", "/home/sentinel/Desktop/vm_share/workspace/main1.py", "/home/sentinel/Desktop/vm_share/workspace/goal_manager2.py", "/home/sentinel/Desktop/vm_share/workspace/config/interface_cli.py", "/home/sentinel/Desktop/vm_share/workspace/config/coreloop.py", "/home/sentinel/Desktop/vm_share/workspace/io/agent_personas.py", "/home/sentinel/Desktop/vm_share/workspace/io/self_critic.py", "/home/sentinel/Desktop/vm_share/workspace/io/trainer.py", "/home/sentinel/Desktop/vm_share/workspace/io/goalmanager.py", "/home/sentinel/Desktop/vm_share/workspace/io/memory.py", "/home/sentinel/Desktop/vm_share/workspace/io/coreloop.py", "/home/sentinel/Desktop/vm_share/workspace/io/interface.py", "/home/sentinel/Desktop/vm_share/workspace/io/planner.py", "/home/sentinel/Desktop/vm_share/workspace/io/emotion.py", "/home/sentinel/Desktop/vm_share/workspace/io/experimenter.py", "/home/sentinel/Desktop/vm_share/workspace/io/memory_prune.py", "/home/sentinel/Desktop/vm_share/workspace/io/memoryloop.py", "/home/sentinel/Desktop/vm_share/workspace/io/persona.py"]}}
{"type": "result", "step": "Analyze code files for potential bugs using static analysis tools (e.g., pylint, flake8)", "result": "```json\n{\n  \"action\": \"tool\",\n  \"tool\": \"py_exec\",\n  \"args\": {\n    \"code\": \"import flake8.api.legacy as flake8\\nstyle_guide = flake8.get_style_guide()\\nreport = style_guide.check_files(['\" + \"', '\".join(tool_result['paths']) + \"'])\\nprint(report.total_errors)\"\n  }\n}\n```"}
{"type": "tool", "call": {"action": "tool", "tool": "fs_read", "args": {"relpath": "io/coreloop.py"}}, "result": {"path": "/home/sentinel/Desktop/vm_share/workspace/io/coreloop.py", "text": "core_loop.py\n\nThe central orchestrator for the AGI's \"mind loop.\"\n\nThis script initializes all necessary modules and runs the main cognitive cycle.\nIt serves as the skeleton of the AGI's thought process, calling each\nspecialized module in sequence.\n\"\"\"\n\nimport time\nimport sys\n\n# Import all modular components of the AGI\n# These will be replaced with actual implementations later.\n# For now, we assume they exist as stub files with the required classes.\ntry:\n    from interface import Interface\n    from memory import MemoryManager\n    from goal_manager import GoalManager\n    from planner import Planner\n    from agent_personas import PERSONAS\n    from self_critic import SelfCritic\n    from emotion import MoodManager\n    from experimenter import Experimenter\n    from memoryloop import Debate\n    from memory_prune import MemoryPruner\n    from trainer import TrainingManager\nexcept ImportError as e:\n    print(f\"Error: A required module is missing. {e}\")\n    print(\"Please ensure all stub files (interface.py, memory.py, etc.) exist.\")\n    sys.exit(1)\n\n\nclass CoreLoop:\n    \"\"\"\n    The main class that drives the AGI's operations.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        Initializes all the AGI's components.\n        \"\"\"\n        print(\"AGI Core: Initializing modules...\")\n        \n        # Phase 1: I/O and Core\n        self.interface = Interface()\n        \n        # Phase 2: Memory System\n        self.memory = MemoryManager()\n        self.memory_loop = Debate()\n        self.memory_pruner = MemoryPruner(self.memory)\n\n        # Phase 3: Goals and Planning\n        self.goal_manager = GoalManager()\n        self.planner = Planner()\n\n        # Phase 4 & 5: Execution, Critique & Self-Reflection\n        # Using a list of personas for the critic\n        self.self_critic = SelfCritic(list(PERSONAS.keys()))\n        self.emotion = MoodManager()\n        \n        # Phase 6: Training and Self-Improvement\n        self.trainer = TrainingManager(self.memory)\n        self.experimenter = Experimenter()\n\n        print(\"AGI Core: All modules initialized.\")\n\n    def run(self):\n        \"\"\"\n        Starts the main cognitive loop of the AGI.\n        \"\"\"\n        self.interface.log_message(\"CORE_LOOP\", \"Starting main cognitive cycle.\")\n        \n        while True:\n            try:\n                # 1. Perceive/Input\n                self.interface.log_message(\"PERCEIVE\", \"Awaiting input...\")\n                user_input = self.interface.get_input()\n                if not user_input:\n                    continue\n\n                # 2. Recall\n                self.interface.log_message(\"RECALL\", \"Retrieving relevant memories...\")\n                # relevant_memories = self.memory.retrieve(user_input)\n\n                # 3. Think/Plan (with Goal Injection)\n                self.interface.log_message(\"PLAN\", \"Consulting goal manager and generating plan...\")\n                # active_goal = self.goal_manager.get_active_goal()\n                # plan = self.planner.generate_plan(active_goal, relevant_memories)\n\n                # 4. Critique\n                self.interface.log_message(\"CRITIQUE\", \"Debating plan for validity...\")\n                # revised_plan = self.self_critic.critique_plan(plan)\n\n                # 5. Decide/Act\n                self.interface.log_message(\"DECIDE\", \"Finalizing action from plan...\")\n                # final_action = revised_plan[0] # Execute first step\n\n                # 6. Execute\n                self.interface.log_message(\"EXECUTE\", \"Performing action...\")\n                # self.interface.send_output(f\"ACTION: {final_action}\")\n\n                # 7. Reflect\n                self.interface.log_message(\"REFLECT\", \"Assessing results and generating candidate memory...\")\n                # candidate_memory = f\"Input: {user_input}, Action: {final_action}, Result: Success\"\n\n                # 8. Remember (Debate/Filter)\n                self.interface.log_message(\"REMEMBER\", \"Debating candidate memory for storage...\")\n                # memory_status = self.memory_loop.run_debate(candidate_memory)\n                # if memory_status == \"ACCEPTED\":\n                #     self.memory.add(candidate_memory)\n\n                # 9. Self-Improve\n                self.interface.log_message(\"SELF-IMPROVE\", \"Checking for training or pruning schedule...\")\n                # self.trainer.schedule_training_run()\n                # self.memory_pruner.prune_low_value_memories()\n                \n                # Loop delay\n                time.sleep(2)\n\n            except KeyboardInterrupt:\n                self.interface.log_message(\"CORE_LOOP\", \"Shutdown signal received. Exiting.\")\n                break\n            except Exception as e:\n                self.interface.log_message(\"CORE_LOOP\", f\"An unexpected error occurred: {e}\")\n                time.sleep(5)\n\n\nif __name__ == \"__main__\":\n    core = CoreLoop()\n    core.run()\n\n"}}
{"type": "result", "step": "Locate the relevant code section associated with the selected bug", "result": "```python\nimport sys\nimport time\n\ntry:\n    from interface import Interface\n    from memory import MemoryManager\n    from goal_manager import GoalManager\n    from planner import Planner\n    from agent_personas import PERSONAS\n    from self_critic import SelfCritic\n    from emotion import MoodManager\n    from experimenter import Experimenter\n    from memoryloop import Debate\n    from memory_prune import MemoryPruner\n    from trainer import TrainingManager\nexcept ImportError as e:\n    print(f\"Error: A required module is missing. {e}\")\n    print(\"Please ensure all stub files (interface.py, memory.py, etc.) exist.\")\n    sys.exit(1)\n\n\nclass CoreLoop:\n    \"\"\"\n    The main class that drives the AGI's operations.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initializes all the AGI's components.\n        \"\"\"\n        print(\"AGI Core: Initializing modules...\")\n        \n        # Phase 1: I/O and Core\n        self.interface = Interface()\n        \n        # Phase 2: Memory System\n        self.memory = MemoryManager()\n        self.memory_loop = Debate()\n        self.memory_pruner = MemoryPruner(self.memory)\n        \n        # Phase 3: Goals and Planning\n        self.goal_manager = GoalManager()\n        self.planner = Planner()\n        \n        # Phase 4 & 5: Execution, Critique & Self-Reflection\n        # Using a list of personas for the critic\n        self.self_critic = SelfCritic(list(PERSONAS.keys()))\n        self.emotion = MoodManager()\n        \n        # Phase 6: Training and Self-Improvement\n        self.trainer = TrainingManager(self.memory)\n        self.experimenter = Experimenter()\n\n        print(\"AGI Core: All modules initialized.\")\n\n    def run(self):\n        \"\"\"\n        Starts the main cognitive loop of the AGI.\n        \"\"\"\n        self.interface.log_message(\"CORE_LOOP\", \"Starting main cognitive cycle.\")\n        \n        while True:\n            try:\n                # 1. Perceive/Input\n                self.interface.log_message(\"PERCEIVE\", \"Awaiting input...\")\n                user_input = self.interface.get_input()\n                if not user_input:\n                    continue\n\n                # 2. Recall\n                self.interface.log_message(\"RECALL\", \"Retrieving relevant memories...\")\n                relevant_memories = self.memory.retrieve(user_input)\n\n                # 3. Think/Plan (with Goal Injection)\n                self.interface.log_message(\"PLAN\", \"Consulting goal manager and generating plan...\")\n                active_goal = self.goal_manager.get_active_goal()\n                plan = self.planner.generate_plan(active_goal, relevant_memories)\n\n                # 4. Critique\n                self.interface.log_message(\"CRITIQUE\", \"Debating plan for validity...\")\n                revised_plan = self.self_critic.critique_plan(plan)\n\n                # 5. Decide/Act\n                self.interface.log_message(\"DECIDE\", \"Finalizing action from plan...\")\n                final_action = revised_plan[0]  # Execute first step\n\n                # 6. Execute\n                self.interface.log_message(\"EXECUTE\", \"Performing action...\")\n                self.interface.send_output(f\"ACTION: {final_action}\")\n\n                # 7. Reflect\n                self.interface.log_message(\"REFLECT\", \"Assessing results and generating candidate memory...\")\n                candidate_memory = f\"Input: {user_input}, Action: {final_action}, Result: Success\"\n\n                # 8. Remember (Debate/Filter)\n                self.interface.log_message(\"REMEMBER\", \"Debating candidate memory for storage...\")\n                memory_status = self.memory_loop.run_debate(candidate_memory)\n                if memory_status == \"ACCEPTED\":\n                    self.memory.add(candidate_memory)\n\n                # 9. Self-Improve\n                self.interface.log_message(\"SELF-IMPROVE\", \"Checking for training or pruning schedule...\")\n                self.trainer.schedule_training_run()\n                self.memory_pruner.prune_low_value_memories()\n\n                # Loop delay\n                time.sleep(2)\n\n            except KeyboardInterrupt:\n                self.interface.log_message(\"CORE_LOOP\", \"Shutdown signal received. Exiting.\")\n                break\n            except Exception as e:\n                self.interface.log_message(\"CORE_LOOP\", f\"An unexpected error occurred: {e}\")\n                time.sleep(5)\n\n\nif __name__ == \"__main__\":\n    core = CoreLoop()\n    core.run()\n```"}
{"type": "tool", "call": {"action": "tool", "tool": "fs_list", "args": {"pattern": "**/*.py"}}, "result": {"paths": ["/home/sentinel/Desktop/vm_share/workspace/engine_openai.py", "/home/sentinel/Desktop/vm_share/workspace/goal_manager.py", "/home/sentinel/Desktop/vm_share/workspace/self_critic.py", "/home/sentinel/Desktop/vm_share/workspace/llamacpp.py", "/home/sentinel/Desktop/vm_share/workspace/trainer.py", "/home/sentinel/Desktop/vm_share/workspace/apply_clean_patch.py", "/home/sentinel/Desktop/vm_share/workspace/memory_debate.py", "/home/sentinel/Desktop/vm_share/workspace/gpt4all.py", "/home/sentinel/Desktop/vm_share/workspace/memery.py", "/home/sentinel/Desktop/vm_share/workspace/main.py", "/home/sentinel/Desktop/vm_share/workspace/core_loop.py", "/home/sentinel/Desktop/vm_share/workspace/interface.py", "/home/sentinel/Desktop/vm_share/workspace/planner.py", "/home/sentinel/Desktop/vm_share/workspace/emotion.py", "/home/sentinel/Desktop/vm_share/workspace/gtp5_personas.py", "/home/sentinel/Desktop/vm_share/workspace/experimenter.py", "/home/sentinel/Desktop/vm_share/workspace/memory2.py", "/home/sentinel/Desktop/vm_share/workspace/main1.py", "/home/sentinel/Desktop/vm_share/workspace/goal_manager2.py", "/home/sentinel/Desktop/vm_share/workspace/config/interface_cli.py", "/home/sentinel/Desktop/vm_share/workspace/config/coreloop.py", "/home/sentinel/Desktop/vm_share/workspace/io/agent_personas.py", "/home/sentinel/Desktop/vm_share/workspace/io/self_critic.py", "/home/sentinel/Desktop/vm_share/workspace/io/trainer.py", "/home/sentinel/Desktop/vm_share/workspace/io/goalmanager.py", "/home/sentinel/Desktop/vm_share/workspace/io/memory.py", "/home/sentinel/Desktop/vm_share/workspace/io/coreloop.py", "/home/sentinel/Desktop/vm_share/workspace/io/interface.py", "/home/sentinel/Desktop/vm_share/workspace/io/planner.py", "/home/sentinel/Desktop/vm_share/workspace/io/emotion.py", "/home/sentinel/Desktop/vm_share/workspace/io/experimenter.py", "/home/sentinel/Desktop/vm_share/workspace/io/memory_prune.py", "/home/sentinel/Desktop/vm_share/workspace/io/memoryloop.py", "/home/sentinel/Desktop/vm_share/workspace/io/persona.py"]}}
{"type": "tool", "call": {"action": "tool", "tool": "sh", "args": {"cmd": "grep -r 'TODO' ."}}, "result": {"stdout": "./.venv/lib/python3.13/site-packages/mpmath/matrices/calculus.py:# TODO: should use diagonalization-based algorithms\n./.venv/lib/python3.13/site-packages/mpmath/matrices/calculus.py:        TODO:\n./.venv/lib/python3.13/site-packages/mpmath/matrices/linalg.py:# TODO:\n./.venv/lib/python3.13/site-packages/mpmath/matrices/linalg.py:                if current > biggest: # TODO: what if equal?\n./.venv/lib/python3.13/site-packages/mpmath/matrices/linalg.py:                    # TODO: necessary to check also b?\n./.venv/lib/python3.13/site-packages/mpmath/matrices/linalg.py:            raise RuntimeError(\"need n*n matrix\") # TODO: really?\n./.venv/lib/python3.13/site-packages/mpmath/matrices/linalg.py:    # TODO: implement this\n./.venv/lib/python3.13/site-packages/mpmath/matrices/matrices.py:# TODO: interpret list as vectors (for multiplication)\n./.venv/lib/python3.13/site-packages/mpmath/matrices/matrices.py:        COMMENT: TODO: the above \"doctest:+SKIP\" may be removed as soon as we\n./.venv/lib/python3.13/site-packages/mpmath/functions/functions.py:# TODO: tests; improve implementation\n./.venv/lib/python3.13/site-packages/mpmath/functions/functions.py:    # TODO: accurately eval the smaller of the real/imag parts\n./.venv/lib/python3.13/site-packages/mpmath/functions/functions.py:    # TODO: accurately eval the smaller of the real/imag part\n./.venv/lib/python3.13/site-packages/mpmath/functions/functions.py:            # TODO: this can be done *much* faster\n./.venv/lib/python3.13/site-packages/mpmath/functions/functions.py:    # TODO: the following could be generalized into a perfect\n./.venv/lib/python3.13/site-packages/mpmath/functions/factorials.py:    # TODO: fixme, obviously\n./.venv/lib/python3.13/site-packages/mpmath/functions/orthogonal.py:    # TODO:\n./.venv/lib/python3.13/site-packages/mpmath/functions/orthogonal.py:        # TODO: something else is required here\n./.venv/lib/python3.13/site-packages/mpmath/functions/orthogonal.py:    # TODO: correct evaluation at singularities\n./.venv/lib/python3.13/site-packages/mpmath/functions/theta.py:    # TODO: write _jacobi_theta2a and _jacobi_theta3a using fixed-point\n./.venv/lib/python3.13/site-packages/mpmath/functions/hypergeometric.py:        # Avoid division by zero in leading factors (TODO:\n./.venv/lib/python3.13/site-packages/mpmath/functions/hypergeometric.py:            # TODO: handle the all-real case more efficiently!\n./.venv/lib/python3.13/site-packages/mpmath/functions/hypergeometric.py:            # TODO: figure out how much precision is needed (exponential growth)\n./.venv/lib/python3.13/site-packages/mpmath/functions/hypergeometric.py:        # TODO: the following logic can be simplified\n./.venv/lib/python3.13/site-packages/mpmath/functions/hypergeometric.py:    # TODO: much of the following could be shared with 2F3 instead of\n./.venv/lib/python3.13/site-packages/mpmath/functions/hypergeometric.py:    # TODO: much of the following could be shared with 2F3 instead of\n./.venv/lib/python3.13/site-packages/mpmath/functions/hypergeometric.py:    # TODO: continuation\n./.venv/lib/python3.13/site-packages/mpmath/functions/hypergeometric.py:    # TODO: continuation\n./.venv/lib/python3.13/site-packages/mpmath/functions/expintegrals.py:            # TODO: reasonable sign of infinity\n./.venv/lib/python3.13/site-packages/mpmath/functions/zeta.py:# TODO: fix the interface wrt contexts\n./.venv/lib/python3.13/site-packages/mpmath/functions/zeta.py:# TODO: for bernpoly and eulerpoly, ensure that all exact zeros are covered\n./.venv/lib/python3.13/site-packages/mpmath/functions/zeta.py:# TODO: this should be implemented low-level\n./.venv/lib/python3.13/site-packages/mpmath/functions/zeta.py:    # TODO: implement for derivatives\n./.venv/lib/python3.13/site-packages/mpmath/functions/zeta.py:            # TODO: the following could perhaps be tidied a bit\n./.venv/lib/python3.13/site-packages/mpmath/functions/bessel.py:        # TODO: the integer special-casing shouldn't be necessary.\n./.venv/lib/python3.13/site-packages/mpmath/functions/bessel.py:    # TODO: avoid cancellation for imaginary arguments\n./.venv/lib/python3.13/site-packages/mpmath/functions/bessel.py:# TODO: do this more generically?\n./.venv/lib/python3.13/site-packages/mpmath/functions/bessel.py:# TODO: could be expressed more elegantly using triple factorials\n./.venv/lib/python3.13/site-packages/mpmath/functions/bessel.py:        # TODO: limits\n./.venv/lib/python3.13/site-packages/mpmath/functions/bessel.py:                # TODO: asymptotic series for derivatives\n./.venv/lib/python3.13/site-packages/mpmath/functions/bessel.py:        # TODO: limits\n./.venv/lib/python3.13/site-packages/mpmath/functions/bessel.py:    # TODO: check that chop=True chops when and only when it should\n./.venv/lib/python3.13/site-packages/mpmath/functions/bessel.py:    # TODO: check that chop=True chops when and only when it should\n./.venv/lib/python3.13/site-packages/mpmath/functions/bessel.py:    TODO: this can be optimized, e.g. by reusing evaluation points.\n./.venv/lib/python3.13/site-packages/mpmath/functions/bessel.py:                # TODO: use v <= j'_{v,1} < y_{v,1}?\n./.venv/lib/python3.13/site-packages/mpmath/ctx_mp.py:    # TODO: add more of these, make consistent, write docstrings, ...\n./.venv/lib/python3.13/site-packages/mpmath/calculus/differentiation.py:    TODO: most exponents are zero, so maybe a sparse representation\n./.venv/lib/python3.13/site-packages/mpmath/calculus/extrapolation.py:    (TODO: find a better solution to this problem.)\n./.venv/lib/python3.13/site-packages/mpmath/calculus/extrapolation.py:            # TODO: we are evaluating log(1+eps) -> eps, which is\n./.venv/lib/python3.13/site-packages/mpmath/calculus/odes.py:    **TODO**\n./.venv/lib/python3.13/site-packages/mpmath/calculus/optimization.py:            # TODO: maybe refactoring with function for divided differences\n./.venv/lib/python3.13/site-packages/mpmath/calculus/optimization.py:# TODO: consider raising a ValueError when there's no sign change in a and b\n./.venv/lib/python3.13/site-packages/mpmath/calculus/optimization.py:                # TODO: better condition (when f is very flat)\n./.venv/lib/python3.13/site-packages/mpmath/calculus/optimization.py:# TODO: check whether it's possible to combine it with Illinois stuff\n./.venv/lib/python3.13/site-packages/mpmath/calculus/optimization.py:                # TODO: better condition (when f is very flat)\n./.venv/lib/python3.13/site-packages/mpmath/calculus/optimization.py:            # TODO: decide not to use convergence acceleration\n./.venv/lib/python3.13/site-packages/mpmath/calculus/optimization.py:# TODO: add Brent\n./.venv/lib/python3.13/site-packages/mpmath/calculus/optimization.py:# TODO: test with user-specified jacobian matrix\n./.venv/lib/python3.13/site-packages/mpmath/calculus/optimization.py:            # damping step size TODO: better strategy (hard task)\n./.venv/lib/python3.13/site-packages/mpmath/calculus/optimization.py:        if verify and norm(f(*xl))**2 > tol: # TODO: better condition?\n./.venv/lib/python3.13/site-packages/mpmath/libmp/libmpf.py:    # TODO: account for precision when doing this\n./.venv/lib/python3.13/site-packages/mpmath/libmp/libmpf.py:    TODO: the rounding does not work properly for large exponents.\n./.venv/lib/python3.13/site-packages/mpmath/libmp/libmpi.py:# TODO: optimize\n./.venv/lib/python3.13/site-packages/mpmath/libmp/libmpi.py:    # TODO: combine evaluation code to avoid duplicate modulo\n./.venv/lib/python3.13/site-packages/mpmath/libmp/libmpi.py:    # TODO: optimize for real/imag cases\n./.venv/lib/python3.13/site-packages/mpmath/libmp/libmpi.py:    # TODO: optimize for real/imag cases\n./.venv/lib/python3.13/site-packages/mpmath/libmp/libmpi.py:    # TODO: accuracy for small x\n./.venv/lib/python3.13/site-packages/mpmath/libmp/libmpi.py:    # TODO: recognize/speed up real cases, integer y\n./.venv/lib/python3.13/site-packages/mpmath/libmp/libmpi.py:        # TODO: reflection formula\n./.venv/lib/python3.13/site-packages/mpmath/libmp/libmpi.py:            # TODO: reflection formula\n./.venv/lib/python3.13/site-packages/mpmath/libmp/libintmath.py:TODO: rename, cleanup, perhaps move the gmpy wrapper code\n./.venv/lib/python3.13/site-packages/mpmath/libmp/libintmath.py:# TODO: speed up for bases 2, 4, 8, 16, ...\n./.venv/lib/python3.13/site-packages/mpmath/libmp/libintmath.py:    TODO: speed up using factorization\n./.venv/lib/python3.13/site-packages/mpmath/libmp/libhyper.py:TODO:\n./.venv/lib/python3.13/site-packages/mpmath/libmp/libhyper.py:        # TODO: when there are several real parameters and just a few complex\n./.venv/lib/python3.13/site-packages/mpmath/libmp/libhyper.py:# TODO: mpf_erf should call mpf_erfc when appropriate (currently\n./.venv/lib/python3.13/site-packages/mpmath/libmp/libhyper.py:        # TODO: interval rounding\n./.venv/lib/python3.13/site-packages/mpmath/libmp/libhyper.py:            # TODO: could return finite imaginary value at -inf\n./.venv/lib/python3.13/site-packages/mpmath/libmp/libhyper.py:# TODO: for extremely large x, we could use an asymptotic\n./.venv/lib/python3.13/site-packages/mpmath/libmp/libhyper.py:# TODO: recompute at higher precision if the fixed-point mantissa\n./.venv/lib/python3.13/site-packages/mpmath/libmp/libhyper.py:    TODO:\n./.venv/lib/python3.13/site-packages/mpmath/libmp/libhyper.py:    # TODO: for |x| << 1/2, one could use fall back to\n./.venv/lib/python3.13/site-packages/mpmath/libmp/gammazeta.py:    # formula to the tail. TODO: choose more intelligently\n./.venv/lib/python3.13/site-packages/mpmath/libmp/gammazeta.py:TODO: the current estimation of N for m > 0 is *very suboptimal*.\n./.venv/lib/python3.13/site-packages/mpmath/libmp/gammazeta.py:TODO: implement the reflection formula for m > 0, Re(z) << 0.\n./.venv/lib/python3.13/site-packages/mpmath/libmp/gammazeta.py:TODO: maybe use exact algorithms to compute psi for integral\n./.venv/lib/python3.13/site-packages/mpmath/libmp/gammazeta.py:# TODO: optimize / cleanup interface / unify with list_primes\n./.venv/lib/python3.13/site-packages/mpmath/libmp/gammazeta.py:    TODO: this is currently only used for gamma, but could\n./.venv/lib/python3.13/site-packages/mpmath/libmp/gammazeta.py:    # a fixed-point value. TODO: determine a precise cutoff of validity\n./.venv/lib/python3.13/site-packages/mpmath/libmp/libmpc.py:    # TODO: handle cancellation when c ~=  -1 and ch ~= 1\n./.venv/lib/python3.13/site-packages/mpmath/libmp/libmpc.py:# TODO: avoid loss of accuracy\n./.venv/lib/python3.13/site-packages/mpmath/libmp/libelefun.py:    # TODO: handle rnd direction of the logarithm carefully\n./.venv/lib/python3.13/site-packages/mpmath/libmp/libelefun.py:        # TODO: if close enough to 1, we could use Taylor series\n./.venv/lib/python3.13/site-packages/mpmath/libmp/libelefun.py:# TODO: cleanup the special cases\n./.venv/lib/python3.13/site-packages/mpmath/libmp/libelefun.py:        # TODO: the best cutoff depends on both x and the precision.\n./.venv/lib/python3.13/site-packages/mpmath/libmp/libelefun.py:    # TODO: optimize division precision\n./.venv/lib/python3.13/site-packages/mpmath/identification.py:        # slowly (e.g. a factor 1-10) with each step TODO: we could\n./.venv/lib/python3.13/site-packages/mpmath/tests/test_linalg.py:# TODO: don't use round\n./.venv/lib/python3.13/site-packages/mpmath/tests/runtests.py:# TODO: add a flag for this\n./.venv/lib/python3.13/site-packages/mpmath/tests/test_matrices.py:    # TODO remove exec() wrapper as soon as we drop support for Python <= 3.5\n./.venv/lib/python3.13/site-packages/mpmath/tests/test_gammazeta.py:    # TODO: more tests for polyexp\n./.venv/lib/python3.13/site-packages/mpmath/tests/test_interval.py:    # TODO: many more tests\n./.venv/lib/python3.13/site-packages/mpmath/tests/test_interval.py:    # TODO: need many more tests\n./.venv/lib/python3.13/site-packages/mpmath/tests/torture.py:TODO:\n./.venv/lib/python3.13/site-packages/mpmath/math2.py:        # TODO: sinpi\n./.venv/lib/python3.13/site-packages/mpmath/math2.py:        # TODO: sinpi\n./.venv/lib/python3.13/site-packages/mpmath/math2.py:# TODO: could implement complex erf and erfc here. Need\n./.venv/lib/python3.13/site-packages/pygments/filters/__init__.py:       highlight ``XXX``, ``TODO``, ``FIXME``, ``BUG`` and ``NOTE``.\n./.venv/lib/python3.13/site-packages/pygments/filters/__init__.py:                            ['XXX', 'TODO', 'FIXME', 'BUG', 'NOTE'])\n./.venv/lib/python3.13/site-packages/pygments/lexers/perl.py:    # TODO: give this to a perl guy who knows how to parse perl...\n./.venv/lib/python3.13/site-packages/pygments/lexers/mojo.py:                \"varname\",  # TODO varname the right fit?\n./.venv/lib/python3.13/site-packages/pygments/lexers/mojo.py:                        \"async for\",  # TODO https://docs.modular.com/mojo/roadmap#no-async-for-or-async-with\n./.venv/lib/python3.13/site-packages/pygments/lexers/mojo.py:                        \"async with\",  # TODO https://docs.modular.com/mojo/roadmap#no-async-for-or-async-with\n./.venv/lib/python3.13/site-packages/pygments/lexers/mojo.py:        # TODO supported?\n./.venv/lib/python3.13/site-packages/pygments/lexers/dns.py:            # TODO, $GENERATE https://bind9.readthedocs.io/en/v9.18.14/chapter3.html#soa-rr\n./.venv/lib/python3.13/site-packages/pygments/lexers/rnc.py:            # TODO single quoted strings and escape sequences outside of\n./.venv/lib/python3.13/site-packages/pygments/lexers/dotnet.py:# TODO support multiple languages within the same source file\n./.venv/lib/python3.13/site-packages/pygments/lexers/oberon.py:            # TODO: nested comments (* (* ... *) ... (* ... *) *) not supported!\n./.venv/lib/python3.13/site-packages/pygments/lexers/urbi.py:    # TODO\n./.venv/lib/python3.13/site-packages/pygments/lexers/python.py:        # different tokens.  TODO: DelegatingLexer should support this\n./.venv/lib/python3.13/site-packages/pygments/lexers/nix.py:            # TODO: we should probably escape also here ''${ \\${\n./.venv/lib/python3.13/site-packages/pygments/lexers/nix.py:        # TODO: let/in\n./.venv/lib/python3.13/site-packages/pygments/lexers/templates.py:            # TODO support other Python syntax like $foo['bar']\n./.venv/lib/python3.13/site-packages/pygments/lexers/parsers.py:            # TODO finish implementing other possibilities for scope\n./.venv/lib/python3.13/site-packages/pygments/lexers/openscad.py:            (r':param: [a-zA-Z_]\\w*|:returns?:|(FIXME|MARK|TODO):',\n./.venv/lib/python3.13/site-packages/pygments/lexers/textedit.py:            # TODO: regexes can have other delims\n./.venv/lib/python3.13/site-packages/pygments/lexers/textedit.py:        # TODO: builtins are only subsequent tokens on lines\n./.venv/lib/python3.13/site-packages/pygments/lexers/sql.py:            # TODO: better logging\n./.venv/lib/python3.13/site-packages/pygments/lexers/sql.py:                # TODO: better handle multiline comments at the end with\n./.venv/lib/python3.13/site-packages/pygments/lexers/sql.py:            # TODO: Backslash escapes?\n./.venv/lib/python3.13/site-packages/pygments/lexers/jvm.py:    # TODO / should divide keywords/symbols into namespace/rest\n./.venv/lib/python3.13/site-packages/pygments/lexers/jvm.py:            (r'\\S+\\s+', Text)   # TODO: make tests pass without \\s+\n./.venv/lib/python3.13/site-packages/pygments/lexers/objective.py:                # TODO unsure if ellipses are allowed elsewhere, see\n./.venv/lib/python3.13/site-packages/pygments/lexers/objective.py:            (r':param: [a-zA-Z_]\\w*|:returns?:|(FIXME|MARK|TODO):',\n./.venv/lib/python3.13/site-packages/pygments/lexers/markup.py:            (r'^(\\* )(TODO)( .*)',\n./.venv/lib/python3.13/site-packages/pygments/lexers/markup.py:            (r'^(\\*\\*+ )(TODO)( .*)',\n./.venv/lib/python3.13/site-packages/pygments/lexers/markup.py:            # Unordered lists items, including TODO items and description items\n./.venv/lib/python3.13/site-packages/pygments/lexers/markup.py:            # TODO: language-dependent syntax highlighting (see Markdown lexer)\n./.venv/lib/python3.13/site-packages/pygments/lexers/markup.py:            (_inline(r'=', r'='), String), # TODO token\n./.venv/lib/python3.13/site-packages/pygments/lexers/mips.py:    # TODO: add '*.s' and '*.asm', which will require designing an analyse_text\n./.venv/lib/python3.13/site-packages/pygments/lexers/inferno.py:    TODO:\n./.venv/lib/python3.13/site-packages/pygments/lexers/inferno.py:# TODO:\n./.venv/lib/python3.13/site-packages/pygments/lexers/ada.py:            # TODO: use Name.Namespace if appropriate.  This needs\n./.venv/lib/python3.13/site-packages/pygments/lexers/scripting.py:            # TODO: JES3 statement\n./.venv/lib/python3.13/site-packages/pygments/lexers/c_like.py:            # TODO: \"correctly\" parse complex code attributes\n./.venv/lib/python3.13/site-packages/pygments/lexers/textfmts.py:    # TODO: Make date regex more ISO 8601 compliant\n./.venv/lib/python3.13/site-packages/pygments/lexers/css.py:            # TODO: broken, and prone to infinite loops.\n./.venv/lib/python3.13/site-packages/pygments/lexers/javascript.py:            # TODO: should this include single-line comments and allow nesting strings?\n./.venv/lib/python3.13/site-packages/pygments/lexers/fantom.py:            # TODO: highlight references in fandocs\n./.venv/lib/python3.13/site-packages/pygments/lexers/fantom.py:        'insideUri': [  # TODO: remove copy/paste str/uri\n./.venv/lib/python3.13/site-packages/pygments/lexers/haskell.py:            # TODO: these don't match the comments in docs, remove.\n./.venv/lib/python3.13/site-packages/pygments/lexers/_asy_builtins.py:    TODO: perl/python script in Asymptote SVN similar to asy-list.pl but only\n./.venv/lib/python3.13/site-packages/pygments/lexers/wgsl.py:            # TODO: Treat context-depedendent names specially\n./.venv/lib/python3.13/site-packages/pygments/lexers/wgsl.py:            # TODO: templates start and end tokens.\n./.venv/lib/python3.13/site-packages/pygments/lexers/testing.py:            (r'(?i)\\bTODO\\b', Comment.Preproc),\n./.venv/lib/python3.13/site-packages/pygments/lexers/meson.py:    # TODO String interpolation @VARNAME@ inner matches\n./.venv/lib/python3.13/site-packages/pygments/lexers/meson.py:    # TODO keyword_arg: value inner matches\n./.venv/lib/python3.13/site-packages/pygments/lexers/modula2.py:        'TODO', 'FFI', 'ADDR', 'VARGLIST', 'VARGC',\n./.venv/lib/python3.13/site-packages/pygments/formatters/terminal256.py:# TODO:\n./.venv/lib/python3.13/site-packages/pygments/formatters/latex.py:        # TODO: add support for background colors\n./.venv/lib/python3.13/site-packages/pygments/formatters/img.py:            # TODO: make sure tab expansion happens earlier in the chain.  It\n./.venv/lib/python3.13/site-packages/pygments/lexer.py:    TODO: clean up the code here.\n./.venv/lib/python3.13/site-packages/typing_extensions.py:                # TODO: Use inspect.VALUE here, and make the annotations lazily evaluated\n./.venv/lib/python3.13/site-packages/packaging/metadata.py:        # TODO: The spec doesn't say anything about if the keys should be\n./.venv/lib/python3.13/site-packages/packaging/metadata.py:    description: _Validator[str | None] = _Validator()  # TODO 2.1: can be in body\n./.venv/lib/python3.13/site-packages/packaging/tags.py:        # TODO: Need to care about 32-bit PPC for ppc64 through 10.2?\n./.venv/lib/python3.13/site-packages/packaging/requirements.py:    # TODO: Can we test whether something is contained within a requirement?\n./.venv/lib/python3.13/site-packages/packaging/requirements.py:    # TODO: Can we normalize the name and extra name?\n./.venv/lib/python3.13/site-packages/httpx/_auth.py:        # TODO: implement auth-int\n./.venv/lib/python3.13/site-packages/google/protobuf/pyext/cpp_message.py:# TODO: Remove this import after fix api_implementation\n./.venv/lib/python3.13/site-packages/google/protobuf/internal/type_checkers.py:# TODO: Remove this warning count after 34.0\n./.venv/lib/python3.13/site-packages/google/protobuf/internal/type_checkers.py:      # TODO: Raise errors in 2026 Q1 release\n./.venv/lib/python3.13/site-packages/google/protobuf/internal/type_checkers.py:      # TODO: Raise errors in 2026 Q1 release\n./.venv/lib/python3.13/site-packages/google/protobuf/internal/api_implementation.py:    # TODO: fail back to python\n./.venv/lib/python3.13/site-packages/google/protobuf/internal/containers.py:# TODO: Remove this. BaseContainer does *not* conform to\n./.venv/lib/python3.13/site-packages/google/protobuf/internal/containers.py:# TODO: Constrain T to be a subtype of Message.\n./.venv/lib/python3.13/site-packages/google/protobuf/internal/extension_dict.py:# TODO: Unify error handling of \"unknown extension\" crap.\n./.venv/lib/python3.13/site-packages/google/protobuf/internal/extension_dict.py:# TODO: Support iteritems()-style iteration over all\n./.venv/lib/python3.13/site-packages/google/protobuf/internal/builder.py:  # TODO: Remove this on-op\n./.venv/lib/python3.13/site-packages/google/protobuf/internal/python_message.py:# TODO: Helpers for verbose, common checks like seeing if a\n./.venv/lib/python3.13/site-packages/google/protobuf/internal/python_message.py:  # TODO: Escape Python keywords (e.g., yield), and test this support.\n./.venv/lib/python3.13/site-packages/google/protobuf/internal/python_message.py:  # TODO:  Remove this method entirely if/when everyone agrees with my\n./.venv/lib/python3.13/site-packages/google/protobuf/internal/python_message.py:    # TODO: This may be broken since there may not be\n./.venv/lib/python3.13/site-packages/google/protobuf/internal/python_message.py:    # TODO: This may be broken since there may not be\n./.venv/lib/python3.13/site-packages/google/protobuf/internal/python_message.py:  # TODO: Remove duplication with similar method\n./.venv/lib/python3.13/site-packages/google/protobuf/internal/python_message.py:  # TODO: Migrate all users of these attributes to functions like\n./.venv/lib/python3.13/site-packages/google/protobuf/internal/python_message.py:    # TODO: Use cls.MESSAGE_FACTORY.pool when available.\n./.venv/lib/python3.13/site-packages/google/protobuf/internal/python_message.py:  # TODO: Don't use the factory of generated messages.\n./.venv/lib/python3.13/site-packages/google/protobuf/internal/python_message.py:  # TODO: For now we just strip the hostname.  Better logic will be\n./.venv/lib/python3.13/site-packages/google/protobuf/internal/python_message.py:    # TODO: Fix UnknownFieldSet to consider MessageSet extensions,\n./.venv/lib/python3.13/site-packages/google/protobuf/message_factory.py:        # TODO: Remove this check here. Duplicate extension\n./.venv/lib/python3.13/site-packages/google/protobuf/message_factory.py:      # TODO: Remove this check here. Duplicate extension\n./.venv/lib/python3.13/site-packages/google/protobuf/text_format.py:# TODO Import thread contention leads to test failures.\n./.venv/lib/python3.13/site-packages/google/protobuf/text_format.py:          # TODO: refactor and optimize if this becomes an issue.\n./.venv/lib/python3.13/site-packages/google/protobuf/text_format.py:        # TODO: Change to _allow_singular_overwrites.\n./.venv/lib/python3.13/site-packages/google/protobuf/text_format.py:# TODO: Migrate violators to textformat_tokenizer.\n./.venv/lib/python3.13/site-packages/google/protobuf/descriptor.py:  # TODO: Remove this import after fix api_implementation\n./.venv/lib/python3.13/site-packages/google/protobuf/descriptor.py:    # TODO: Add function to calculate full_name instead of having it in\n./.venv/lib/python3.13/site-packages/google/protobuf/descriptor.py:# TODO: We should have aggressive checking here,\n./.venv/lib/python3.13/site-packages/google/protobuf/descriptor.py:# TODO: for this and other *Descriptor classes, we\n./.venv/lib/python3.13/site-packages/google/protobuf/descriptor.py:  # TODO: Find a way to eliminate this repetition.\n./.venv/lib/python3.13/site-packages/google/protobuf/descriptor.py:  # TODO: Find a way to eliminate this repetition.\n./.venv/lib/python3.13/site-packages/google/protobuf/descriptor.py:  # TODO: Find a way to eliminate this repetition.\n./.venv/lib/python3.13/site-packages/google/protobuf/message.py:# TODO: We should just make these methods all \"pure-virtual\" and move\n./.venv/lib/python3.13/site-packages/google/protobuf/message.py:  # TODO: Link to an HTML document here.\n./.venv/lib/python3.13/site-packages/google/protobuf/message.py:  # TODO: Document that instances of this class will also\n./.venv/lib/python3.13/site-packages/google/protobuf/message.py:  # TODO: Document these fields and methods.\n./.venv/lib/python3.13/site-packages/google/protobuf/message.py:    # TODO: Remove this once the UPB implementation is improved.\n./.venv/lib/python3.13/site-packages/google/protobuf/message.py:  # TODO: MergeFromString() should probably return None and be\n./.venv/lib/python3.13/site-packages/google/protobuf/message.py:    # TODO: Document handling of unknown fields.\n./.venv/lib/python3.13/site-packages/google/protobuf/message.py:    # TODO: When we switch to a helper, this will return None.\n./.venv/lib/python3.13/site-packages/google/protobuf/message.py:  # TODO: Decide whether we like these better\n./.venv/lib/python3.13/site-packages/google/protobuf/message.py:  # TODO: Be sure to document (and test) exactly\n./.venv/lib/python3.13/site-packages/google/protobuf/descriptor_database.py:    # TODO: implement this API.\n./.venv/lib/python3.13/site-packages/google/protobuf/descriptor_database.py:    # TODO: implement this API.\n./.venv/lib/python3.13/site-packages/google/protobuf/descriptor_pool.py:  # TODO: This pool could be constructed from Python code, when we\n./.venv/lib/python3.13/site-packages/google/protobuf/symbol_database.py:    # TODO: Fix the differences with MessageFactory.\n./.venv/lib/python3.13/site-packages/google/auth/aio/_helpers.py:        # TODO(https://github.com/googleapis/google-auth-library-python/issues/1745):\n./.venv/lib/python3.13/site-packages/google/auth/aio/_helpers.py:        # TODO(https://github.com/googleapis/google-auth-library-python/issues/1755):\n./.venv/lib/python3.13/site-packages/google/auth/transport/_aiohttp_requests.py:        # TODO: Use auto_decompress property for aiohttp 3.7+\n./.venv/lib/python3.13/site-packages/google/auth/compute_engine/_metadata.py:        # TODO: implement GCE residency detection on Windows\n./.venv/lib/python3.13/site-packages/google/auth/_helpers.py:# TODO(https://github.com/googleapis/google-auth-library-python/issues/1684): Audit and update the list below.\n./.venv/lib/python3.13/site-packages/google/auth/_helpers.py:        # TODO(https://github.com/googleapis/google-auth-library-python/issues/1701):\n./.venv/lib/python3.13/site-packages/google/auth/_helpers.py:        # TODO(https://github.com/googleapis/google-auth-library-python/issues/1744):\n./.venv/lib/python3.13/site-packages/opentelemetry/exporter/otlp/proto/grpc/metric_exporter/__init__.py:        # TODO(#2663): OTLPExporterMixin should pass timeout to gRPC\n./.venv/lib/python3.13/site-packages/opentelemetry/sdk/metrics/_internal/__init__.py:            metrics. TODO: reference ``PeriodicExportingMetricReader`` usage with push\n./.venv/lib/python3.13/site-packages/opentelemetry/sdk/metrics/_internal/measurement.py:    # TODO Fix doc - if using valid Google `Attributes:` key, the attributes are duplicated\n./.venv/lib/python3.13/site-packages/opentelemetry/sdk/metrics/_internal/exemplar/exemplar.py:    # TODO Fix doc - if using valid Google `Attributes:` key, the attributes are duplicated\n./.venv/lib/python3.13/site-packages/opentelemetry/sdk/_shared_internal/__init__.py:        # TODO(https://github.com/open-telemetry/opentelemetry-python/issues/4555): figure out what this should do.\n./.venv/lib/python3.13/site-packages/opentelemetry/sdk/_shared_internal/__init__.py:    # TODO: Fix force flush so the timeout is used https://github.com/open-telemetry/opentelemetry-python/issues/4568.\n./.venv/lib/python3.13/site-packages/opentelemetry/sdk/trace/__init__.py:        trace_config: TODO\n./.venv/lib/python3.13/site-packages/opentelemetry/sdk/trace/__init__.py:        trace_config: None = None,  # TODO\n./.venv/lib/python3.13/site-packages/opentelemetry/sdk/trace/__init__.py:        # TODO: keep only exception as first argument after baseline is 3.10\n./.venv/lib/python3.13/site-packages/opentelemetry/proto/common/v1/common_pb2.pyi:    TODO: discuss if we are happy with this somewhat complicated definition of what\n./.venv/lib/python3.13/site-packages/triton/language/semantic.py:        #TODO: validate types.\n./.venv/lib/python3.13/site-packages/triton/language/core.py:    # TODO: remove\n./.venv/lib/python3.13/site-packages/triton/language/random.py:    # TODO: fix frontend issues and cleanup\n./.venv/lib/python3.13/site-packages/triton/backends/driver.py:        # TODO: support other frameworks than torch\n./.venv/lib/python3.13/site-packages/triton/backends/driver.py:    # TODO: remove once TMA is cleaned up\n./.venv/lib/python3.13/site-packages/triton/backends/nvidia/driver.py:        self.utils = CudaUtils()  # TODO: make static\n./.venv/lib/python3.13/site-packages/triton/backends/nvidia/compiler.py:    # TODO: Handle non-\"a\" sms\n./.venv/lib/python3.13/site-packages/triton/backends/nvidia/compiler.py:        # TODO(Qingyi): Move PlanCTAPass to the front of CoalescePass\n./.venv/lib/python3.13/site-packages/triton/backends/nvidia/include/cuda_runtime.h: * TODO detail\n./.venv/lib/python3.13/site-packages/triton/backends/nvidia/include/cooperative_groups.h:// TODO: Use a static dispatch to determine appropriate return type\n./.venv/lib/python3.13/site-packages/triton/backends/amd/include/hip/hip_runtime_api.h:// TODO-ctx - more description on error codes.\n./.venv/lib/python3.13/site-packages/triton/backends/amd/include/hip/hip_runtime_api.h:// TODO: implement IPC apis\n./.venv/lib/python3.13/site-packages/triton/backends/amd/include/hip/hip_runtime_api.h: *  - TODO - more 2D, 3D, array APIs here.\n./.venv/lib/python3.13/site-packages/triton/backends/amd/include/hip/hip_runtime_api.h://TODO - Match CUoccupancyB2DSize\n./.venv/lib/python3.13/site-packages/triton/backends/amd/include/hip/hip_runtime_api.h://TODO - Match CUoccupancyB2DSize\n./.venv/lib/python3.13/site-packages/triton/backends/amd/include/hip/hip_runtime_api.h:// TODO - expand descriptions:\n./.venv/lib/python3.13/site-packages/triton/backends/amd/include/hip/hip_runtime_api.h://TODO: Move this to hip_ext.h\n./.venv/lib/python3.13/site-packages/triton/backends/amd/include/hip/amd_detail/amd_device_functions.h:      //TODO : CU Mode impl\n./.venv/lib/python3.13/site-packages/triton/backends/amd/include/hip/amd_detail/amd_hip_vector_types.h:        template< // TODO: constrain based on type as well.\n./.venv/lib/python3.13/site-packages/triton/backends/amd/include/hip/amd_detail/amd_hip_fp8.h:  // TODO: SWDEV-452411\n./.venv/lib/python3.13/site-packages/triton/backends/amd/include/hip/amd_detail/amd_hip_fp8.h:  // TODO: SWDEV-452411\n./.venv/lib/python3.13/site-packages/triton/backends/amd/include/hip/amd_detail/grid_launch.hpp:  // TODO - set to NULL?\n./.venv/lib/python3.13/site-packages/triton/backends/amd/include/hip/amd_detail/texture_fetch_functions.h:    // TODO missing in device libs.\n./.venv/lib/python3.13/site-packages/triton/backends/amd/include/hip/amd_detail/texture_fetch_functions.h:    // TODO missing in device libs.\n./.venv/lib/python3.13/site-packages/triton/backends/amd/include/hip/amd_detail/texture_indirect_functions.h:    // TODO missing in device libs.\n./.venv/lib/python3.13/site-packages/triton/backends/amd/include/hip/amd_detail/texture_indirect_functions.h:    // TODO missing in device libs.\n./.venv/lib/python3.13/site-packages/triton/backends/amd/include/hip/amd_detail/hip_fp16_math_fwd.h://TODO: remove these after they get into clang header __clang_hip_libdevice_declares.h'\n./.venv/lib/python3.13/site-packages/triton/backends/amd/include/hip/amd_detail/amd_hip_complex.h:// TODO: Clang has a bug which allows device functions to call std functions\n./.venv/lib/python3.13/site-packages/triton/backends/amd/include/hip/amd_detail/amd_hip_bf16.h:  // TODO: SWDEV-452411\n./.venv/lib/python3.13/site-packages/triton/backends/amd/include/hip/amd_detail/amd_hip_bf16.h:  // TODO: SWDEV-452411 add operator which converts unsigned long long and long long to bfloat\n./.venv/lib/python3.13/site-packages/triton/backends/amd/include/hip/amd_detail/amd_hip_runtime.h:// TODO-HCC remove old definitions ; ~1602 hcc supports __HCC_ACCELERATOR__ define.\n./.venv/lib/python3.13/site-packages/triton/backends/amd/include/hip/amd_detail/host_defines.h: *  @brief TODO-doc\n./.venv/lib/python3.13/site-packages/triton/backends/amd/include/hip/amd_detail/helpers.hpp:namespace std {  // TODO: these should be removed as soon as possible.\n./.venv/lib/python3.13/site-packages/triton/backends/amd/include/hip/amd_detail/amd_hip_atomic.h://TODO: Remove this after compiler pre-defines the following Macros.\n./.venv/lib/python3.13/site-packages/triton/backends/amd/include/hip/amd_detail/macro_based_grid_launch.hpp:// TODO: - dispatch rank should be derived from the domain dimensions passed\n./.venv/lib/python3.13/site-packages/triton/backends/amd/include/hip/amd_detail/macro_based_grid_launch.hpp:// TODO: these are workarounds, they should be removed.\n./.venv/lib/python3.13/site-packages/triton/backends/amd/include/hip/amd_detail/macro_based_grid_launch.hpp:// TODO: these are temporary and purposefully noisy and disruptive.\n./.venv/lib/python3.13/site-packages/triton/backends/amd/include/hip/amd_detail/amd_hip_fp16.h:            // TODO: rounding behaviour is not correct for host functions.\n./.venv/lib/python3.13/site-packages/triton/backends/amd/include/hsa/hsa_ext_amd.h:  // TODO: Clarify KFD return value for non-uniform access attribute.\n./.venv/lib/python3.13/site-packages/triton/backends/amd/include/hsa/hsa.h:  // TODO: Should this be per Agent?\n./.venv/lib/python3.13/site-packages/triton/backends/amd/include/hsa/amd_hsa_kernel_code.h:// TODO: this struct should be completely gone once debugger designs/implements\n./.venv/lib/python3.13/site-packages/triton/backends/amd/include/hsa/amd_hsa_elf.h:// TODO: Deprecate and remove V1 support and replace this header with using the\n./.venv/lib/python3.13/site-packages/triton/tools/extra/cuda/compile.c:// TODO: some code duplication with `runtime/backend/cuda.c`\n./.venv/lib/python3.13/site-packages/triton/tools/extra/cuda/compile.c:    // TODO: shared memory\n./.venv/lib/python3.13/site-packages/triton/experimental/gluon/language/_core.py:    block_type,  # TODO: block type with layout info\n./.venv/lib/python3.13/site-packages/triton/experimental/gluon/language/_core.py:# TODO: split these\n./.venv/lib/python3.13/site-packages/triton/profiler/viewer.py:    # TODO: generalize to support multiple metrics, not just the first one\n./.venv/lib/python3.13/site-packages/triton/profiler/specs.py:        lambda width, **kwargs: (330.3 * 1e12) / (width / 8),  # TODO(Keren): Implement fp16 acc-> 660.6 fp8\n./.venv/lib/python3.13/site-packages/triton/knobs.py:    # TODO: Use enum to constrain / 'typecheck' the values\n./.venv/lib/python3.13/site-packages/triton/knobs.py:# TODO: Use tuple instead of list for better typing.\n./.venv/lib/python3.13/site-packages/triton/runtime/jit.py:            # TODO(jlebar): I don't actually know how to hit this.  You don't\n./.venv/lib/python3.13/site-packages/triton/runtime/jit.py:        # TODO(jlebar): Remove uses of these fields outside this file, then\n./.venv/lib/python3.13/site-packages/triton/runtime/jit.py:        # TODO : hash should be attribute of `self`\n./.venv/lib/python3.13/site-packages/triton/runtime/interpreter.py:# TODO: wrap everything in triton tensors\n./.venv/lib/python3.13/site-packages/triton/runtime/interpreter.py:        from .jit import _normalize_ty  # TODO: modularize\n./.venv/lib/python3.13/site-packages/triton/compiler/code_generator.py:        # TODO: optimize the following case in which we actually don't have\n./.venv/lib/python3.13/site-packages/triton/compiler/code_generator.py:        # TODO: we currently generate illegal names for non-kernel functions involving constexprs!\n./.venv/lib/python3.13/site-packages/triton/compiler/code_generator.py:        # TODO: raise something meaningful if getattr fails below, esp for reverse method\n./.venv/lib/python3.13/site-packages/triton/compiler/code_generator.py:        # TODO: could probably be cleaned up\n./.venv/lib/python3.13/site-packages/triton/compiler/code_generator.py:    # TODO: refactor\n./.venv/lib/python3.13/site-packages/triton/compiler/code_generator.py:            # TODO: Deal w/ more complicated return types (e.g tuple)\n./.venv/lib/python3.13/site-packages/triton/compiler/compiler.py:    # TODO: Capture and support shared memory space\n./.venv/lib/python3.13/site-packages/triton/compiler/compiler.py:        # TODO - replace with a proper parser\n./.venv/lib/python3.13/site-packages/triton/compiler/compiler.py:    # TODO: Reconcile the difference here between the ASAN and non-ASAN path with enabling\n./.venv/lib/python3.13/site-packages/triton/compiler/compiler.py:        # TODO: n_regs, n_spills should be metadata generated when calling `ptxas`\n./.venv/lib/python3.13/site-packages/click/_termui_impl.py:    # TODO: This never terminates if the passed generator never terminates.\n./.venv/lib/python3.13/site-packages/posthog/__init__.py:            # TODO: Currently this monitoring begins only when the Client is initialised (which happens when you do something with the SDK)\n./.venv/lib/python3.13/site-packages/posthog/exception_integrations/django.py:    # TODO: Abstract integrations one we have more and can see patterns\n./.venv/lib/python3.13/site-packages/posthog/exception_integrations/django.py:        # TODO: Right now this seems too complicated / overkill for us, but seems like we can automatically plug in middlewares\n./.venv/lib/python3.13/site-packages/posthog/exception_integrations/django.py:            # TODO: Align on the format of the distinct_id in tracestate\n./.venv/lib/python3.13/site-packages/posthog/exception_capture.py:    # TODO: Add client side rate limiting to prevent spamming the server with exceptions\n./.venv/lib/python3.13/site-packages/posthog/exception_capture.py:            # TODO: Maybe find a better way of enabling integrations\n./.venv/lib/python3.13/site-packages/posthog/exception_utils.py:            ],  # TODO: We can expand on this type\n./.venv/lib/python3.13/site-packages/posthog/exception_utils.py:            \"errors\": List[Dict[str, Any]],  # TODO: We can expand on this type\n./.venv/lib/python3.13/site-packages/posthog/exception_utils.py:            ],  # TODO: We can expand on this type\n./.venv/lib/python3.13/site-packages/posthog/exception_utils.py:            ],  # TODO: We can expand on this type\n./.venv/lib/python3.13/site-packages/posthog/exception_utils.py:            # \"transaction_info\": Mapping[str, Any],  # TODO: We can expand on this type\n./.venv/lib/python3.13/site-packages/posthog/exception_utils.py:        # TODO - we don't support local variables, yet\n./.venv/lib/python3.13/site-packages/joblib/_memmapping_reducer.py:        # TODO: check scipy sparse datastructure if scipy is installed\n./.venv/lib/python3.13/site-packages/joblib/compressor.py:    TODO python2_drop: is it still needed since we dropped Python 2 support A\n./.venv/lib/python3.13/site-packages/joblib/memory.py:# TODO: The following object should have a data store object as a sub\n./.venv/lib/python3.13/site-packages/joblib/memory.py:# TODO: Same remark for the logger, and probably use the Python logging\n./.venv/lib/python3.13/site-packages/joblib/memory.py:            # TODO (pierreglaser): do the same with get_func_name?\n./.venv/lib/python3.13/site-packages/joblib/test/common.py:# TODO straight removal since in joblib.test.common?\n./.venv/lib/python3.13/site-packages/joblib/test/common.py:# TODO: Turn this back on after refactoring yield based tests in test_hashing\n./.venv/lib/python3.13/site-packages/joblib/test/test_memory.py:    # TODO: test that the cache related to the function cache persists across\n./.venv/lib/python3.13/site-packages/joblib/test/test_memory.py:            # TODO when Python 3.11 is the minimum supported version, use\n./.venv/lib/python3.13/site-packages/joblib/numpy_pickle_utils.py:    TODO python2_drop: is it still needed? The docstring mentions python 2.6\n./.venv/lib/python3.13/site-packages/joblib/pool.py:    TODO python2_drop : can this be simplified ?\n./.venv/lib/python3.13/site-packages/joblib/externals/cloudpickle/cloudpickle.py:        # TODO: decorrelate reducer_override (which is tied to CPython's\n./.venv/lib/python3.13/site-packages/joblib/externals/loky/backend/synchronize.py:# TODO: investigate which Python version is required to be able to use\n./.venv/lib/python3.13/site-packages/joblib/externals/loky/_base.py:# TODO investigate why using `concurrent.futures.Future` directly does not\n./.venv/lib/python3.13/site-packages/joblib/func_inspect.py:        # TODO: Maybe add a warning here?\n./.venv/lib/python3.13/site-packages/joblib/parallel.py:            # TODO: this iterator should be batch_size * n_jobs\n./.venv/lib/python3.13/site-packages/joblib/_store_backends.py:                        # TODO(1.5) turn into error\n./.venv/lib/python3.13/site-packages/huggingface_hub/commands/delete_cache.py:TODO: add support for `huggingface-cli delete-cache aaaaaa bbbbbb cccccc (...)` ?\n./.venv/lib/python3.13/site-packages/huggingface_hub/commands/delete_cache.py:TODO: add \"--keep-last\" arg to delete revisions that are not on `main` ref\n./.venv/lib/python3.13/site-packages/huggingface_hub/commands/delete_cache.py:TODO: add \"--filter\" arg to filter repositories by name ?\n./.venv/lib/python3.13/site-packages/huggingface_hub/commands/delete_cache.py:TODO: add \"--limit\" arg to limit to X repos ?\n./.venv/lib/python3.13/site-packages/huggingface_hub/commands/delete_cache.py:TODO: add \"-y\" arg for immediate deletion ?\n./.venv/lib/python3.13/site-packages/huggingface_hub/commands/delete_cache.py:    # TODO: refactor this + imports in a unified pattern across codebase\n./.venv/lib/python3.13/site-packages/huggingface_hub/keras_mixin.py:                TODO - Some args above aren't used since we are calling\n./.venv/lib/python3.13/site-packages/huggingface_hub/keras_mixin.py:        # TODO: change this in a future PR. We are not returning a KerasModelHubMixin instance here...\n./.venv/lib/python3.13/site-packages/huggingface_hub/serialization/_torch.py:    >>> from huggingface_hub import load_torch_model  # TODO\n./.venv/lib/python3.13/site-packages/huggingface_hub/repocard_data.py:        # TODO - maybe handle this similarly to EvalResult?\n./.venv/lib/python3.13/site-packages/huggingface_hub/repocard_data.py:    # TODO - Check if there cases where this list is longer than one?\n./.venv/lib/python3.13/site-packages/huggingface_hub/constants.py:hf_cache_home = HF_HOME  # for backward compatibility. TODO: remove this in 1.0.0\n./.venv/lib/python3.13/site-packages/huggingface_hub/inference/_client.py:# Some TODO:\n./.venv/lib/python3.13/site-packages/huggingface_hub/inference/_client.py:        # TODO: this should be handled in provider helpers directly\n./.venv/lib/python3.13/site-packages/huggingface_hub/inference/_common.py:    TODO: handle base64 as input\n./.venv/lib/python3.13/site-packages/huggingface_hub/inference/_common.py:            yield get_session().get(content).content  # TODO: retrieve as stream and pipe to post request ?\n./.venv/lib/python3.13/site-packages/huggingface_hub/inference/_generated/_async_client.py:        # TODO: this should be handled in provider helpers directly\n./.venv/lib/python3.13/site-packages/huggingface_hub/inference/_mcp/mcp_client.py:            # ^ TODO: should be handle `get_session_id_callback`? (function to retrieve the current session ID)\n./.venv/lib/python3.13/site-packages/huggingface_hub/hf_api.py:            # TODO: remove this in v1.0\n./.venv/lib/python3.13/site-packages/huggingface_hub/hf_api.py:            # TODO: remove this in v1.0\n./.venv/lib/python3.13/site-packages/huggingface_hub/_local_folder.py:    TODO: factorize logic with `read_download_metadata`.\n./.venv/lib/python3.13/site-packages/huggingface_hub/_local_folder.py:            # TODO: can we do better?\n./.venv/lib/python3.13/site-packages/huggingface_hub/_commit_api.py:                    # TODO: (optimization) download regular files to copy concurrently\n./.venv/lib/python3.13/site-packages/huggingface_hub/_oauth.py:    # TODO: handle generic case (handling OAuth in a non-Space environment with custom dev values) (low priority)\n./.venv/lib/python3.13/site-packages/huggingface_hub/utils/_validators.py:    # TODO: add an argument to opt-out validation for specific argument?\n./.venv/lib/python3.13/site-packages/huggingface_hub/utils/_hf_folder.py:    # TODO: deprecate when adapted in transformers/datasets/gradio\n./.venv/lib/python3.13/site-packages/huggingface_hub/utils/_hf_folder.py:    # TODO: deprecate when adapted in transformers/datasets/gradio\n./.venv/lib/python3.13/site-packages/huggingface_hub/utils/_hf_folder.py:    # TODO: deprecate when adapted in transformers/datasets/gradio\n./.venv/lib/python3.13/site-packages/huggingface_hub/utils/_deprecation.py:    TODO: could be useful to be able to set a custom error message.\n./.venv/lib/python3.13/site-packages/huggingface_hub/hf_file_system.py:        # TODO: use `commit_description` to list all the deleted paths?\n./.venv/lib/python3.13/site-packages/huggingface_hub/hf_file_system.py:                    \"tree_id\": None,  # TODO: tree_id of the root directory?\n./.venv/lib/python3.13/site-packages/transformers/loss/loss_for_object_detection.py:        # TODO use valid to mask invalid areas due to padding in loss\n./.venv/lib/python3.13/site-packages/transformers/quantizers/quantizer_hqq.py:            # TODO: This is a compatibility hack. HQQ-quantized linear layers do not have a `weight` attribute,\n./.venv/lib/python3.13/site-packages/transformers/quantizers/quantizer_bnb_4bit.py:        # TODO: consider bringing replace_with_bnb_linear() code from ..integrations/bitsandbyter.py to here\n./.venv/lib/python3.13/site-packages/transformers/quantizers/quantizer_awq.py:            model._awq_is_fused = True  # TODO: consider storing this flag in model.config instead\n./.venv/lib/python3.13/site-packages/transformers/quantizers/quantizer_eetq.py:                # TODO: Update message once eetq releases a fix\n./.venv/lib/python3.13/site-packages/transformers/quantizers/quantizer_bnb_8bit.py:        # TODO: consider bringing replace_with_bnb_linear() code from ..integrations/bitsandbyter.py to here\n./.venv/lib/python3.13/site-packages/transformers/image_transforms.py:# TODO (Amy): Accept 1/3/4 channel numpy array as input and return np.array as default\n./.venv/lib/python3.13/site-packages/transformers/commands/serving.py:# TODO (joao, matt): streamline tool token detection logic\n./.venv/lib/python3.13/site-packages/transformers/commands/serving.py:    # TODO\n./.venv/lib/python3.13/site-packages/transformers/commands/serving.py:                    # TODO: add other fields\n./.venv/lib/python3.13/site-packages/transformers/commands/serving.py:            # TODO (Joao, Lysandre): the logits processors should be fixed in continuous batching\n./.venv/lib/python3.13/site-packages/transformers/commands/serving.py:        # TODO (Joao, Lysandre): this should also work with tool support\n./.venv/lib/python3.13/site-packages/transformers/commands/serving.py:        # TODO: trigger 2 constrained generations after the tool call start token is emitted:\n./.venv/lib/python3.13/site-packages/transformers/commands/serving.py:                                # TODO: other models will likely need more elaborate processing here\n./.venv/lib/python3.13/site-packages/transformers/commands/serving.py:        # TODO -- Implement non-streaming mode\n./.venv/lib/python3.13/site-packages/transformers/commands/serving.py:                        logprobs=[{\"token\": \"\", \"logprob\": 99.9}],  # TODO: add actual logprobs\n./.venv/lib/python3.13/site-packages/transformers/commands/serving.py:                    logprobs=[{\"token\": \"\", \"logprob\": 99.9}],  # TODO: add actual logprobs\n./.venv/lib/python3.13/site-packages/transformers/commands/serving.py:        # TODO: implement streaming transcription (currently, it's not streaming)\n./.venv/lib/python3.13/site-packages/transformers/tokenization_utils_base.py:        # TODO clean this up at some point (probably by switching to fast tokenizers)\n./.venv/lib/python3.13/site-packages/transformers/audio_utils.py:    # TODO: @eustlb, we actually don't need librosa but soxr is installed with librosa\n./.venv/lib/python3.13/site-packages/transformers/audio_utils.py:    # TODO: @eustlb, we actually don't need librosa but soxr is installed with librosa\n./.venv/lib/python3.13/site-packages/transformers/audio_utils.py:# TODO This method does not support batching yet as we are mainly focused on inference.\n./.venv/lib/python3.13/site-packages/transformers/modeling_tf_utils.py:            # TODO Matt: This is a workaround for older versions of datasets that are missing the `cols_to_retain`\n./.venv/lib/python3.13/site-packages/transformers/modeling_tf_utils.py:        # TODO (joao): flagged for replacement (by `_v2_resized_token_embeddings`) due to embeddings refactor\n./.venv/lib/python3.13/site-packages/transformers/modeling_tf_utils.py:        # TODO (joao): flagged for detection due to embeddings refactor\n./.venv/lib/python3.13/site-packages/transformers/modeling_tf_utils.py:        # TODO (joao): flagged for replacement (by `_v2_resize_token_embeddings`) due to embeddings refactor\n./.venv/lib/python3.13/site-packages/transformers/modeling_tf_utils.py:            # TODO (joao): this one probably needs a v2 version with other models\n./.venv/lib/python3.13/site-packages/transformers/modeling_tf_utils.py:        # TODO (joao): flagged for replacement (by `_v2_get_resized_lm_head_bias`) due to embeddings refactor\n./.venv/lib/python3.13/site-packages/transformers/modeling_tf_utils.py:        # TODO (joao): flagged for replacement (by `_v2_get_resized_embeddings`) due to embeddings refactor\n./.venv/lib/python3.13/site-packages/transformers/modeling_tf_utils.py:            # TODO Matt: This is a temporary workaround to allow weight renaming, but requires a method\n./.venv/lib/python3.13/site-packages/transformers/modeling_tf_utils.py:    # TODO (joao): flagged for detection due to embeddings refactor\n./.venv/lib/python3.13/site-packages/transformers/masking_utils.py:    # TODO: cyril -> probably revisit and remove this, but a lot of tests rely on it\n./.venv/lib/python3.13/site-packages/transformers/masking_utils.py:    # TODO: cyril -> probably revisit and remove this, but a lot of tests rely on it\n./.venv/lib/python3.13/site-packages/transformers/masking_utils.py:    # TODO: cyril -> probably revisit and remove this, but a lot of tests rely on it\n./.venv/lib/python3.13/site-packages/transformers/image_processing_utils_fast.py:        # TODO: remove this once the bug is fixed (detected with torch==2.7.0+git1fee196, torchvision==0.22.0+9eb57cd)\n./.venv/lib/python3.13/site-packages/transformers/modeling_flash_attention_utils.py:# TODO Deprecate when all models have the attention interface\n./.venv/lib/python3.13/site-packages/transformers/modeling_flash_attention_utils.py:# TODO Deprecate when all models have the attention interface\n./.venv/lib/python3.13/site-packages/transformers/modeling_flash_attention_utils.py:        # TODO for now this is required to work with\n./.venv/lib/python3.13/site-packages/transformers/modeling_flash_attention_utils.py:        # TODO for now this is required to work with\n./.venv/lib/python3.13/site-packages/transformers/testing_utils.py:                # TODO: Remove once eetq releases a fix and this release is used in CI\n./.venv/lib/python3.13/site-packages/transformers/testing_utils.py:    # TODO (if possible): Avoid exceptional cases\n./.venv/lib/python3.13/site-packages/transformers/trainer.py:    # TODO: @AjayP13, @younesbelkada replace this check with version check at the next `accelerate` release\n./.venv/lib/python3.13/site-packages/transformers/trainer.py:                # TODO Change dtypes back to M=FP32, Var = BF16, Kahan = False once they can be cast together in torchdistx.\n./.venv/lib/python3.13/site-packages/transformers/trainer.py:            # TODO: in the future support only specific min PEFT versions\n./.venv/lib/python3.13/site-packages/transformers/trainer.py:                    # TODO: in the future support only specific min PEFT versions\n./.venv/lib/python3.13/site-packages/transformers/trainer.py:                # TODO: check this only once or always, with speed being the cost\n./.venv/lib/python3.13/site-packages/transformers/trainer.py:            # TODO Matt: This syntax is deprecated and the preferred version is\n./.venv/lib/python3.13/site-packages/transformers/trainer.py:        # TODO: this needs to be fixed and made cleaner later.\n./.venv/lib/python3.13/site-packages/transformers/trainer.py:                    # TODO: this needs to be fixed and made cleaner later.\n./.venv/lib/python3.13/site-packages/transformers/integrations/flex_attention.py:# TODO: deprecate / rename to make_flex_block_mask for clarity as it's not only causal anymore\n./.venv/lib/python3.13/site-packages/transformers/integrations/executorch.py:        # TODO: The default inputs only work for text models. We need to add support for vision/audio models.\n./.venv/lib/python3.13/site-packages/transformers/integrations/executorch.py:        # TODO (tmanlaibaatar) This won't be needed in torch 2.7.\n./.venv/lib/python3.13/site-packages/transformers/integrations/finegrained_fp8.py:            # when changing a layer the TP PLAN for that layer should be updated. TODO\n./.venv/lib/python3.13/site-packages/transformers/integrations/mxfp4.py:# TODO: Add absolute link when the repo is public\n./.venv/lib/python3.13/site-packages/transformers/integrations/mxfp4.py:    rows_per_chunk: int = 32768 * 1024,  # TODO these values are not here by mistake ;)\n./.venv/lib/python3.13/site-packages/transformers/integrations/mxfp4.py:    scales = scales.to(torch.int32) - 127  # TODO that's because 128=2**7\n./.venv/lib/python3.13/site-packages/transformers/integrations/mxfp4.py:# TODO: Add absolute link when the repo is public\n./.venv/lib/python3.13/site-packages/transformers/integrations/tensor_parallel.py:        # TODO: figure out dynamo support for instance method and switch this to instance method\n./.venv/lib/python3.13/site-packages/transformers/integrations/tensor_parallel.py:        param = param / device_mesh.size()  # TODO should be optionable\n./.venv/lib/python3.13/site-packages/transformers/integrations/tensor_parallel.py:        # TODO: assumes parent module will allreduce the output afterwards (e.g rowlinear bias is IsolatedParallel and parent module is GatherParallel)\n./.venv/lib/python3.13/site-packages/transformers/integrations/tensor_parallel.py:        # TODO: figure out dynamo support for instance method and switch this to instance method\n./.venv/lib/python3.13/site-packages/transformers/integrations/tensor_parallel.py:        # TODO: figure out dynamo support for instance method and switch this to instance method\n./.venv/lib/python3.13/site-packages/transformers/integrations/tensor_parallel.py:        # TODO: i'd like for this to be the default\n./.venv/lib/python3.13/site-packages/transformers/integrations/tensor_parallel.py:        # TODO: need an abstract Parallel class that is different from TensorParallelLayer\n./.venv/lib/python3.13/site-packages/transformers/integrations/tensor_parallel.py:    # TODO: this logic should be wrapped in a function, this is copied from corresponding tp classes.\n./.venv/lib/python3.13/site-packages/transformers/integrations/tensor_parallel.py:):  # TODO: rename to shard_and_distribute_param\n./.venv/lib/python3.13/site-packages/transformers/integrations/tensor_parallel.py:    module_to_tp = model.get_submodule(param_name)  # TODO: can i loop over modules?\n./.venv/lib/python3.13/site-packages/transformers/modeling_layers.py:            # TODO cyril: this one without `S` can be removed after deprection cycle\n./.venv/lib/python3.13/site-packages/transformers/model_debugging_utils.py:    # summary-only version for readability - traversing the tree again #TODO optimize?\n./.venv/lib/python3.13/site-packages/transformers/modeling_attn_mask_utils.py:            # TODO: When tracing with TorchDynamo with fullgraph=True, the model is recompiled depending on the input\n./.venv/lib/python3.13/site-packages/transformers/modeling_attn_mask_utils.py:                # TODO: maybe revisit this with https://github.com/pytorch/pytorch/pull/114823 in PyTorch 2.3.\n./.venv/lib/python3.13/site-packages/transformers/modeling_attn_mask_utils.py:    # TODO: For dynamo, rather use a check on fullgraph=True once this is possible (https://github.com/pytorch/pytorch/pull/120400).\n./.venv/lib/python3.13/site-packages/transformers/onnx/config.py:        # TODO: should we set seq_length = 1 when self.use_past = True?\n./.venv/lib/python3.13/site-packages/transformers/onnx/config.py:            # TODO: test this.\n./.venv/lib/python3.13/site-packages/transformers/onnx/convert.py:            # TODO: Check when exporting QA we provide \"is_pair=True\"\n./.venv/lib/python3.13/site-packages/transformers/pipelines/document_question_answering.py:    # TODO: Update task_summary docs to include an example with document QA and then update the first sentence\n./.venv/lib/python3.13/site-packages/transformers/pipelines/document_question_answering.py:            # TODO: check why slower `LayoutLMTokenizer` and `LayoutLMv2Tokenizer` don't have this key in outputs\n./.venv/lib/python3.13/site-packages/transformers/pipelines/document_question_answering.py:        # TODO: A lot of this logic is specific to Donut and should probably be handled in the tokenizer\n./.venv/lib/python3.13/site-packages/transformers/pipelines/mask_generation.py:                    # TODO: Identifying the model by the type of its returned embeddings is brittle.\n./.venv/lib/python3.13/site-packages/transformers/pipelines/base.py:                # TODO (joao): no PT model should reach this line. However, some audio models with complex\n./.venv/lib/python3.13/site-packages/transformers/pipelines/base.py:        # TODO hack by collating feature_extractor and image_processor\n./.venv/lib/python3.13/site-packages/transformers/pipelines/base.py:        # TODO make the get_iterator work also for `tf` (and `flax`).\n./.venv/lib/python3.13/site-packages/transformers/pipelines/base.py:        # TODO hack by collating feature_extractor and image_processor\n./.venv/lib/python3.13/site-packages/transformers/pipelines/automatic_speech_recognition.py:    # TODO  Use a faster algorithm this can probably be done in O(n)\n./.venv/lib/python3.13/site-packages/transformers/pipelines/text_classification.py:        # TODO try and retrieve it in a nicer way from _sanitize_parameters.\n./.venv/lib/python3.13/site-packages/transformers/tf_utils.py:    # TODO: When the issue linked above gets sorted, add a check on TF version here and use the original function if\n./.venv/lib/python3.13/site-packages/transformers/image_utils.py:        # TODO raise a warning here instead of simply logging?\n./.venv/lib/python3.13/site-packages/transformers/generation/continuous_batching/continuous_api.py:        slice_inputs: bool = True,  # TODO: remove this once parity is ensured\n./.venv/lib/python3.13/site-packages/transformers/generation/continuous_batching/continuous_api.py:                batch_processor.output_probs.copy_(logits)  # TODO\n./.venv/lib/python3.13/site-packages/transformers/generation/continuous_batching/continuous_api.py:        # Pass continuous batching context to logits processor if it supports it. TODO we should find a way to make this a little bit cleaner!\n./.venv/lib/python3.13/site-packages/transformers/generation/continuous_batching/cache.py:            # self.num_key_value_heads //= tp_size # TODO: why is this commented out?\n./.venv/lib/python3.13/site-packages/transformers/generation/continuous_batching/classes.py:    # TODO: this logic seems one token off, check it out\n./.venv/lib/python3.13/site-packages/transformers/generation/tf_logits_process.py:        # TODO (Joao): this function might trigger XLA retracing as `cur_len` increases. Fix it if it becomes\n./.venv/lib/python3.13/site-packages/transformers/generation/tf_logits_process.py:        # TODO (joao): enable XLA on this logits processor. See discussion and attempts in\n./.venv/lib/python3.13/site-packages/transformers/generation/utils.py:# TODO (joao): remove the equivalent classes and typing shortcuts below in v5\n./.venv/lib/python3.13/site-packages/transformers/generation/utils.py:        # 8. Remove unexpected `generate` inputs (TODO @joao: fix trainer and examples)\n./.venv/lib/python3.13/site-packages/transformers/generation/utils.py:                # TODO (joao): remove output/input mismatch when these old models (xlnet, reformer) are deprecated\n./.venv/lib/python3.13/site-packages/transformers/generation/utils.py:        # TODO (joao): find a strategy to specify the order of the processors\n./.venv/lib/python3.13/site-packages/transformers/generation/utils.py:            # TODO: A better way to handle this.\n./.venv/lib/python3.13/site-packages/transformers/generation/utils.py:        # TODO (joao): per-model generation config classes.\n./.venv/lib/python3.13/site-packages/transformers/generation/utils.py:        # TODO(joao): support static caches in assisted generation. assisted generation needs to roll back caches,\n./.venv/lib/python3.13/site-packages/transformers/generation/utils.py:            # TODO (joao): generalize this check with other types of inputs\n./.venv/lib/python3.13/site-packages/transformers/generation/utils.py:        # TODO joao, manuel: remove this in v4.62.0\n./.venv/lib/python3.13/site-packages/transformers/generation/utils.py:        # TODO joao, manuel: remove this in v4.62.0\n./.venv/lib/python3.13/site-packages/transformers/generation/utils.py:                # TODO (joao): this OP throws \"skipping cudagraphs due to ['incompatible ops']\", find solution\n./.venv/lib/python3.13/site-packages/transformers/generation/utils.py:        # TODO (joao): This function should take an optional beam scorer function, to manipulate the scores after\n./.venv/lib/python3.13/site-packages/transformers/generation/utils.py:        # TODO (joao): standardize special cases\n./.venv/lib/python3.13/site-packages/transformers/generation/tf_utils.py:        # TODO (Joao): fix cache format or find programmatic way to detect cache index\n./.venv/lib/python3.13/site-packages/transformers/generation/tf_utils.py:        # TODO (Joao): fix cache format or find programmatic way to detect cache index\n./.venv/lib/python3.13/site-packages/transformers/generation/tf_utils.py:        # TODO (Joao): fix cache format or find programmatic way to detect cache index\n./.venv/lib/python3.13/site-packages/transformers/generation/tf_utils.py:        # TODO (Joao): fix cache format or find programmatic way to detect cache index\n./.venv/lib/python3.13/site-packages/transformers/generation/logits_process.py:# TODO (joao): We shouldn't need this, but there would be a circular import\n./.venv/lib/python3.13/site-packages/transformers/generation/logits_process.py:        # TODO(Patrick): Make sure that official models have max_initial_timestamp_index set to 50\n./.venv/lib/python3.13/site-packages/transformers/generation/configuration_utils.py:        # Deprecated (moved to the Hub). TODO joao, manuel: remove in v4.62.0\n./.venv/lib/python3.13/site-packages/transformers/generation/configuration_utils.py:        # TODO joao: find out a way of not depending on external fields (e.g. `assistant_model`), then make this a\n./.venv/lib/python3.13/site-packages/transformers/generation/configuration_utils.py:        # TODO joao, manuel: remove this in v4.62.0\n./.venv/lib/python3.13/site-packages/transformers/tokenization_utils.py:        # TODO this is fairly slow to improve!\n./.venv/lib/python3.13/site-packages/transformers/tokenization_utils.py:        # TODO @ArthurZ in version 5, special tokens should be handled in convert_tokens_to_string, while _convert_tokens_to_string\n./.venv/lib/python3.13/site-packages/transformers/sagemaker/training_args_sm.py:# TODO: should be moved to `utils` after refactoring of SageMakerTrainer\n./.venv/lib/python3.13/site-packages/transformers/distributed/configuration_utils.py:    # TODO: add tp_plan, pp_plan, device_mesh etc..\n./.venv/lib/python3.13/site-packages/transformers/configuration_utils.py:        # TODO: remove later, deprecated arguments for TF models\n./.venv/lib/python3.13/site-packages/transformers/configuration_utils.py:            # TODO (joao): this should be an exception if the user has modified the loaded config. See #33886\n./.venv/lib/python3.13/site-packages/transformers/cache_utils.py:    # TODO(gante, sanchit-gandhi): move following functionality into `.generate`\n./.venv/lib/python3.13/site-packages/transformers/cache_utils.py:    # TODO (joao, manuel): Remove this class in v4.59.0\n./.venv/lib/python3.13/site-packages/transformers/training_args.py:# TODO: `TrainingArguments` users rely on it being fully mutable. In the future see if we can narrow this to a few keys: https://github.com/huggingface/transformers/pull/25903\n./.venv/lib/python3.13/site-packages/transformers/training_args.py:        # those deprecated arguments are removed from TrainingArguments. (TODO: v5)\n./.venv/lib/python3.13/site-packages/transformers/modeling_rope_utils.py:            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n./.venv/lib/python3.13/site-packages/transformers/modeling_rope_utils.py:    # TODO (joao): use the new `original_max_position_embeddings` from rope_scaling\n./.venv/lib/python3.13/site-packages/transformers/modeling_rope_utils.py:    # TODO (joao): use the new `original_max_position_embeddings` from rope_scaling\n./.venv/lib/python3.13/site-packages/transformers/modeling_rope_utils.py:    # TODO (joao): update logic for the inclusion of `original_max_position_embeddings`\n./.venv/lib/python3.13/site-packages/transformers/modeling_rope_utils.py:    # TODO (joao): update logic for the inclusion of `original_max_position_embeddings`\n./.venv/lib/python3.13/site-packages/transformers/trainer_seq2seq.py:        # TODO: remove this hack when the legacy code that initializes generation_config from a model config is\n./.venv/lib/python3.13/site-packages/transformers/data/datasets/language_modeling.py:        # TODO: randomness could apply a random seed, ex. rng = random.Random(random_seed)\n./.venv/lib/python3.13/site-packages/transformers/data/processors/squad.py:        encoded_dict = tokenizer.encode_plus(  # TODO(thom) update this logic\n./.venv/lib/python3.13/site-packages/transformers/modeling_utils.py:                # TODO naming is stupid it loads it as well\n./.venv/lib/python3.13/site-packages/transformers/modeling_utils.py:        # check `supports_flash_attn_2` for BC with custom code. TODO: remove after a few releases\n./.venv/lib/python3.13/site-packages/transformers/modeling_utils.py:            # check `supports_flash_attn_2` for BC with custom code. TODO: remove after a few releases\n./.venv/lib/python3.13/site-packages/transformers/modeling_utils.py:            # TODO: fix safe_serialization for tied weights\n./.venv/lib/python3.13/site-packages/transformers/modeling_utils.py:            # TODO: we can relax this check when we support taking tp_plan from a json file, for example.\n./.venv/lib/python3.13/site-packages/transformers/modeling_utils.py:                # TODO Cyril: raise an error instead of the warning in v4.53 (and change the test to check for raise instead of success)\n./.venv/lib/python3.13/site-packages/transformers/modeling_utils.py:        # TODO: we should only be calling hf_quantizer.skip_placement or something like that\n./.venv/lib/python3.13/site-packages/transformers/modeling_utils.py:        if \"llama4\" in self.config.model_type:  # TODO try to enable for FULL COMPILE HYBRID CACHE SUPPORT\n./.venv/lib/python3.13/site-packages/transformers/processing_utils.py:                    # TODO: @yoni, change logic in v4.52 (when use_fast set to True by default)\n./.venv/lib/python3.13/site-packages/transformers/utils/auto_docstring.py:AUTODOC_FILES = [\n./.venv/lib/python3.13/site-packages/transformers/utils/auto_docstring.py:    for file_type in AUTODOC_FILES:\n./.venv/lib/python3.13/site-packages/transformers/utils/auto_docstring.py:            # elif param_type == \"\" and False:  # TODO: Enforce typing for all parameters\n./.venv/lib/python3.13/site-packages/transformers/utils/auto_docstring.py:        # TODO (Yoni): Add support for Attributes section in docs\n./.venv/lib/python3.13/site-packages/transformers/utils/hub.py:# TODO: clean this for v5?\n./.venv/lib/python3.13/site-packages/transformers/utils/hub.py:        )  # TODO: This is only used for testing and should be removed once save_jinja_files becomes the default\n./.venv/lib/python3.13/site-packages/transformers/utils/quantization_config.py:                # TODO: Remove this check once configuration version is handled natively by Quark.\n./.venv/lib/python3.13/site-packages/transformers/utils/import_utils.py:# TODO: This doesn't work for all packages (`bs4`, `faiss`, etc.) Talk to Sylvain to see how to do with it better.\n./.venv/lib/python3.13/site-packages/transformers/utils/import_utils.py:            # TODO: Once python 3.9 support is dropped, `importlib.metadata.packages_distributions()`\n./.venv/lib/python3.13/site-packages/transformers/utils/import_utils.py:                # TODO: remove once `importlib.metadata.packages_distributions()` is supported.\n./.venv/lib/python3.13/site-packages/transformers/utils/import_utils.py:    # TODO check if some bugs cause push backs on the exact version\n./.venv/lib/python3.13/site-packages/transformers/utils/import_utils.py:        # TODO: more precise exception matching, if possible.\n./.venv/lib/python3.13/site-packages/transformers/utils/import_utils.py:        # TODO: more precise exception matching, if possible.\n./.venv/lib/python3.13/site-packages/transformers/utils/import_utils.py:        # TODO: Bump the requirement to 2.1.0 once released in https://github.com/ROCmSoftwarePlatform/flash-attention\n./.venv/lib/python3.13/site-packages/transformers/utils/import_utils.py:    # TODO: Check for a minimum version when FA3 is stable\n./.venv/lib/python3.13/site-packages/transformers/utils/attention_visualizer.py:            if \"token_type_ids\" in inputs:  # TODO inspect signature of update causal mask\n./.venv/lib/python3.13/site-packages/transformers/utils/fx.py:    # TODO: add support for them as it should be quite easy to do so (small blocking issues).\n./.venv/lib/python3.13/site-packages/transformers/utils/fx.py:    # TODO: infer shape without performing the computation, this might be quite hard.\n./.venv/lib/python3.13/site-packages/transformers/utils/fx.py:        # TODO: infer shape without performing the computation.\n./.venv/lib/python3.13/site-packages/transformers/utils/fx.py:            # TODO: solves GraphModule creation.\n./.venv/lib/python3.13/site-packages/transformers/image_processing_base.py:# TODO: Move BatchFeature to be imported by both image_processing_utils and image_processing_utils_fast\n./.venv/lib/python3.13/site-packages/transformers/image_processing_base.py:# TODO: (Amy) - factor out the common parts of this and the feature extractor\n./.venv/lib/python3.13/site-packages/transformers/models/encoder_decoder/modeling_tf_encoder_decoder.py:            # TODO (joao): the `TFBaseModelOutput` wrapper should not be needed after the generate refactor is complete\n./.venv/lib/python3.13/site-packages/transformers/models/sam/modeling_tf_sam.py:        # TODO Matt: What is going on here? Why is a non-trainable weight randomly initialized?\n./.venv/lib/python3.13/site-packages/transformers/models/gpt_neox_japanese/modeling_gpt_neox_japanese.py:        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n./.venv/lib/python3.13/site-packages/transformers/models/lightglue/image_processing_lightglue.py:    Converts an image to grayscale format using the NTSC formula. Only support numpy and PIL Image. TODO support torch\n./.venv/lib/python3.13/site-packages/transformers/models/sam2_video/modeling_sam2_video.py:# TODO: This leads to ~1e-07 max diff and ~1e-09 avg diff for q_embed and k_embed from the original implementation, most likely due to the use of complex tensors in the original implementation.\n./.venv/lib/python3.13/site-packages/transformers/models/sam2_video/modular_sam2_video.py:# TODO: This leads to ~1e-07 max diff and ~1e-09 avg diff for q_embed and k_embed from the original implementation, most likely due to the use of complex tensors in the original implementation.\n./.venv/lib/python3.13/site-packages/transformers/models/granitemoe/modeling_granitemoe.py:        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n./.venv/lib/python3.13/site-packages/transformers/models/bert/modeling_bert.py:            # TODO: Improve this warning with e.g. `model.config._attn_implementation = \"manual\"` once implemented.\n./.venv/lib/python3.13/site-packages/transformers/models/dia/processing_dia.py:            # TODO: dac with batching is currently broken, but non-batch is working\n./.venv/lib/python3.13/site-packages/transformers/models/dia/processing_dia.py:        # TODO: see above, dac doesn't work in batches yet\n./.venv/lib/python3.13/site-packages/transformers/models/dia/processing_dia.py:        # TODO: @eustlb, this should be in AudioProcessor\n./.venv/lib/python3.13/site-packages/transformers/models/hunyuan_v1_moe/configuration_hunyuan_v1_moe.py:        # self._rope_scaling_validation()   # TODO: Need validation?\n./.venv/lib/python3.13/site-packages/transformers/models/patchtsmixer/modeling_patchtsmixer.py:        # TODO: we need a refactor so that the different attention modules can get their specific kwargs\n./.venv/lib/python3.13/site-packages/transformers/models/roberta/modeling_roberta.py:            # TODO: Improve this warning with e.g. `model.config._attn_implementation = \"manual\"` once implemented.\n./.venv/lib/python3.13/site-packages/transformers/models/llama4/configuration_llama4.py:        vision_feature_layer (``, *optional*, defaults to -1): TODO\n./.venv/lib/python3.13/site-packages/transformers/models/llama4/configuration_llama4.py:        vision_feature_select_strategy (`int`, *optional*, defaults to `\"default\"`): TODO\n./.venv/lib/python3.13/site-packages/transformers/models/llama4/configuration_llama4.py:        pixel_shuffle_ratio (`int`, *optional*, defaults to 0.5): TODO\n./.venv/lib/python3.13/site-packages/transformers/models/llama4/configuration_llama4.py:        projector_input_dim (`int`, *optional*, defaults to 4096): TODO\n./.venv/lib/python3.13/site-packages/transformers/models/llama4/configuration_llama4.py:        projector_output_dim (`int`, *optional*, defaults to 4096): TODO\n./.venv/lib/python3.13/site-packages/transformers/models/llama4/configuration_llama4.py:        multi_modal_projector_bias (`int`, *optional*, defaults to `False`): TODO\n./.venv/lib/python3.13/site-packages/transformers/models/llama4/configuration_llama4.py:        projector_dropout (`int`, *optional*, defaults to 0.0): TODO\n./.venv/lib/python3.13/site-packages/transformers/models/llama4/configuration_llama4.py:        attention_dropout (`int`, *optional*, defaults to 0.0): TODO\n./.venv/lib/python3.13/site-packages/transformers/models/llama4/configuration_llama4.py:        rope_theta (`int`, *optional*, defaults to 10000): TODO\n./.venv/lib/python3.13/site-packages/transformers/models/llama4/configuration_llama4.py:        intermediate_size_mlp (`int`, *optional*, defaults to 16384): TODO\n./.venv/lib/python3.13/site-packages/transformers/models/llama4/configuration_llama4.py:        head_dim (`int`, *optional*, defaults to 128): TODO\n./.venv/lib/python3.13/site-packages/transformers/models/llama4/configuration_llama4.py:        attention_dropout (`int`, *optional*, defaults to 0.0): TODO\n./.venv/lib/python3.13/site-packages/transformers/models/llama4/configuration_llama4.py:        num_experts_per_tok (`int`, *optional*, defaults to 1): TODO\n./.venv/lib/python3.13/site-packages/transformers/models/llama4/configuration_llama4.py:        num_local_experts (`int`, *optional*, defaults to 16): TODO\n./.venv/lib/python3.13/site-packages/transformers/models/llama4/configuration_llama4.py:        moe_layers (`int`, *optional*): TODO\n./.venv/lib/python3.13/site-packages/transformers/models/llama4/configuration_llama4.py:        interleave_moe_layer_step (`int`, *optional*, defaults to 1): TODO\n./.venv/lib/python3.13/site-packages/transformers/models/llama4/configuration_llama4.py:        use_qk_norm (`int`, *optional*, defaults to `True`): TODO\n./.venv/lib/python3.13/site-packages/transformers/models/llama4/configuration_llama4.py:        output_router_logits (`int`, *optional*, defaults to `False`): TODO\n./.venv/lib/python3.13/site-packages/transformers/models/llama4/configuration_llama4.py:        router_aux_loss_coef (`int`, *optional*, defaults to 0.001): TODO\n./.venv/lib/python3.13/site-packages/transformers/models/llama4/configuration_llama4.py:        router_jitter_noise (`int`, *optional*, defaults to 0.0): TODO\n./.venv/lib/python3.13/site-packages/transformers/models/llama4/configuration_llama4.py:            <TODO>\n./.venv/lib/python3.13/site-packages/transformers/models/llama4/configuration_llama4.py:            <TODO>\n./.venv/lib/python3.13/site-packages/transformers/models/llama4/configuration_llama4.py:            <TODO>\n./.venv/lib/python3.13/site-packages/transformers/models/llama4/configuration_llama4.py:        floor_scale (`int`, *optional*, defaults to 8192): TODO\n./.venv/lib/python3.13/site-packages/transformers/models/llama4/configuration_llama4.py:        attn_scale (`int`, *optional*, defaults to 0.1): TODO\n./.venv/lib/python3.13/site-packages/transformers/models/llama4/modeling_llama4.py:# TODO there is a different RoPE for vision encoder, defined as below\n./.venv/lib/python3.13/site-packages/transformers/models/llama4/modeling_llama4.py:            scaling=None,  # TODO Might be enforced here for TP compatibility as scaling is not just sqrt(head_dim)\n./.venv/lib/python3.13/site-packages/transformers/models/llama4/modeling_llama4.py:        freqs_ci: torch.Tensor,  # TODO move this to an attribute instead of keeping it around\n./.venv/lib/python3.13/site-packages/transformers/models/mamba/modeling_mamba.py:    # TODO (joao): add layer_device_map arg and update code in `generate` accordingly\n./.venv/lib/python3.13/site-packages/transformers/models/pixtral/modeling_pixtral.py:        # TODO maybe make it torch compatible later on. We can also just slice\n./.venv/lib/python3.13/site-packages/transformers/models/canine/modeling_canine.py:            # TODO add support for MLM\n./.venv/lib/python3.13/site-packages/transformers/models/deprecated/tapex/tokenization_tapex.py:        # TODO (Qian): is it possible to revert the original cell if it is in the final answer?\n./.venv/lib/python3.13/site-packages/transformers/models/deprecated/vit_hybrid/modeling_vit_hybrid.py:        # TODO: maybe have a cleaner way to cast the input (from `ImageProcessor` side?)\n./.venv/lib/python3.13/site-packages/transformers/models/deprecated/deta/modeling_deta.py:        # TODO fix this\n./.venv/lib/python3.13/site-packages/transformers/models/deprecated/deta/modeling_deta.py:        # TODO find a better way of exposing other arguments\n./.venv/lib/python3.13/site-packages/transformers/models/deprecated/deta/modeling_deta.py:            # TODO: valid_ratios could be useless here. check https://github.com/fundamentalvision/Deformable-DETR/issues/36\n./.venv/lib/python3.13/site-packages/transformers/models/deprecated/nat/modeling_nat.py:            # TODO: Support arbitrary patch sizes.\n./.venv/lib/python3.13/site-packages/transformers/models/deprecated/nat/modeling_nat.py:                # TODO can we simplify this?\n./.venv/lib/python3.13/site-packages/transformers/models/deprecated/xlm_prophetnet/tokenization_xlm_prophetnet.py:        # TODO ArthurZ fairseq_ids_to_tokens should be removed\n./.venv/lib/python3.13/site-packages/transformers/models/data2vec/modeling_data2vec_audio.py:        # TODO: we need a refactor so that the different attention modules can get their specific kwargs\n./.venv/lib/python3.13/site-packages/transformers/models/xlnet/modeling_tf_xlnet.py:            qlen: TODO Lysandre didn't fill\n./.venv/lib/python3.13/site-packages/transformers/models/xlnet/modeling_tf_xlnet.py:            mlen: TODO Lysandre didn't fill\n./.venv/lib/python3.13/site-packages/transformers/models/voxtral/configuration_voxtral.py:        # TODO: @eustlb, we do not use dropout and layerdrop, yet we need to hardcode them\n./.venv/lib/python3.13/site-packages/transformers/models/voxtral/modular_voxtral.py:# TODO: @eustlb, I would really prefer to use WhisperEncoder but it's messing with modular\n./.venv/lib/python3.13/site-packages/transformers/models/voxtral/processing_voxtral.py:    # TODO: @eustlb, this should be moved to mistral_common + testing\n./.venv/lib/python3.13/site-packages/transformers/models/cohere/tokenization_cohere_fast.py:        # TODO @ArthurZucker this can only work one way for now, to update later-on. Tests should also properly\n./.venv/lib/python3.13/site-packages/transformers/models/cohere/tokenization_cohere_fast.py:    # TODO ArthurZ let's rely on the template processor instead, refactor all fast tokenizers\n./.venv/lib/python3.13/site-packages/transformers/models/flava/modeling_flava.py:        # TODO: Check fp32 layer norm possibility\n./.venv/lib/python3.13/site-packages/transformers/models/led/modeling_tf_led.py:        # TODO: This code is most likely not very efficient and should be improved\n./.venv/lib/python3.13/site-packages/transformers/models/led/modeling_tf_led.py:        # TODO (Joao): investigate why LED has numerical issues in XLA generate\n./.venv/lib/python3.13/site-packages/transformers/models/led/modeling_led.py:        # TODO: remove the redundant computation\n./.venv/lib/python3.13/site-packages/transformers/models/led/modeling_led.py:        # TODO replace this with\n./.venv/lib/python3.13/site-packages/transformers/models/informer/modeling_informer.py:        # TODO: we need a refactor so that the different attention modules can get their specific kwargs\n./.venv/lib/python3.13/site-packages/transformers/models/conditional_detr/modeling_conditional_detr.py:        # TODO find a better way of exposing other arguments\n./.venv/lib/python3.13/site-packages/transformers/models/conditional_detr/image_processing_conditional_detr.py:    # POSTPROCESSING METHODS - TODO: add support for other frameworks\n./.venv/lib/python3.13/site-packages/transformers/models/idefics2/modeling_idefics2.py:        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n./.venv/lib/python3.13/site-packages/transformers/models/camembert/modeling_camembert.py:            # TODO: Improve this warning with e.g. `model.config._attn_implementation = \"manual\"` once implemented.\n./.venv/lib/python3.13/site-packages/transformers/models/camembert/tokenization_camembert.py:        # TODO decode outputs do not match between fast and slow\n./.venv/lib/python3.13/site-packages/transformers/models/dinat/modeling_dinat.py:            # TODO: Support arbitrary patch sizes.\n./.venv/lib/python3.13/site-packages/transformers/models/csm/processing_csm.py:        # TODO: @eustlb, this should be in AudioProcessor\n./.venv/lib/python3.13/site-packages/transformers/models/csm/modeling_csm.py:            # TODO: @eustlb, this should be batched !!!\n./.venv/lib/python3.13/site-packages/transformers/models/csm/generation_csm.py:                # TODO (joao): this OP throws \"skipping cudagraphs due to ['incompatible ops']\", find solution\n./.venv/lib/python3.13/site-packages/transformers/models/csm/generation_csm.py:                # TODO: @eustlb, this should be batched !!!\n./.venv/lib/python3.13/site-packages/transformers/models/csm/modular_csm.py:            # TODO: @eustlb, this should be batched !!!\n./.venv/lib/python3.13/site-packages/transformers/models/bridgetower/configuration_bridgetower.py:        # TODO: remove this once the Hub files are updated.\n./.venv/lib/python3.13/site-packages/transformers/models/ijepa/modeling_ijepa.py:        # TODO: maybe have a cleaner way to cast the input (from `ImageProcessor` side?)\n./.venv/lib/python3.13/site-packages/transformers/models/hgnet_v2/configuration_hgnet_v2.py:# TODO: Modular conversion for resnet must be fixed as\n./.venv/lib/python3.13/site-packages/transformers/models/hgnet_v2/modular_hgnet_v2.py:# TODO: Modular conversion for resnet must be fixed as\n./.venv/lib/python3.13/site-packages/transformers/models/mask2former/image_processing_mask2former.py:# TODO: (Amy) Move to image_transforms\n./.venv/lib/python3.13/site-packages/transformers/models/mask2former/image_processing_mask2former.py:        # TODO: (Amy)\n./.venv/lib/python3.13/site-packages/transformers/models/superpoint/image_processing_superpoint.py:    Converts an image to grayscale format using the NTSC formula. Only support numpy and PIL Image. TODO support torch\n./.venv/lib/python3.13/site-packages/transformers/models/pegasus_x/modeling_pegasus_x.py:        # TODO: we need a refactor so that the different attention modules can get their specific kwargs\n./.venv/lib/python3.13/site-packages/transformers/models/pegasus/tokenization_pegasus.py:# TODO ArthurZ refactor this to only use the added_tokens_encoder\n./.venv/lib/python3.13/site-packages/transformers/models/pegasus/modeling_pegasus.py:        # TODO: we need a refactor so that the different attention modules can get their specific kwargs\n./.venv/lib/python3.13/site-packages/transformers/models/wav2vec2/modeling_tf_wav2vec2.py:        # TODO Matt: Assigning to attributes in call() is deeply sinful in TensorFlow, as it should be idempotent.\n./.venv/lib/python3.13/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:        # TODO: we need a refactor so that the different attention modules can get their specific kwargs\n./.venv/lib/python3.13/site-packages/transformers/models/llama/tokenization_llama_fast.py:    # TODO ArthurZ let's rely on the template processor instead, refactor all fast tokenizers\n./.venv/lib/python3.13/site-packages/transformers/models/qwen2_5_omni/modular_qwen2_5_omni.py:    # TODO: raushan, defaults should be saved in generation config\n./.venv/lib/python3.13/site-packages/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py:    # TODO: raushan, defaults should be saved in generation config\n./.venv/lib/python3.13/site-packages/transformers/models/dots1/configuration_dots1.py:    base_model_tp_plan = {  # TODO: only replicate attention layers when > first_k_dense_replace\n./.venv/lib/python3.13/site-packages/transformers/models/diffllama/modeling_diffllama.py:        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n./.venv/lib/python3.13/site-packages/transformers/models/diffllama/modeling_diffllama.py:        # TODO: These transpose are quite inefficient but Flash Attention requires the layout [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor the KV cache\n./.venv/lib/python3.13/site-packages/transformers/models/diffllama/modular_diffllama.py:        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n./.venv/lib/python3.13/site-packages/transformers/models/diffllama/modular_diffllama.py:        # TODO: These transpose are quite inefficient but Flash Attention requires the layout [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor the KV cache\n./.venv/lib/python3.13/site-packages/transformers/models/stablelm/modeling_stablelm.py:            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n./.venv/lib/python3.13/site-packages/transformers/models/stablelm/modeling_stablelm.py:        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n./.venv/lib/python3.13/site-packages/transformers/models/stablelm/modeling_stablelm.py:        # TODO: These transpose are quite inefficient but Flash Attention requires the layout [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor the KV cache\n./.venv/lib/python3.13/site-packages/transformers/models/stablelm/modeling_stablelm.py:        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n./.venv/lib/python3.13/site-packages/transformers/models/granitemoeshared/modeling_granitemoeshared.py:        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n./.venv/lib/python3.13/site-packages/transformers/models/big_bird/modeling_big_bird.py:            # TODO(PVP): need to verify if below code is correct\n./.venv/lib/python3.13/site-packages/transformers/models/perceiver/tokenization_perceiver.py:    # TODO @ArthurZ refactor this as well....\n./.venv/lib/python3.13/site-packages/transformers/models/sew/modeling_sew.py:        # TODO: we need a refactor so that the different attention modules can get their specific kwargs\n./.venv/lib/python3.13/site-packages/transformers/models/deberta_v2/tokenization_deberta_v2.py:    # TODO add a deprecation cycle as this can have different behaviour from our API\n./.venv/lib/python3.13/site-packages/transformers/models/musicgen/modeling_musicgen.py:        # TODO: we need a refactor so that the different attention modules can get their specific kwargs\n./.venv/lib/python3.13/site-packages/transformers/models/t5/modeling_t5.py:            # TODO(thom): Add z_loss https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/layers.py#L666\n./.venv/lib/python3.13/site-packages/transformers/models/t5/tokenization_t5_fast.py:# TODO(PVP) - this should be removed in Transformers v5\n./.venv/lib/python3.13/site-packages/transformers/models/moonshine/modular_moonshine.py:    # TODO arthur, how do we separate when it cross / self coming from different layer?\n./.venv/lib/python3.13/site-packages/transformers/models/moonshine/modeling_moonshine.py:    # TODO arthur, how do we separate when it cross / self coming from different layer?\n./.venv/lib/python3.13/site-packages/transformers/models/longt5/modeling_longt5.py:# TODO: Update before the merge\n./.venv/lib/python3.13/site-packages/transformers/models/longt5/modeling_longt5.py:    _can_compile_fullgraph = False  # TODO: @raushan more involved due to local/global attn\n./.venv/lib/python3.13/site-packages/transformers/models/longt5/modeling_longt5.py:            # TODO(thom): Add z_loss https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/layers.py#L666\n./.venv/lib/python3.13/site-packages/transformers/models/time_series_transformer/modeling_time_series_transformer.py:        # TODO: we need a refactor so that the different attention modules can get their specific kwargs\n./.venv/lib/python3.13/site-packages/transformers/models/time_series_transformer/modeling_time_series_transformer.py:    # TODO: tests would need a rewrite to check for correct implementation\n./.venv/lib/python3.13/site-packages/transformers/models/rwkv/modeling_rwkv.py:    # TODO: maybe jit, otherwise move inside forward\n./.venv/lib/python3.13/site-packages/transformers/models/barthez/tokenization_barthez.py:# TODO this class is useless. This is the most standard sentencpiece model. Let's find which one is closest and nuke this.\n./.venv/lib/python3.13/site-packages/transformers/models/deepseek_v3/modular_deepseek_v3.py:    TODO let's just use the original freqcis computation to not have the view\n./.venv/lib/python3.13/site-packages/transformers/models/deepseek_v3/configuration_deepseek_v3.py:    base_model_tp_plan = {  # TODO: only replicate attention layers when > first_k_dense_replace\n./.venv/lib/python3.13/site-packages/transformers/models/deepseek_v3/modeling_deepseek_v3.py:    TODO let's just use the original freqcis computation to not have the view\n./.venv/lib/python3.13/site-packages/transformers/models/zoedepth/image_processing_zoedepth.py:        # TODO support align_corners=True in image_transforms.resize\n./.venv/lib/python3.13/site-packages/transformers/models/mluke/tokenization_mluke.py:        # TODO check if the t5/llama PR also applies here\n./.venv/lib/python3.13/site-packages/transformers/models/granite_speech/modeling_granite_speech.py:        # TODO (@avihu111) find a fast alternative to einsum\n./.venv/lib/python3.13/site-packages/transformers/models/granite_speech/modeling_granite_speech.py:        # TODO (@alex-jw-brooks) add an example to this docstring once models are released\n./.venv/lib/python3.13/site-packages/transformers/models/granite_speech/feature_extraction_granite_speech.py:        # TODO (@alex-jw-brooks): Currently input_features_mask is not\n./.venv/lib/python3.13/site-packages/transformers/models/granite_speech/processing_granite_speech.py:            # TODO (@alex-jw-brooks); we should add a util to get_num_audio_tokens\n./.venv/lib/python3.13/site-packages/transformers/models/bart/configuration_bart.py:            # TODO: figure this case out.\n./.venv/lib/python3.13/site-packages/transformers/models/bart/configuration_bart.py:            # TODO: test this.\n./.venv/lib/python3.13/site-packages/transformers/models/bart/modeling_bart.py:        # TODO: we need a refactor so that the different attention modules can get their specific kwargs\n./.venv/lib/python3.13/site-packages/transformers/models/gpt2/configuration_gpt2.py:            # TODO: how to do that better?\n./.venv/lib/python3.13/site-packages/transformers/models/kosmos2_5/modeling_kosmos2_5.py:# TODO: check with krip\n./.venv/lib/python3.13/site-packages/transformers/models/kosmos2_5/modeling_kosmos2_5.py:    # TODO (ydshieh): Remove this (to match Llama's code)\n./.venv/lib/python3.13/site-packages/transformers/models/kosmos2_5/modeling_kosmos2_5.py:    # TODO (ydshieh): Remove this (to match Llama's code)\n./.venv/lib/python3.13/site-packages/transformers/models/kosmos2_5/image_processing_kosmos2_5_fast.py:        # TODO: revert once the issue is fixed: https://huggingface.slack.com/archives/C02TXKQQLE5/p1743411133979019\n./.venv/lib/python3.13/site-packages/transformers/models/kosmos2_5/image_processing_kosmos2_5_fast.py:        # TODO: correct this return type, and the docstring\n./.venv/lib/python3.13/site-packages/transformers/models/kosmos2_5/image_processing_kosmos2_5_fast.py:            # TODO: if it's possible to do in batch mode\n./.venv/lib/python3.13/site-packages/transformers/models/kosmos2_5/image_processing_kosmos2_5_fast.py:            # TODO: we need this to be in batch from\n./.venv/lib/python3.13/site-packages/transformers/models/unispeech_sat/modular_unispeech_sat.py:        >>> # TODO: Add full pretraining example\n./.venv/lib/python3.13/site-packages/transformers/models/unispeech_sat/modular_unispeech_sat.py:        # TODO(PVP) - add pretraining logic and add to tests\n./.venv/lib/python3.13/site-packages/transformers/models/unispeech_sat/modeling_unispeech_sat.py:        # TODO: we need a refactor so that the different attention modules can get their specific kwargs\n./.venv/lib/python3.13/site-packages/transformers/models/unispeech_sat/modeling_unispeech_sat.py:        >>> # TODO: Add full pretraining example\n./.venv/lib/python3.13/site-packages/transformers/models/unispeech_sat/modeling_unispeech_sat.py:        # TODO(PVP) - add pretraining logic and add to tests\n./.venv/lib/python3.13/site-packages/transformers/models/jamba/modeling_jamba.py:        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n./.venv/lib/python3.13/site-packages/transformers/models/jamba/modeling_jamba.py:            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n./.venv/lib/python3.13/site-packages/transformers/models/clip/modeling_tf_clip.py:        # TODO: As is this currently fails with saved_model=True, because\n./.venv/lib/python3.13/site-packages/transformers/models/patchtst/modeling_patchtst.py:        # TODO: we need a refactor so that the different attention modules can get their specific kwargs\n./.venv/lib/python3.13/site-packages/transformers/models/recurrent_gemma/modeling_recurrent_gemma.py:        # `sqrt` in order to prevent NaNs during training in bfloat16. TODO a bit annoying\n./.venv/lib/python3.13/site-packages/transformers/models/recurrent_gemma/modeling_recurrent_gemma.py:    # TODO refactor\n./.venv/lib/python3.13/site-packages/transformers/models/recurrent_gemma/modeling_recurrent_gemma.py:        if use_cache and inputs_embeds.shape[1] != 1:  # TODO let's maybe only call in the `generate`?\n./.venv/lib/python3.13/site-packages/transformers/models/recurrent_gemma/modeling_recurrent_gemma.py:# TODO: re-enable check: Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM with LLAMA->RECURRENTGEMMA,Llama->RecurrentGemma,llama->gemma\n./.venv/lib/python3.13/site-packages/transformers/models/recurrent_gemma/modeling_recurrent_gemma.py:        # Soft-cap the logits TODO remove if always done.\n./.venv/lib/python3.13/site-packages/transformers/models/bark/generation_configuration_bark.py:    # TODO (joao): nested from_dict\n./.venv/lib/python3.13/site-packages/transformers/models/bark/modeling_bark.py:        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n./.venv/lib/python3.13/site-packages/transformers/models/bark/modeling_bark.py:        # TODO (joao):workaround until nested generation config is compatible with PreTrained Model\n./.venv/lib/python3.13/site-packages/transformers/models/nemotron/modeling_nemotron.py:# TODO cyril: modular\n./.venv/lib/python3.13/site-packages/transformers/models/nemotron/modeling_nemotron.py:        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n./.venv/lib/python3.13/site-packages/transformers/models/nemotron/modeling_nemotron.py:        # TODO: These transpose are quite inefficient but Flash Attention requires the layout [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor the KV cache\n./.venv/lib/python3.13/site-packages/transformers/models/nemotron/modeling_nemotron.py:# TODO cyril: modular\n./.venv/lib/python3.13/site-packages/transformers/models/nemotron/modeling_nemotron.py:            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n./.venv/lib/python3.13/site-packages/transformers/models/nemotron/modeling_nemotron.py:# TODO: re-enable check: Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM with LLAMA->NEMOTRON,Llama->Nemotron,llama->nemotron\n./.venv/lib/python3.13/site-packages/transformers/models/mbart/modeling_mbart.py:        # TODO: we need a refactor so that the different attention modules can get their specific kwargs\n./.venv/lib/python3.13/site-packages/transformers/models/mbart/configuration_mbart.py:            # TODO: figure this case out.\n./.venv/lib/python3.13/site-packages/transformers/models/mbart/configuration_mbart.py:            # TODO: test this.\n./.venv/lib/python3.13/site-packages/transformers/models/gpt_neox/modular_gpt_neox.py:        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n./.venv/lib/python3.13/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n./.venv/lib/python3.13/site-packages/transformers/models/git/modeling_git.py:        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n./.venv/lib/python3.13/site-packages/transformers/models/detr/image_processing_detr.py:# TODO - (Amy) make compatible with other frameworks\n./.venv/lib/python3.13/site-packages/transformers/models/detr/image_processing_detr.py:# TODO - (Amy) make compatible with other frameworks\n./.venv/lib/python3.13/site-packages/transformers/models/detr/image_processing_detr.py:    # TODO (Amy) - update to use `rescale_factor` instead of `scale`\n./.venv/lib/python3.13/site-packages/transformers/models/detr/image_processing_detr.py:    # POSTPROCESSING METHODS - TODO: add support for other frameworks\n./.venv/lib/python3.13/site-packages/transformers/models/detr/modeling_detr.py:        # TODO find a better way of exposing other arguments\n./.venv/lib/python3.13/site-packages/transformers/models/olmoe/modeling_olmoe.py:        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n./.venv/lib/python3.13/site-packages/transformers/models/olmoe/modeling_olmoe.py:        # TODO: These transpose are quite inefficient but Flash Attention requires the layout [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor the KV cache\n./.venv/lib/python3.13/site-packages/transformers/models/olmoe/modeling_olmoe.py:            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n./.venv/lib/python3.13/site-packages/transformers/models/olmoe/modeling_olmoe.py:        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n./.venv/lib/python3.13/site-packages/transformers/models/bigbird_pegasus/configuration_bigbird_pegasus.py:            # TODO: figure this case out.\n./.venv/lib/python3.13/site-packages/transformers/models/bigbird_pegasus/configuration_bigbird_pegasus.py:            # TODO: test this.\n./.venv/lib/python3.13/site-packages/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py:            # TODO(PVP): need to verify if below code is correct\n./.venv/lib/python3.13/site-packages/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py:        # TODO: we need a refactor so that the different attention modules can get their specific kwargs\n./.venv/lib/python3.13/site-packages/transformers/models/table_transformer/modeling_table_transformer.py:        # TODO find a better way of exposing other arguments\n./.venv/lib/python3.13/site-packages/transformers/models/hubert/modeling_tf_hubert.py:        # TODO Matt: Assigning to attributes in call() is deeply sinful in TensorFlow, as it should be idempotent.\n./.venv/lib/python3.13/site-packages/transformers/models/hubert/modeling_hubert.py:        # TODO: we need a refactor so that the different attention modules can get their specific kwargs\n./.venv/lib/python3.13/site-packages/transformers/models/biogpt/modeling_biogpt.py:        # TODO: we need a refactor so that the different attention modules can get their specific kwargs\n./.venv/lib/python3.13/site-packages/transformers/models/groupvit/modeling_tf_groupvit.py:        # TODO: As is this currently fails with saved_model=True, because\n./.venv/lib/python3.13/site-packages/transformers/models/mobilenet_v2/image_processing_mobilenet_v2.py:        # TODO: add support for other frameworks\n./.venv/lib/python3.13/site-packages/transformers/models/mobilenet_v2/image_processing_mobilenet_v2_fast.py:        # TODO: add support for other frameworks\n./.venv/lib/python3.13/site-packages/transformers/models/instructblip/modeling_instructblip.py:    _keep_in_fp32_modules = [\"query_tokens\"]  # TODO @ArthurZucker I don't know why this is required for FP8\n./.venv/lib/python3.13/site-packages/transformers/models/instructblip/modeling_instructblip.py:    _keep_in_fp32_modules = [\"query_tokens\"]  # TODO @ArthurZucker I don't know why this is required for FP8\n./.venv/lib/python3.13/site-packages/transformers/models/musicgen_melody/modeling_musicgen_melody.py:        # TODO: we need a refactor so that the different attention modules can get their specific kwargs\n./.venv/lib/python3.13/site-packages/transformers/models/rt_detr/modeling_rt_detr.py:# TODO: Replace all occurrences of the checkpoint with the final one\n./.venv/lib/python3.13/site-packages/transformers/models/longformer/modeling_tf_longformer.py:        # TODO: This code is most likely not very efficient and should be improved\n./.venv/lib/python3.13/site-packages/transformers/models/longformer/modeling_longformer.py:        # TODO: remove the redundant computation\n./.venv/lib/python3.13/site-packages/transformers/models/longformer/modeling_longformer.py:        # TODO replace this with\n./.venv/lib/python3.13/site-packages/transformers/models/vit/modeling_vit.py:        # TODO: maybe have a cleaner way to cast the input (from `ImageProcessor` side?)\n./.venv/lib/python3.13/site-packages/transformers/models/nougat/tokenization_nougat_fast.py:        # TODO Come up with footnote formatting inside a table\n./.venv/lib/python3.13/site-packages/transformers/models/falcon/modeling_falcon.py:        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n./.venv/lib/python3.13/site-packages/transformers/models/falcon/modeling_falcon.py:        # TODO: These transpose are quite inefficient but Flash Attention requires the layout [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor the KV cache\n./.venv/lib/python3.13/site-packages/transformers/models/falcon/modeling_falcon.py:        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n./.venv/lib/python3.13/site-packages/transformers/models/falcon/modeling_falcon.py:        # TODO: As of torch==2.2.0, the `attention_mask` passed to the model in `generate` is 2D and of dynamic length even when the static\n./.venv/lib/python3.13/site-packages/transformers/models/efficientloftr/image_processing_efficientloftr.py:    Converts an image to grayscale format using the NTSC formula. Only support numpy and PIL Image. TODO support torch\n./.venv/lib/python3.13/site-packages/transformers/models/chameleon/modeling_chameleon.py:# TODO(joao): add me back asap :)\n./.venv/lib/python3.13/site-packages/transformers/models/chameleon/modeling_chameleon.py:            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: this may break with compilation\n./.venv/lib/python3.13/site-packages/transformers/models/chameleon/modeling_chameleon.py:    # TODO(joao): add me back asap :)\n./.venv/lib/python3.13/site-packages/transformers/models/blenderbot_small/modeling_blenderbot_small.py:        # TODO: we need a refactor so that the different attention modules can get their specific kwargs\n./.venv/lib/python3.13/site-packages/transformers/models/blenderbot_small/configuration_blenderbot_small.py:            # TODO: figure this case out.\n./.venv/lib/python3.13/site-packages/transformers/models/blenderbot_small/configuration_blenderbot_small.py:            # TODO: test this.\n./.venv/lib/python3.13/site-packages/transformers/models/xlm_roberta_xl/modeling_xlm_roberta_xl.py:            # TODO: Improve this warning with e.g. `model.config._attn_implementation = \"manual\"` once implemented.\n./.venv/lib/python3.13/site-packages/transformers/models/beit/image_processing_beit.py:        # TODO: add support for other frameworks\n./.venv/lib/python3.13/site-packages/transformers/models/beit/image_processing_beit_fast.py:        # TODO: add support for other frameworks\n./.venv/lib/python3.13/site-packages/transformers/models/gemma3n/modeling_gemma3n.py:        # TODO (raushan): Fix this after RoPE refactor. For now we hack it by\n./.venv/lib/python3.13/site-packages/transformers/models/gemma3n/feature_extraction_gemma3n.py:        # TODO: The filtered mask is always exactly 3 elements longer than the mel_spectrogram. Why???\n./.venv/lib/python3.13/site-packages/transformers/models/gemma3n/modular_gemma3n.py:        # TODO (raushan): Fix this after RoPE refactor. For now we hack it by\n./.venv/lib/python3.13/site-packages/transformers/models/fuyu/image_processing_fuyu.py:        # TODO refer to https://github.com/ArthurZucker/transformers/blob/0f0a3fe5ca5697ee58faeb5b53f049af720b5e98/src/transformers/models/vit_mae/modeling_vit_mae.py#L871\n./.venv/lib/python3.13/site-packages/transformers/models/fuyu/processing_fuyu.py:        # TODO Remove this logic in a subsequent release since subsequences are not supported.\n./.venv/lib/python3.13/site-packages/transformers/models/fuyu/processing_fuyu.py:        self.max_position_embeddings = 16384  # TODO Can't derive this from model files: where to set it?\n./.venv/lib/python3.13/site-packages/transformers/models/fuyu/modeling_fuyu.py:            # don't pass kwargs because Persimmon-backbone doesn't accept FA2 kwargs yet, TODO: raushan\n./.venv/lib/python3.13/site-packages/transformers/models/jetmoe/modeling_jetmoe.py:            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n./.venv/lib/python3.13/site-packages/transformers/models/jetmoe/modeling_jetmoe.py:        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n./.venv/lib/python3.13/site-packages/transformers/models/jetmoe/modeling_jetmoe.py:        # TODO: These transpose are quite inefficient but Flash Attention requires the layout [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor the KV cache\n./.venv/lib/python3.13/site-packages/transformers/models/jetmoe/modeling_jetmoe.py:        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n./.venv/lib/python3.13/site-packages/transformers/models/speech_to_text/modeling_speech_to_text.py:        # TODO: we need a refactor so that the different attention modules can get their specific kwargs\n./.venv/lib/python3.13/site-packages/transformers/models/speech_to_text/modeling_speech_to_text.py:# TODO: change copy when applying cache class\n./.venv/lib/python3.13/site-packages/transformers/models/speech_to_text/modeling_speech_to_text.py:    # TODO: tests would need a rewrite to check for correct implementation\n./.venv/lib/python3.13/site-packages/transformers/models/speech_to_text/modeling_tf_speech_to_text.py:        # TODO (Joao): investigate why Speech2Text has numerical issues in XLA generate\n./.venv/lib/python3.13/site-packages/transformers/models/whisper/modeling_whisper.py:        # TODO: we need a refactor so that the different attention modules can get their specific kwargs\n./.venv/lib/python3.13/site-packages/transformers/models/whisper/modeling_tf_whisper.py:        # TODO: Implement `WhisperTimeStampLogitsProcessor`.\n./.venv/lib/python3.13/site-packages/transformers/models/whisper/tokenization_whisper.py:                    # TODO Handle when language is different from the previous\n./.venv/lib/python3.13/site-packages/transformers/models/mllama/processing_mllama.py:            TODO: add aspect_ratio_ids and aspect_ratio_mask and cross_attention_mask\n./.venv/lib/python3.13/site-packages/transformers/models/superglue/image_processing_superglue.py:    Converts an image to grayscale format using the NTSC formula. Only support numpy and PIL Image. TODO support torch\n./.venv/lib/python3.13/site-packages/transformers/models/esm/modeling_esmfold.py:# TODO Add information to the docstring about any methods that convert to PDB format, or otherwise prepare\n./.venv/lib/python3.13/site-packages/transformers/models/esm/openfold_utils/residue_constants.py:# TODO: ^ interpret this\n./.venv/lib/python3.13/site-packages/transformers/models/esm/openfold_utils/residue_constants.py:    # TODO: this file should be downloaded in a setup script\n./.venv/lib/python3.13/site-packages/transformers/models/esm/configuration_esm.py:# TODO Update this\n./.venv/lib/python3.13/site-packages/transformers/models/esm/tokenization_esm.py:        # TODO, all the tokens are added? But they are also part of the vocab... bit strange.\n./.venv/lib/python3.13/site-packages/transformers/models/segformer/image_processing_segformer_fast.py:        # TODO: add support for other frameworks\n./.venv/lib/python3.13/site-packages/transformers/models/segformer/image_processing_segformer.py:        # TODO: add support for other frameworks\n./.venv/lib/python3.13/site-packages/transformers/models/qwen2_5_vl/configuration_qwen2_5_vl.py:        # TODO: @raushan update config in the hub\n./.venv/lib/python3.13/site-packages/transformers/models/owlvit/image_processing_owlvit_fast.py:        # TODO: (amy) add support for other frameworks\n./.venv/lib/python3.13/site-packages/transformers/models/owlvit/image_processing_owlvit.py:        # TODO: (amy) add support for other frameworks\n./.venv/lib/python3.13/site-packages/transformers/models/phimoe/modeling_phimoe.py:            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n./.venv/lib/python3.13/site-packages/transformers/models/phimoe/modeling_phimoe.py:        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n./.venv/lib/python3.13/site-packages/transformers/models/auto/image_processing_auto.py:        # TODO: @yoni, change in v4.48 (use_fast set to True by default)\n./.venv/lib/python3.13/site-packages/transformers/models/auto/image_processing_auto.py:        # TODO: @yoni, change logic in v4.52 (when use_fast set to True by default)\n./.venv/lib/python3.13/site-packages/transformers/models/kyutai_speech_to_text/modeling_kyutai_speech_to_text.py:# TODO cyril: modular\n./.venv/lib/python3.13/site-packages/transformers/models/kyutai_speech_to_text/modeling_kyutai_speech_to_text.py:        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n./.venv/lib/python3.13/site-packages/transformers/models/kyutai_speech_to_text/modeling_kyutai_speech_to_text.py:        # TODO: These transpose are quite inefficient but Flash Attention requires the layout [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor the KV cache\n./.venv/lib/python3.13/site-packages/transformers/models/kyutai_speech_to_text/modeling_kyutai_speech_to_text.py:# TODO cyril: modular\n./.venv/lib/python3.13/site-packages/transformers/models/kyutai_speech_to_text/modeling_kyutai_speech_to_text.py:            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n./.venv/lib/python3.13/site-packages/transformers/models/kyutai_speech_to_text/modeling_kyutai_speech_to_text.py:        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n./.venv/lib/python3.13/site-packages/transformers/models/kyutai_speech_to_text/modeling_kyutai_speech_to_text.py:    # TODO: @eustlb, this should be standardized\n./.venv/lib/python3.13/site-packages/transformers/models/kyutai_speech_to_text/modeling_kyutai_speech_to_text.py:    # TODO: @eustlb, this should be standardized\n./.venv/lib/python3.13/site-packages/transformers/models/kyutai_speech_to_text/modeling_kyutai_speech_to_text.py:        # TODO: @eustlb, we should have per-batch-idx values\n./.venv/lib/python3.13/site-packages/transformers/models/kyutai_speech_to_text/modular_kyutai_speech_to_text.py:    # TODO: @eustlb, this should be standardized\n./.venv/lib/python3.13/site-packages/transformers/models/kyutai_speech_to_text/modular_kyutai_speech_to_text.py:    # TODO: @eustlb, this should be standardized\n./.venv/lib/python3.13/site-packages/transformers/models/kyutai_speech_to_text/modular_kyutai_speech_to_text.py:        # TODO: @eustlb, we should have per-batch-idx values\n./.venv/lib/python3.13/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py:            # TODO: Improve this warning with e.g. `model.config._attn_implementation = \"manual\"` once implemented.\n./.venv/lib/python3.13/site-packages/transformers/models/xlm_roberta/tokenization_xlm_roberta.py:        # TODO check if the t5/llama PR also applies here\n./.venv/lib/python3.13/site-packages/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py:# TODO: Could have better fused kernels depending on scaling, dropout and head mask.\n./.venv/lib/python3.13/site-packages/transformers/models/dbrx/modeling_dbrx.py:        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n./.venv/lib/python3.13/site-packages/transformers/models/dbrx/modeling_dbrx.py:        # TODO: These transpose are quite inefficient but Flash Attention requires the layout\n./.venv/lib/python3.13/site-packages/transformers/models/dbrx/modeling_dbrx.py:            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n./.venv/lib/python3.13/site-packages/transformers/models/dbrx/modeling_dbrx.py:        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n./.venv/lib/python3.13/site-packages/transformers/models/vision_encoder_decoder/modeling_tf_vision_encoder_decoder.py:            # TODO (joao): the `TFBaseModelOutput` wrapper should not be needed after the generate refactor is complete\n./.venv/lib/python3.13/site-packages/transformers/models/funnel/modeling_funnel.py:        # TODO: deal with head_mask\n./.venv/lib/python3.13/site-packages/transformers/models/funnel/modeling_funnel.py:        # TODO: deal with head_mask\n./.venv/lib/python3.13/site-packages/transformers/models/phi4_multimodal/feature_extraction_phi4_multimodal.py:    # TODO; @eustlb, move this to audio_utils in a general spectogram_batch function that handles torch and numpy\n./.venv/lib/python3.13/site-packages/transformers/models/mimi/modeling_mimi.py:# TODO cyril: modular\n./.venv/lib/python3.13/site-packages/transformers/models/mimi/modeling_mimi.py:        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n./.venv/lib/python3.13/site-packages/transformers/models/mimi/modeling_mimi.py:        # TODO: These transpose are quite inefficient but Flash Attention requires the layout [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor the KV cache\n./.venv/lib/python3.13/site-packages/transformers/models/mimi/modeling_mimi.py:# TODO cyril: modular\n./.venv/lib/python3.13/site-packages/transformers/models/mimi/modeling_mimi.py:            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n./.venv/lib/python3.13/site-packages/transformers/models/mimi/modeling_mimi.py:        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n./.venv/lib/python3.13/site-packages/transformers/models/mimi/modeling_mimi.py:        # TODO: @eustlb, let's make the encoder support padding_mask so that batched inputs are supported.\n./.venv/lib/python3.13/site-packages/transformers/models/mimi/modeling_mimi.py:        # TODO: @eustlb, convert the padding mask to attention mask.\n./.venv/lib/python3.13/site-packages/transformers/models/rag/modeling_rag.py:        # because RAG expands input for doc-size internally. TODO: raushan, remove me when all models support\n./.venv/lib/python3.13/site-packages/transformers/models/rag/retrieval_rag.py:            # TODO(Patrick): if we train more RAG models, I want to put the input first to take advantage of effortless truncation\n./.venv/lib/python3.13/site-packages/transformers/models/rag/retrieval_rag.py:            # TODO(piktus): better handling of truncation\n./.venv/lib/python3.13/site-packages/transformers/models/codegen/modeling_codegen.py:        # TODO(enijkamp): factor out number of logical TPU-v4 cores or make forward pass agnostic\n./.venv/lib/python3.13/site-packages/transformers/models/codegen/modeling_codegen.py:        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n./.venv/lib/python3.13/site-packages/transformers/models/codegen/configuration_codegen.py:            # TODO: how to do that better?\n./.venv/lib/python3.13/site-packages/transformers/models/d_fine/configuration_d_fine.py:# TODO: Attribute map assignment logic should be fixed in modular\n./.venv/lib/python3.13/site-packages/transformers/models/d_fine/modular_d_fine.py:# TODO: Attribute map assignment logic should be fixed in modular\n./.venv/lib/python3.13/site-packages/transformers/models/idefics/modeling_idefics.py:        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n./.venv/lib/python3.13/site-packages/transformers/models/idefics/modeling_idefics.py:            # TODO(ls): Add cross attention values to respective lists\n./.venv/lib/python3.13/site-packages/transformers/models/idefics/modeling_tf_idefics.py:                # TODO(ls): Add cross attention values to respective lists\n./.venv/lib/python3.13/site-packages/transformers/models/grounding_dino/processing_grounding_dino.py:                    # TODO: @pavel, set labels to None since v4.51.0 or find a way to extract ids\n./.venv/lib/python3.13/site-packages/transformers/models/grounding_dino/modeling_grounding_dino.py:            # TODO: valid_ratios could be useless here. check https://github.com/fundamentalvision/Deformable-DETR/issues/36\n./.venv/lib/python3.13/site-packages/transformers/models/yolos/image_processing_yolos.py:    # POSTPROCESSING METHODS - TODO: add support for other frameworks\n./.venv/lib/python3.13/site-packages/transformers/models/falcon_mamba/modeling_falcon_mamba.py:    # TODO (joao): add layer_device_map arg and update code in `generate` accordingly\n./.venv/lib/python3.13/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py:        # TODO: we need a refactor so that the different attention modules can get their specific kwargs\n./.venv/lib/python3.13/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py:    # TODO: If anyone is up to it to make sure tests pass etc\n./.venv/lib/python3.13/site-packages/transformers/models/oneformer/image_processing_oneformer.py:        # TODO: (Amy)\n./.venv/lib/python3.13/site-packages/transformers/models/m2m_100/configuration_m2m_100.py:            # TODO: test this.\n./.venv/lib/python3.13/site-packages/transformers/models/m2m_100/modeling_m2m_100.py:        # TODO: we need a refactor so that the different attention modules can get their specific kwargs\n./.venv/lib/python3.13/site-packages/transformers/models/bloom/modeling_bloom.py:    TODO @thomasw21 this doesn't work as nicely due to the masking strategy, and so masking varies slightly.\n./.venv/lib/python3.13/site-packages/transformers/models/bloom/modeling_bloom.py:        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n./.venv/lib/python3.13/site-packages/transformers/models/bloom/configuration_bloom.py:            # TODO: how to do that better?\n./.venv/lib/python3.13/site-packages/transformers/models/bloom/tokenization_bloom_fast.py:        # TODO @ArthurZucker this can only work one way for now, to update later-on. Tests should also properly\n./.venv/lib/python3.13/site-packages/transformers/models/distilbert/modeling_distilbert.py:        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n./.venv/lib/python3.13/site-packages/transformers/models/fsmt/modeling_fsmt.py:# TODO:\n./.venv/lib/python3.13/site-packages/transformers/models/fsmt/modeling_fsmt.py:            # TODO(SS): do we need to ignore pad tokens in labels?\n./.venv/lib/python3.13/site-packages/transformers/models/moshi/modeling_moshi.py:# TODO cyril: modular\n./.venv/lib/python3.13/site-packages/transformers/models/moshi/modeling_moshi.py:        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n./.venv/lib/python3.13/site-packages/transformers/models/moshi/modeling_moshi.py:        # TODO: These transpose are quite inefficient but Flash Attention requires the layout [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor the KV cache\n./.venv/lib/python3.13/site-packages/transformers/models/moshi/modeling_moshi.py:# TODO cyril: modular\n./.venv/lib/python3.13/site-packages/transformers/models/moshi/modeling_moshi.py:            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n./.venv/lib/python3.13/site-packages/transformers/models/moshi/modeling_moshi.py:        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n./.venv/lib/python3.13/site-packages/transformers/models/shieldgemma2/processing_shieldgemma2.py:        # TODO(ryanmullins): Support images from PIL or URLs.\n./.venv/lib/python3.13/site-packages/transformers/models/qwen2_moe/modeling_qwen2_moe.py:# TODO cyril: modular\n./.venv/lib/python3.13/site-packages/transformers/models/qwen2_moe/modeling_qwen2_moe.py:        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n./.venv/lib/python3.13/site-packages/transformers/models/qwen2_moe/modeling_qwen2_moe.py:# TODO cyril: modular\n./.venv/lib/python3.13/site-packages/transformers/models/qwen2_moe/modeling_qwen2_moe.py:            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n./.venv/lib/python3.13/site-packages/transformers/models/qwen2_moe/modeling_qwen2_moe.py:        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n./.venv/lib/python3.13/site-packages/transformers/models/blenderbot/modeling_blenderbot.py:        # TODO: we need a refactor so that the different attention modules can get their specific kwargs\n./.venv/lib/python3.13/site-packages/transformers/models/sam2/modular_sam2.py:        # TODO: add connected components kernel for postprocessing\n./.venv/lib/python3.13/site-packages/transformers/models/sam2/image_processing_sam2_fast.py:        # TODO: add connected components kernel for postprocessing\n./.venv/lib/python3.13/site-packages/transformers/models/maskformer/image_processing_maskformer.py:# TODO: (Amy) Move to image_transforms\n./.venv/lib/python3.13/site-packages/transformers/models/maskformer/image_processing_maskformer.py:        # TODO: (Amy)\n./.venv/lib/python3.13/site-packages/transformers/models/hunyuan_v1_dense/configuration_hunyuan_v1_dense.py:        # self._rope_scaling_validation()   # TODO: Need validation?\n./.venv/lib/python3.13/site-packages/transformers/models/plbart/modeling_plbart.py:        # TODO: we need a refactor so that the different attention modules can get their specific kwargs\n./.venv/lib/python3.13/site-packages/transformers/models/xlm/tokenization_xlm.py:            # TODO: make sure we are using `FacebookAI/xlm-mlm-enro-1024`, since XLM-100 doesn't have this step\n./.venv/lib/python3.13/site-packages/transformers/models/deit/modeling_deit.py:        # TODO: maybe have a cleaner way to cast the input (from `ImageProcessor` side?)\n./.venv/lib/python3.13/site-packages/transformers/models/sew_d/modeling_sew_d.py:        # TODO: We should check if the opset_version being used to export\n./.venv/lib/python3.13/site-packages/transformers/models/deformable_detr/modeling_deformable_detr.py:        # TODO find a better way of exposing other arguments\n./.venv/lib/python3.13/site-packages/transformers/models/deformable_detr/modeling_deformable_detr.py:            # TODO: valid_ratios could be useless here. check https://github.com/fundamentalvision/Deformable-DETR/issues/36\n./.venv/lib/python3.13/site-packages/transformers/models/deformable_detr/image_processing_deformable_detr.py:    # POSTPROCESSING METHODS - TODO: add support for other frameworks\n./.venv/lib/python3.13/site-packages/transformers/models/vision_text_dual_encoder/modeling_vision_text_dual_encoder.py:                # TODO: Should we use the pre-trained projection as well ?\n./.venv/lib/python3.13/site-packages/transformers/models/qwen2_vl/configuration_qwen2_vl.py:        # TODO: @raushan update config in the hub\n./.venv/lib/python3.13/site-packages/transformers/models/unispeech/modular_unispeech.py:        >>> # TODO: Add full pretraining example\n./.venv/lib/python3.13/site-packages/transformers/models/unispeech/modular_unispeech.py:        # TODO(PVP) - add negative sampling & loss computation\n./.venv/lib/python3.13/site-packages/transformers/models/unispeech/modeling_unispeech.py:        # TODO: we need a refactor so that the different attention modules can get their specific kwargs\n./.venv/lib/python3.13/site-packages/transformers/models/unispeech/modeling_unispeech.py:        >>> # TODO: Add full pretraining example\n./.venv/lib/python3.13/site-packages/transformers/models/unispeech/modeling_unispeech.py:        # TODO(PVP) - add negative sampling & loss computation\n./.venv/lib/python3.13/site-packages/transformers/models/marian/modeling_marian.py:        # TODO: we need a refactor so that the different attention modules can get their specific kwargs\n./.venv/lib/python3.13/site-packages/transformers/models/marian/configuration_marian.py:            # TODO: figure this case out.\n./.venv/lib/python3.13/site-packages/transformers/models/marian/configuration_marian.py:            # TODO: test this.\n./.venv/lib/python3.13/site-packages/transformers/models/gemma3/modular_gemma3.py:        # TODO: raushan fix this after RoPE refactor. For now we hack it by reassigning thetas\n./.venv/lib/python3.13/site-packages/transformers/models/gemma3/modeling_gemma3.py:        # TODO: raushan fix this after RoPE refactor. For now we hack it by reassigning thetas\n./.venv/lib/python3.13/site-packages/transformers/models/instructblipvideo/modeling_instructblipvideo.py:    _keep_in_fp32_modules = [\"query_tokens\"]  # TODO @ArthurZucker I don't know why this is required for FP8\n./.venv/lib/python3.13/site-packages/transformers/models/instructblipvideo/modeling_instructblipvideo.py:    _keep_in_fp32_modules = [\"query_tokens\"]  # TODO @ArthurZucker I don't know why this is required for FP8\n./.venv/lib/python3.13/site-packages/transformers/models/instructblipvideo/image_processing_instructblipvideo.py:# TODO (raushan): processor can be removed after v5 release. Kept for backwards compatibility\n./.venv/lib/python3.13/site-packages/transformers/models/videomae/modeling_videomae.py:    # TODO: make it with torch instead of numpy\n./.venv/lib/python3.13/site-packages/transformers/models/gptj/modeling_gptj.py:        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n./.venv/lib/python3.13/site-packages/transformers/models/gptj/modeling_gptj.py:        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n./.venv/lib/python3.13/site-packages/transformers/models/gptj/configuration_gptj.py:            # TODO: how to do that better?\n./.venv/lib/python3.13/site-packages/transformers/models/gpt_sw3/tokenization_gpt_sw3.py:                # TODO: Check if this is needed, as it ensures that decode(encode(doc)) != doc by adding extra whitespace in the decoded document\n./.venv/lib/python3.13/site-packages/transformers/models/mobilevit/image_processing_mobilevit.py:        # TODO: add support for other frameworks\n./.venv/lib/python3.13/site-packages/transformers/models/seamless_m4t_v2/configuration_seamless_m4t_v2.py:        self.t2u_variance_predictor_embed_dim = t2u_variance_predictor_embed_dim  # TODO: add to docstrings\n./.venv/lib/python3.13/site-packages/transformers/models/seamless_m4t_v2/configuration_seamless_m4t_v2.py:        self.t2u_variance_predictor_hidden_dim = t2u_variance_predictor_hidden_dim  # TODO: add to docstrings\n./.venv/lib/python3.13/site-packages/transformers/models/seamless_m4t_v2/configuration_seamless_m4t_v2.py:        self.t2u_variance_predictor_kernel_size = t2u_variance_predictor_kernel_size  # TODO: add to docstrings\n./.venv/lib/python3.13/site-packages/transformers/models/seamless_m4t_v2/configuration_seamless_m4t_v2.py:        self.t2u_variance_pred_dropout = t2u_variance_pred_dropout  # TODO: add to docstrings\n./.venv/lib/python3.13/site-packages/transformers/models/gpt_neo/modeling_gpt_neo.py:        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n./.venv/lib/python3.13/site-packages/transformers/models/gpt_neo/modeling_gpt_neo.py:    _can_compile_fullgraph = False  # TODO: needs a hybrid cache\n./.venv/lib/python3.13/site-packages/transformers/models/gpt_neo/modeling_gpt_neo.py:        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n./.venv/lib/python3.13/site-packages/transformers/models/mm_grounding_dino/modeling_mm_grounding_dino.py:            # TODO: valid_ratios could be useless here. check https://github.com/fundamentalvision/Deformable-DETR/issues/36\n./.venv/lib/python3.13/site-packages/transformers/models/byt5/tokenization_byt5.py:            additional_special_tokens=additional_special_tokens,  # TODO extra ids are not used :sweatywmile:\n./.venv/lib/python3.13/site-packages/transformers/models/dpt/image_processing_dpt_fast.py:        # TODO: add support for other frameworks\n./.venv/lib/python3.13/site-packages/transformers/models/dpt/image_processing_dpt.py:        # TODO: add support for other frameworks\n./.venv/lib/python3.13/site-packages/transformers/models/owlv2/image_processing_owlv2_fast.py:        # TODO: (amy) add support for other frameworks\n./.venv/lib/python3.13/site-packages/transformers/models/t5gemma/modeling_t5gemma.py:        # TODO: support intialization for encoders and decoders separately(?)\n./.venv/lib/python3.13/site-packages/transformers/models/t5gemma/modular_t5gemma.py:        # TODO: support intialization for encoders and decoders separately(?)\n./.venv/lib/python3.13/site-packages/transformers/models/persimmon/modeling_persimmon.py:        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n./.venv/lib/python3.13/site-packages/transformers/models/mt5/modeling_mt5.py:            # TODO(thom): Add z_loss https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/layers.py#L666\n./.venv/lib/python3.13/site-packages/kubernetes/dynamic/resource.py:    #TODO(fabianvf): Determine proper way to handle differences between resources + subresources\n./.venv/lib/python3.13/site-packages/kubernetes/stream/ws_client.py:        TODO: Maybe we can process this and return a more meaningful map with\n./.venv/lib/python3.13/site-packages/kubernetes/watch/watch.py:            # TODO: make use of the resource_version in BOOKMARK event for more\n./.venv/lib/python3.13/site-packages/numpy/__init__.py:    # TODO: Remove the environment variable entirely now that it is \"weak\"\n./.venv/lib/python3.13/site-packages/numpy/lib/_index_tricks_impl.pyi:    # TODO(jorenham): annotate this\n./.venv/lib/python3.13/site-packages/numpy/lib/_function_base_impl.py:    # TODO: This preserves the Python int, float, complex manually to get the\n./.venv/lib/python3.13/site-packages/numpy/lib/mixins.py:    # TODO: handle the optional third argument for __pow__?\n./.venv/lib/python3.13/site-packages/numpy/lib/_user_array_impl.pyi:    # TODO(jorenham): complete these binary ops\n./.venv/lib/python3.13/site-packages/numpy/lib/_datasource.py:# TODO: .zip support, .tar support?\n./.venv/lib/python3.13/site-packages/numpy/lib/_datasource.py:        # TODO: Doesn't handle compressed files!\n./.venv/lib/python3.13/site-packages/numpy/lib/_datasource.py:        # TODO:  This should be more robust.  Handles case where path includes\n./.venv/lib/python3.13/site-packages/numpy/lib/_datasource.py:        # TODO: There is no support for opening a file for writing which\n./.venv/lib/python3.13/site-packages/numpy/lib/_datasource.py:        # TODO: Add a ``subdir`` parameter for specifying the subdirectory\n./.venv/lib/python3.13/site-packages/numpy/lib/_nanfunctions_impl.py:    # TODO: What to do when arr1d = [1, np.nan] and weights = [0, 1]?\n./.venv/lib/python3.13/site-packages/numpy/lib/recfunctions.pyi:# TODO(jorenham): Stop passing `void` directly once structured dtypes are implemented,\n./.venv/lib/python3.13/site-packages/numpy/lib/recfunctions.pyi:# TODO(jorenham): Attempt shape-typing (return type has ndim == arr.ndim + 1)\n./.venv/lib/python3.13/site-packages/numpy/lib/recfunctions.pyi:# TODO(jorenham): Attempt shape-typing\n./.venv/lib/python3.13/site-packages/numpy/lib/tests/test_io.py:                sup.filter(ResourceWarning)  # TODO: specify exact message\n./.venv/lib/python3.13/site-packages/numpy/lib/tests/test_function_base.py:        # TODO: Note that times have dubious rounding as of fixing NaTs!\n./.venv/lib/python3.13/site-packages/numpy/lib/tests/test_function_base.py:        # TODO: Median does not support Datetime, due to `mean`.\n./.venv/lib/python3.13/site-packages/numpy/lib/_arraypad_impl.pyi:# TODO: In practice each keyword argument is exclusive to one or more\n./.venv/lib/python3.13/site-packages/numpy/polynomial/_polybase.py:        # TODO: we're stuck with disabling math formatting until we handle\n./.venv/lib/python3.13/site-packages/numpy/polynomial/_polybase.pyi:    # TODO: Once `_ShapeT@ndarray` is covariant and bounded (see #26081),\n./.venv/lib/python3.13/site-packages/numpy/fft/__init__.py:# TODO: `numpy.fft.helper`` was deprecated in NumPy 2.0. It should\n./.venv/lib/python3.13/site-packages/numpy/ctypeslib/_ctypeslib.pyi:# TODO: Add a proper `_Shape` bound once we've got variadic typevars\n./.venv/lib/python3.13/site-packages/numpy/ctypeslib/_ctypeslib.pyi:# TODO: Add a shape typevar once we have variadic typevars (PEP 646)\n./.venv/lib/python3.13/site-packages/numpy/typing/tests/data/fail/bitwise_ops.pyi:# TODO: Certain mixes like i4 << u8 go to float and thus should fail\n./.venv/lib/python3.13/site-packages/numpy/typing/tests/data/fail/lib_function_base.pyi:# TODO: enable these once mypy actually supports ParamSpec (released in 2021)\n./.venv/lib/python3.13/site-packages/numpy/typing/tests/data/fail/scalars.pyi:# TODO: protocols (can't check for non-existent protocols w/ __getattr__)\n./.venv/lib/python3.13/site-packages/numpy/typing/tests/data/reveal/fromnumeric.pyi:# TODO: array_bdd tests for np.put()\n./.venv/lib/python3.13/site-packages/numpy/_core/arrayprint.py:        # TODO: Custom repr for user DTypes, logic should likely move.\n./.venv/lib/python3.13/site-packages/numpy/_core/_internal.pyi:# TODO: Let the likes of `shape_as` and `strides_as` return `None`\n./.venv/lib/python3.13/site-packages/numpy/_core/_dtype.py:        # TODO: this path can never be reached\n./.venv/lib/python3.13/site-packages/numpy/_core/_dtype.py:    # TODO: this duplicates the C metastr_to_unicode functionality\n./.venv/lib/python3.13/site-packages/numpy/_core/numeric.py:    # TODO: this works around .astype(bool) not working properly (gh-9847)\n./.venv/lib/python3.13/site-packages/numpy/_core/getlimits.py:    TODO: MachAr should be retired completely ideally.  We currently only\n./.venv/lib/python3.13/site-packages/numpy/_core/fromnumeric.py:    # but this follows what was done before. TODO: revisit this.\n./.venv/lib/python3.13/site-packages/numpy/_core/fromnumeric.pyi:# TODO: `np.prod()``: For object arrays `initial` does not necessarily\n./.venv/lib/python3.13/site-packages/numpy/_core/numeric.pyi:# TODO: Add overloads for bool, int, float, complex, str, bytes, and memoryview\n./.venv/lib/python3.13/site-packages/numpy/_core/multiarray.pyi:# TODO: Sort out any and all missing functions in this namespace\n./.venv/lib/python3.13/site-packages/numpy/_core/einsumfunc.pyi:# TODO: Properly handle the `casting`-based combinatorics\n./.venv/lib/python3.13/site-packages/numpy/_core/einsumfunc.pyi:# TODO: We need to evaluate the content `__subscripts` in order\n./.venv/lib/python3.13/site-packages/numpy/_core/tests/test_scalarmath.py:        # TODO: It would be nice to resolve this issue.\n./.venv/lib/python3.13/site-packages/numpy/_core/tests/test_scalarmath.py:    # TODO: Power is a bit special, but here mostly bools seem to behave oddly\n./.venv/lib/python3.13/site-packages/numpy/_core/tests/test_stringdtype.py:    # TODO: generalize to more ufuncs\n./.venv/lib/python3.13/site-packages/numpy/_core/tests/test_multiarray.py:# TODO: test for multidimensional\n./.venv/lib/python3.13/site-packages/numpy/_core/tests/test_datetime.py:        # TODO: Allowing unsafe casting by\n./.venv/lib/python3.13/site-packages/numpy/_core/tests/test_datetime.py:        # TODO: add absolute (gold standard) time span limit strings\n./.venv/lib/python3.13/site-packages/numpy/_core/tests/test_casting_unittests.py:        # TODO: While this test is fairly thorough, right now, it does not\n./.venv/lib/python3.13/site-packages/numpy/_core/tests/examples/limited_api/meson.build:# TODO: This is a hack due to https://github.com/cython/cython/issues/5820,\n./.venv/lib/python3.13/site-packages/numpy/_core/tests/examples/cython/meson.build:# TODO: This is a hack due to gh-25135, where cython may not find the right\n./.venv/lib/python3.13/site-packages/numpy/_core/tests/test_array_coercion.py:        # TODO: This discrepancy _should_ be resolved, either by relaxing the\n./.venv/lib/python3.13/site-packages/numpy/_core/tests/test_array_coercion.py:    # TODO: This is arguably weird/wrong, but seems old:\n./.venv/lib/python3.13/site-packages/numpy/_core/tests/test_umath_complex.py:# TODO: branch cuts (use Pauli code)\n./.venv/lib/python3.13/site-packages/numpy/_core/tests/test_umath_complex.py:# TODO: conj 'symmetry'\n./.venv/lib/python3.13/site-packages/numpy/_core/tests/test_umath_complex.py:# TODO: FPU exceptions\n./.venv/lib/python3.13/site-packages/numpy/_core/tests/test_umath_complex.py:# TODO: replace with a check on whether platform-provided C99 funcs are used\n./.venv/lib/python3.13/site-packages/numpy/_core/tests/test_umath_complex.py:# TODO This can be xfail when the generator functions are got rid of.\n./.venv/lib/python3.13/site-packages/numpy/_core/tests/test_umath_complex.py:    # TODO This can be xfail when the generator functions are got rid of.\n./.venv/lib/python3.13/site-packages/numpy/_core/tests/test_umath_complex.py:    # TODO This can be xfail when the generator functions are got rid of.\n./.venv/lib/python3.13/site-packages/numpy/_core/_add_newdocs_scalars.py:# TODO: These docs probably need an if to highlight the default rather than\n./.venv/lib/python3.13/site-packages/numpy/_core/_add_newdocs_scalars.py:# TODO: work out how to put this on the base class, np.floating\n./.venv/lib/python3.13/site-packages/numpy/_core/_methods.py:        # TODO: Optimize case when `where` is broadcast along a non-reduction\n./.venv/lib/python3.13/site-packages/numpy/_core/_add_newdocs.py:        assignment examples; TODO).\n./.venv/lib/python3.13/site-packages/numpy/_core/include/numpy/_public_dtype_api_table.h: * TODO: This file should be consolidated with the API table generation\n./.venv/lib/python3.13/site-packages/numpy/_core/include/numpy/dtype_api.h: * TODO: Due to the fact that `resolve_descriptors` is also used for `can_cast`\n./.venv/lib/python3.13/site-packages/numpy/_core/include/numpy/dtype_api.h:// TODO: These slots probably still need some thought, and/or a way to \"grow\"?\n./.venv/lib/python3.13/site-packages/numpy/_core/include/numpy/dtype_api.h: * TODO: These two functions are currently only used for experimental DType\n./.venv/lib/python3.13/site-packages/numpy/_core/include/numpy/random/libdivide.h:        // TODO: do something better than 128 bit math\n./.venv/lib/python3.13/site-packages/numpy/_core/include/numpy/random/libdivide.h:        // TODO: do something better than 128 bit math\n./.venv/lib/python3.13/site-packages/numpy/_core/include/numpy/ndarraytypes.h:    /* TODO: Make this definition public in the API, as soon as its settled */\n./.venv/lib/python3.13/site-packages/numpy/ma/core.py:        # TODO: This is probably a mess, but should best preserve behavior?\n./.venv/lib/python3.13/site-packages/numpy/ma/core.py:        # TODO: It seems better to always store a valid fill_value, the oddity\n./.venv/lib/python3.13/site-packages/numpy/ma/core.py:        # TODO: We don't actually support K, so use A instead.  We could\n./.venv/lib/python3.13/site-packages/numpy/ma/tests/test_core.py:    # TODO: Test masked_object, masked_equal, ...\n./.venv/lib/python3.13/site-packages/numpy/ma/tests/test_old_ma.py:        # TODO FIXME: Find out what the following raises a warning in r8247\n./.venv/lib/python3.13/site-packages/numpy/dtypes.pyi:    # TODO: Once implemented, don't forget the`unit: L[\"μs\"]` overload.\n./.venv/lib/python3.13/site-packages/numpy/dtypes.pyi:    # TODO: Once implemented, don't forget to overload on `unit: L[\"μs\"]`.\n./.venv/lib/python3.13/site-packages/numpy/dtypes.pyi:    # TODO(jorenham): change once we have a string scalar type:\n./.venv/lib/python3.13/site-packages/numpy/random/_generator.pyi:    # TODO: Use a TypeVar _T here to get away from Any output?\n./.venv/lib/python3.13/site-packages/numpy/random/tests/test_random.py:    # TODO: Include test for randint once it can broadcast\n./.venv/lib/python3.13/site-packages/numpy/__init__.pyi:    # TODO: `__setitem__` operates via `unsafe` casting rules, and can\n./.venv/lib/python3.13/site-packages/numpy/__init__.pyi:    # TODO: How to deal with the non-commutative nature of `==` and `!=`?\n./.venv/lib/python3.13/site-packages/numpy/__init__.pyi:    # TODO: Annotate kwargs with an unpacked `TypedDict`\n./.venv/lib/python3.13/site-packages/numpy/__init__.pyi:    # TODO: Uncomment once https://github.com/python/mypy/issues/14070 is fixed\n./.venv/lib/python3.13/site-packages/numpy/__init__.pyi:    # TODO: Support the \"1d @ 1d -> scalar\" case\n./.venv/lib/python3.13/site-packages/numpy/__init__.pyi:# TODO: The type of each `__next__` and `iters` return-type depends\n./.venv/lib/python3.13/site-packages/numpy/__init__.pyi:# TODO: Add a mypy plugin for managing functions whose output type is dependent\n./.venv/lib/python3.13/site-packages/numpy/_typing/_array_like.py:# TODO: Wait until mypy supports recursive objects in combination with typevars\n./.venv/lib/python3.13/site-packages/numpy/_typing/_char_codes.py:    # TODO: add `_StringCodes` once it has a scalar type\n./.venv/lib/python3.13/site-packages/numpy/_typing/_dtype_like.py:_DTypeLikeNested: TypeAlias = Any  # TODO: wait for support for recursive types\n./.venv/lib/python3.13/site-packages/numpy/linalg/_linalg.pyi:    # TODO: `sign` and `logabsdet` are scalars for input 2D arrays and\n./.venv/lib/python3.13/site-packages/numpy/linalg/_linalg.pyi:# TODO: The supported input and output dtypes are dependent on the value of `n`.\n./.venv/lib/python3.13/site-packages/numpy/linalg/_linalg.pyi:# TODO: Returns a scalar for 2D arrays and\n./.venv/lib/python3.13/site-packages/numpy/linalg/_linalg.pyi:# TODO: Returns `int` for <2D arrays and `intp` otherwise\n./.venv/lib/python3.13/site-packages/numpy/linalg/_linalg.pyi:# TODO: Returns a 2-tuple of scalars for 2D arrays and\n./.venv/lib/python3.13/site-packages/numpy/linalg/_linalg.pyi:# TODO: Returns a 2-tuple of scalars for 2D arrays and\n./.venv/lib/python3.13/site-packages/numpy/linalg/_linalg.pyi:# TODO: Returns a scalar or array\n./.venv/lib/python3.13/site-packages/numpy/f2py/cfuncs.py:        /* TODO: change the type of `len` so that we can remove this */\n./.venv/lib/python3.13/site-packages/numpy/f2py/cfuncs.py:            // TODO: update when numpy will support 1-byte and\n./.venv/lib/python3.13/site-packages/numpy/f2py/cfuncs.py:        /* TODO: This error (and most other) error handling needs cleaning. */\n./.venv/lib/python3.13/site-packages/numpy/f2py/cfuncs.py:# TODO: These should be dynamically generated, too many mapped to int things,\n./.venv/lib/python3.13/site-packages/numpy/f2py/_isocbind.py:# TODO: See gh-25229\n./.venv/lib/python3.13/site-packages/numpy/f2py/f2py2e.py:    # TODO: Remove all this when scaninputline is replaced\n./.venv/lib/python3.13/site-packages/numpy/f2py/f2py2e.py:    # TODO: Once distutils is dropped completely, i.e. min_ver >= 3.12, unify into --fflags\n./.venv/lib/python3.13/site-packages/numpy/f2py/src/fortranobject.c:  // TODO: detect the size of buf and make sure that size(buf) >= size(localbuf).\n./.venv/lib/python3.13/site-packages/numpy/f2py/capi_maps.py:    # TODO: support Fortran `len` function with optional kind parameter\n./.venv/lib/python3.13/site-packages/numpy/f2py/capi_maps.py:        # TODO: Evaluate intent_flags here.\n./.venv/lib/python3.13/site-packages/numpy/f2py/crackfortran.py:TODO:\n./.venv/lib/python3.13/site-packages/numpy/f2py/crackfortran.py:    TODO:\n./.venv/lib/python3.13/site-packages/numpy/f2py/crackfortran.py:                    # TODO: test .eq., .neq., etc replacements.\n./.venv/lib/python3.13/site-packages/numpy/f2py/crackfortran.py:                outmess(f'get_parameters[TODO]: '\n./.venv/lib/python3.13/site-packages/numpy/f2py/crackfortran.py:        # TODO: use symbolic from PR #19805\n./.venv/lib/python3.13/site-packages/numpy/f2py/tests/test_docs.py:    # TODO: implement test methods for other example Fortran codes\n./.venv/lib/python3.13/site-packages/numpy/f2py/tests/test_f2py2e.py:    # TODO: Clean up to prevent passing --overwrite-signature\n./.venv/lib/python3.13/site-packages/numpy/f2py/tests/test_f2py2e.py:    TODO: Test to ensure this has no effect without --latex-doc\n./.venv/lib/python3.13/site-packages/numpy/f2py/tests/test_f2py2e.py:    TODO: Document this in the help string\n./.venv/lib/python3.13/site-packages/numpy/f2py/tests/test_f2py2e.py:    # TODO: populate\n./.venv/lib/python3.13/site-packages/numpy/f2py/tests/test_f2py2e.py:    # TODO: populate\n./.venv/lib/python3.13/site-packages/numpy/f2py/tests/test_f2py2e.py:    # TODO: f2py2e should not call sys.exit() after printing the version\n./.venv/lib/python3.13/site-packages/numpy/f2py/tests/test_f2py2e.py:# TODO: These should be tested separately\n./.venv/lib/python3.13/site-packages/numpy/f2py/tests/test_f2py2e.py:    # TODO: populate\n./.venv/lib/python3.13/site-packages/numpy/f2py/tests/test_f2py2e.py:    # TODO: populate\n./.venv/lib/python3.13/site-packages/numpy/f2py/tests/test_f2py2e.py:    # TODO: populate\n./.venv/lib/python3.13/site-packages/numpy/f2py/tests/test_f2py2e.py:    # TODO: populate\n./.venv/lib/python3.13/site-packages/numpy/f2py/tests/test_f2py2e.py:    # TODO: populate\n./.venv/lib/python3.13/site-packages/numpy/f2py/tests/test_f2py2e.py:    # TODO: populate\n./.venv/lib/python3.13/site-packages/numpy/f2py/tests/test_f2py2e.py:    # TODO: populate\n./.venv/lib/python3.13/site-packages/numpy/f2py/tests/test_f2py2e.py:    # TODO: populate\n./.venv/lib/python3.13/site-packages/numpy/f2py/tests/test_f2py2e.py:    # TODO: populate\n./.venv/lib/python3.13/site-packages/numpy/f2py/tests/test_f2py2e.py:    # TODO: populate\n./.venv/lib/python3.13/site-packages/numpy/f2py/tests/test_f2py2e.py:    # TODO: populate\n./.venv/lib/python3.13/site-packages/numpy/f2py/tests/test_f2py2e.py:    # TODO: populate\n./.venv/lib/python3.13/site-packages/numpy/f2py/tests/test_f2py2e.py:    # TODO: populate\n./.venv/lib/python3.13/site-packages/numpy/f2py/tests/test_f2py2e.py:    # TODO: populate\n./.venv/lib/python3.13/site-packages/numpy/f2py/tests/test_f2py2e.py:    # TODO: populate\n./.venv/lib/python3.13/site-packages/numpy/f2py/tests/test_f2py2e.py:    # TODO: populate\n./.venv/lib/python3.13/site-packages/numpy/f2py/tests/test_f2py2e.py:    # TODO: populate\n./.venv/lib/python3.13/site-packages/numpy/f2py/tests/test_f2py2e.py:    # TODO: populate\n./.venv/lib/python3.13/site-packages/numpy/f2py/symbolic.py:# TODO: support logical constants (Op.BOOLEAN)\n./.venv/lib/python3.13/site-packages/numpy/f2py/symbolic.py:# TODO: support logical operators (.AND., ...)\n./.venv/lib/python3.13/site-packages/numpy/f2py/symbolic.py:# TODO: support defined operators (.MYOP., ...)\n./.venv/lib/python3.13/site-packages/numpy/f2py/symbolic.py:                # TODO: other kind not used\n./.venv/lib/python3.13/site-packages/numpy/f2py/symbolic.py:        # TODO: implement a method for deciding when __call__ should\n./.venv/lib/python3.13/site-packages/numpy/f2py/symbolic.py:            # TODO: determine correct kind\n./.venv/lib/python3.13/site-packages/numpy/f2py/symbolic.py:            # TODO: determine correct kind\n./.venv/lib/python3.13/site-packages/numpy/f2py/symbolic.py:            # TODO: denom kind not used\n./.venv/lib/python3.13/site-packages/numpy/f2py/symbolic.py:            # TODO: find common divisor of coefficients\n./.venv/lib/python3.13/site-packages/jinja2/ext.py:    # TODO: the i18n extension is currently reevaluating values in a few\n./.venv/lib/python3.13/site-packages/typing_inspection/introspection.py:# TODO at some point, we could switch to an enum flag, so that multiple sources\n./.venv/lib/python3.13/site-packages/typing_inspection/introspection.py:    # TODO if/when https://peps.python.org/pep-0767/ is accepted, add 'read_only'\n./.venv/lib/python3.13/site-packages/typing_inspection/introspection.py:        # TODO use a match statement when Python 3.9 support is dropped.\n./.venv/lib/python3.13/site-packages/setuptools/config/setupcfg.py:        # TODO: Temporary cast until mypy 1.12 is released with upstream fixes from typeshed\n./.venv/lib/python3.13/site-packages/setuptools/config/setupcfg.py:                # TODO: define due date, see setuptools.dist:check_nsp.\n./.venv/lib/python3.13/site-packages/setuptools/config/setupcfg.py:    # TODO: should we include due_date here? Initially introduced in 6 Aug 2022.\n./.venv/lib/python3.13/site-packages/setuptools/config/_apply_pyprojecttoml.py:        # TODO: remove check when `bdist_wheel` has been fully removed from pypa/wheel\n./.venv/lib/python3.13/site-packages/setuptools/config/_apply_pyprojecttoml.py:    # TODO: Consider removing this check in the future?\n./.venv/lib/python3.13/site-packages/setuptools/config/_validate_pyproject/fastjsonschema_validations.py:        raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \" must be object\", value=data, name=\"\" + (name_prefix or \"data\") + \"\", definition={'$schema': 'http://json-schema.org/draft-07/schema#', '$id': 'https://packaging.python.org/en/latest/specifications/declaring-build-dependencies/', 'title': 'Data structure for ``pyproject.toml`` files', '$$description': ['File format containing build-time configurations for the Python ecosystem. ', ':pep:`517` initially defined a build-system independent format for source trees', 'which was complemented by :pep:`518` to provide a way of specifying dependencies ', 'for building Python projects.', 'Please notice the ``project`` table (as initially defined in  :pep:`621`) is not included', 'in this schema and should be considered separately.'], 'type': 'object', 'additionalProperties': False, 'properties': {'build-system': {'type': 'object', 'description': 'Table used to store build-related data', 'additionalProperties': False, 'properties': {'requires': {'type': 'array', '$$description': ['List of dependencies in the :pep:`508` format required to execute the build', 'system. Please notice that the resulting dependency graph', '**MUST NOT contain cycles**'], 'items': {'type': 'string'}}, 'build-backend': {'type': 'string', 'description': 'Python object that will be used to perform the build according to :pep:`517`', 'format': 'pep517-backend-reference'}, 'backend-path': {'type': 'array', '$$description': ['List of directories to be prepended to ``sys.path`` when loading the', 'back-end, and running its hooks'], 'items': {'type': 'string', '$comment': 'Should be a path (TODO: enforce it with format?)'}}}, 'required': ['requires']}, 'project': {'$schema': 'http://json-schema.org/draft-07/schema#', '$id': 'https://packaging.python.org/en/latest/specifications/pyproject-toml/', 'title': 'Package metadata stored in the ``project`` table', '$$description': ['Data structure for the **project** table inside ``pyproject.toml``', '(as initially defined in :pep:`621`)'], 'type': 'object', 'properties': {'name': {'type': 'string', 'description': 'The name (primary identifier) of the project. MUST be statically defined.', 'format': 'pep508-identifier'}, 'version': {'type': 'string', 'description': 'The version of the project as supported by :pep:`440`.', 'format': 'pep440'}, 'description': {'type': 'string', '$$description': ['The `summary description of the project', '<https://packaging.python.org/specifications/core-metadata/#summary>`_']}, 'readme': {'$$description': ['`Full/detailed description of the project in the form of a README', '<https://peps.python.org/pep-0621/#readme>`_', \"with meaning similar to the one defined in `core metadata's Description\", '<https://packaging.python.org/specifications/core-metadata/#description>`_'], 'oneOf': [{'type': 'string', '$$description': ['Relative path to a text file (UTF-8) containing the full description', 'of the project. If the file path ends in case-insensitive ``.md`` or', '``.rst`` suffixes, then the content-type is respectively', '``text/markdown`` or ``text/x-rst``']}, {'type': 'object', 'allOf': [{'anyOf': [{'properties': {'file': {'type': 'string', '$$description': ['Relative path to a text file containing the full description', 'of the project.']}}, 'required': ['file']}, {'properties': {'text': {'type': 'string', 'description': 'Full text describing the project.'}}, 'required': ['text']}]}, {'properties': {'content-type': {'type': 'string', '$$description': ['Content-type (:rfc:`1341`) of the full description', '(e.g. ``text/markdown``). The ``charset`` parameter is assumed', 'UTF-8 when not present.'], '$comment': 'TODO: add regex pattern or format?'}}, 'required': ['content-type']}]}]}, 'requires-python': {'type': 'string', 'format': 'pep508-versionspec', '$$description': ['`The Python version requirements of the project', '<https://packaging.python.org/specifications/core-metadata/#requires-python>`_.']}, 'license': {'description': '`Project license <https://peps.python.org/pep-0621/#license>`_.', 'oneOf': [{'type': 'string', 'description': 'An SPDX license identifier', 'format': 'SPDX'}, {'type': 'object', 'properties': {'file': {'type': 'string', '$$description': ['Relative path to the file (UTF-8) which contains the license for the', 'project.']}}, 'required': ['file']}, {'type': 'object', 'properties': {'text': {'type': 'string', '$$description': ['The license of the project whose meaning is that of the', '`License field from the core metadata', '<https://packaging.python.org/specifications/core-metadata/#license>`_.']}}, 'required': ['text']}]}, 'license-files': {'description': 'Paths or globs to paths of license files', 'type': 'array', 'items': {'type': 'string'}}, 'authors': {'type': 'array', 'items': {'$ref': '#/definitions/author'}, '$$description': [\"The people or organizations considered to be the 'authors' of the project.\", 'The exact meaning is open to interpretation (e.g. original or primary authors,', 'current maintainers, or owners of the package).']}, 'maintainers': {'type': 'array', 'items': {'$ref': '#/definitions/author'}, '$$description': [\"The people or organizations considered to be the 'maintainers' of the project.\", 'Similarly to ``authors``, the exact meaning is open to interpretation.']}, 'keywords': {'type': 'array', 'items': {'type': 'string'}, 'description': 'List of keywords to assist searching for the distribution in a larger catalog.'}, 'classifiers': {'type': 'array', 'items': {'type': 'string', 'format': 'trove-classifier', 'description': '`PyPI classifier <https://pypi.org/classifiers/>`_.'}, '$$description': ['`Trove classifiers <https://pypi.org/classifiers/>`_', 'which apply to the project.']}, 'urls': {'type': 'object', 'description': 'URLs associated with the project in the form ``label => value``.', 'additionalProperties': False, 'patternProperties': {'^.+$': {'type': 'string', 'format': 'url'}}}, 'scripts': {'$ref': '#/definitions/entry-point-group', '$$description': ['Instruct the installer to create command-line wrappers for the given', '`entry points <https://packaging.python.org/specifications/entry-points/>`_.']}, 'gui-scripts': {'$ref': '#/definitions/entry-point-group', '$$description': ['Instruct the installer to create GUI wrappers for the given', '`entry points <https://packaging.python.org/specifications/entry-points/>`_.', 'The difference between ``scripts`` and ``gui-scripts`` is only relevant in', 'Windows.']}, 'entry-points': {'$$description': ['Instruct the installer to expose the given modules/functions via', '``entry-point`` discovery mechanism (useful for plugins).', 'More information available in the `Python packaging guide', '<https://packaging.python.org/specifications/entry-points/>`_.'], 'propertyNames': {'format': 'python-entrypoint-group'}, 'additionalProperties': False, 'patternProperties': {'^.+$': {'$ref': '#/definitions/entry-point-group'}}}, 'dependencies': {'type': 'array', 'description': 'Project (mandatory) dependencies.', 'items': {'$ref': '#/definitions/dependency'}}, 'optional-dependencies': {'type': 'object', 'description': 'Optional dependency for the project', 'propertyNames': {'format': 'pep508-identifier'}, 'additionalProperties': False, 'patternProperties': {'^.+$': {'type': 'array', 'items': {'$ref': '#/definitions/dependency'}}}}, 'dynamic': {'type': 'array', '$$description': ['Specifies which fields are intentionally unspecified and expected to be', 'dynamically provided by build tools'], 'items': {'enum': ['version', 'description', 'readme', 'requires-python', 'license', 'license-files', 'authors', 'maintainers', 'keywords', 'classifiers', 'urls', 'scripts', 'gui-scripts', 'entry-points', 'dependencies', 'optional-dependencies']}}}, 'required': ['name'], 'additionalProperties': False, 'allOf': [{'if': {'not': {'required': ['dynamic'], 'properties': {'dynamic': {'contains': {'const': 'version'}, '$$description': ['version is listed in ``dynamic``']}}}, '$$comment': ['According to :pep:`621`:', '    If the core metadata specification lists a field as \"Required\", then', '    the metadata MUST specify the field statically or list it in dynamic', 'In turn, `core metadata`_ defines:', '    The required fields are: Metadata-Version, Name, Version.', '    All the other fields are optional.', 'Since ``Metadata-Version`` is defined by the build back-end, ``name`` and', '``version`` are the only mandatory information in ``pyproject.toml``.', '.. _core metadata: https://packaging.python.org/specifications/core-metadata/']}, 'then': {'required': ['version'], '$$description': ['version should be statically defined in the ``version`` field']}}, {'if': {'required': ['license-files']}, 'then': {'properties': {'license': {'type': 'string'}}}}], 'definitions': {'author': {'$id': '#/definitions/author', 'title': 'Author or Maintainer', '$comment': 'https://peps.python.org/pep-0621/#authors-maintainers', 'type': 'object', 'additionalProperties': False, 'properties': {'name': {'type': 'string', '$$description': ['MUST be a valid email name, i.e. whatever can be put as a name, before an', 'email, in :rfc:`822`.']}, 'email': {'type': 'string', 'format': 'idn-email', 'description': 'MUST be a valid email address'}}}, 'entry-point-group': {'$id': '#/definitions/entry-point-group', 'title': 'Entry-points', 'type': 'object', '$$description': ['Entry-points are grouped together to indicate what sort of capabilities they', 'provide.', 'See the `packaging guides', '<https://packaging.python.org/specifications/entry-points/>`_', 'and `setuptools docs', '<https://setuptools.pypa.io/en/latest/userguide/entry_point.html>`_', 'for more information.'], 'propertyNames': {'format': 'python-entrypoint-name'}, 'additionalProperties': False, 'patternProperties': {'^.+$': {'type': 'string', '$$description': ['Reference to a Python object. It is either in the form', '``importable.module``, or ``importable.module:object.attr``.'], 'format': 'python-entrypoint-reference', '$comment': 'https://packaging.python.org/specifications/entry-points/'}}}, 'dependency': {'$id': '#/definitions/dependency', 'title': 'Dependency', 'type': 'string', 'description': 'Project dependency specification according to PEP 508', 'format': 'pep508'}}}, 'tool': {'type': 'object', 'properties': {'distutils': {'$schema': 'http://json-schema.org/draft-07/schema#', '$id': 'https://setuptools.pypa.io/en/latest/deprecated/distutils/configfile.html', 'title': '``tool.distutils`` table', '$$description': ['**EXPERIMENTAL** (NOT OFFICIALLY SUPPORTED): Use ``tool.distutils``', 'subtables to configure arguments for ``distutils`` commands.', 'Originally, ``distutils`` allowed developers to configure arguments for', '``setup.py`` commands via `distutils configuration files', '<https://setuptools.pypa.io/en/latest/deprecated/distutils/configfile.html>`_.', 'See also `the old Python docs <https://docs.python.org/3.11/install/>_`.'], 'type': 'object', 'properties': {'global': {'type': 'object', 'description': 'Global options applied to all ``distutils`` commands'}}, 'patternProperties': {'.+': {'type': 'object'}}, '$comment': 'TODO: Is there a practical way of making this schema more specific?'}, 'setuptools': {'$schema': 'http://json-schema.org/draft-07/schema#', '$id': 'https://setuptools.pypa.io/en/latest/userguide/pyproject_config.html', 'title': '``tool.setuptools`` table', '$$description': ['``setuptools``-specific configurations that can be set by users that require', 'customization.', 'These configurations are completely optional and probably can be skipped when', 'creating simple packages. They are equivalent to some of the `Keywords', '<https://setuptools.pypa.io/en/latest/references/keywords.html>`_', 'used by the ``setup.py`` file, and can be set via the ``tool.setuptools`` table.', 'It considers only ``setuptools`` `parameters', '<https://setuptools.pypa.io/en/latest/userguide/pyproject_config.html#setuptools-specific-configuration>`_', 'that are not covered by :pep:`621`; and intentionally excludes ``dependency_links``', 'and ``setup_requires`` (incompatible with modern workflows/standards).'], 'type': 'object', 'additionalProperties': False, 'properties': {'platforms': {'type': 'array', 'items': {'type': 'string'}}, 'provides': {'$$description': ['Package and virtual package names contained within this package', '**(not supported by pip)**'], 'type': 'array', 'items': {'type': 'string', 'format': 'pep508-identifier'}}, 'obsoletes': {'$$description': ['Packages which this package renders obsolete', '**(not supported by pip)**'], 'type': 'array', 'items': {'type': 'string', 'format': 'pep508-identifier'}}, 'zip-safe': {'$$description': ['Whether the project can be safely installed and run from a zip file.', '**OBSOLETE**: only relevant for ``pkg_resources``, ``easy_install`` and', '``setup.py install`` in the context of ``eggs`` (**DEPRECATED**).'], 'type': 'boolean'}, 'script-files': {'$$description': ['Legacy way of defining scripts (entry-points are preferred).', 'Equivalent to the ``script`` keyword in ``setup.py``', '(it was renamed to avoid confusion with entry-point based ``project.scripts``', 'defined in :pep:`621`).', '**DISCOURAGED**: generic script wrappers are tricky and may not work properly.', 'Whenever possible, please use ``project.scripts`` instead.'], 'type': 'array', 'items': {'type': 'string'}, '$comment': 'TODO: is this field deprecated/should be removed?'}, 'eager-resources': {'$$description': ['Resources that should be extracted together, if any of them is needed,', 'or if any C extensions included in the project are imported.', '**OBSOLETE**: only relevant for ``pkg_resources``, ``easy_install`` and', '``setup.py install`` in the context of ``eggs`` (**DEPRECATED**).'], 'type': 'array', 'items': {'type': 'string'}}, 'packages': {'$$description': ['Packages that should be included in the distribution.', 'It can be given either as a list of package identifiers', 'or as a ``dict``-like structure with a single key ``find``', 'which corresponds to a dynamic call to', '``setuptools.config.expand.find_packages`` function.', 'The ``find`` key is associated with a nested ``dict``-like structure that can', 'contain ``where``, ``include``, ``exclude`` and ``namespaces`` keys,', 'mimicking the keyword arguments of the associated function.'], 'oneOf': [{'title': 'Array of Python package identifiers', 'type': 'array', 'items': {'$ref': '#/definitions/package-name'}}, {'$ref': '#/definitions/find-directive'}]}, 'package-dir': {'$$description': [':class:`dict`-like structure mapping from package names to directories where their', 'code can be found.', 'The empty string (as key) means that all packages are contained inside', 'the given directory will be included in the distribution.'], 'type': 'object', 'additionalProperties': False, 'propertyNames': {'anyOf': [{'const': ''}, {'$ref': '#/definitions/package-name'}]}, 'patternProperties': {'^.*$': {'type': 'string'}}}, 'package-data': {'$$description': ['Mapping from package names to lists of glob patterns.', 'Usually this option is not needed when using ``include-package-data = true``', 'For more information on how to include data files, check ``setuptools`` `docs', '<https://setuptools.pypa.io/en/latest/userguide/datafiles.html>`_.'], 'type': 'object', 'additionalProperties': False, 'propertyNames': {'anyOf': [{'type': 'string', 'format': 'python-module-name'}, {'const': '*'}]}, 'patternProperties': {'^.*$': {'type': 'array', 'items': {'type': 'string'}}}}, 'include-package-data': {'$$description': ['Automatically include any data files inside the package directories', 'that are specified by ``MANIFEST.in``', 'For more information on how to include data files, check ``setuptools`` `docs', '<https://setuptools.pypa.io/en/latest/userguide/datafiles.html>`_.'], 'type': 'boolean'}, 'exclude-package-data': {'$$description': ['Mapping from package names to lists of glob patterns that should be excluded', 'For more information on how to include data files, check ``setuptools`` `docs', '<https://setuptools.pypa.io/en/latest/userguide/datafiles.html>`_.'], 'type': 'object', 'additionalProperties': False, 'propertyNames': {'anyOf': [{'type': 'string', 'format': 'python-module-name'}, {'const': '*'}]}, 'patternProperties': {'^.*$': {'type': 'array', 'items': {'type': 'string'}}}}, 'namespace-packages': {'type': 'array', 'items': {'type': 'string', 'format': 'python-module-name-relaxed'}, '$comment': 'https://setuptools.pypa.io/en/latest/userguide/package_discovery.html', 'description': '**DEPRECATED**: use implicit namespaces instead (:pep:`420`).'}, 'py-modules': {'description': 'Modules that setuptools will manipulate', 'type': 'array', 'items': {'type': 'string', 'format': 'python-module-name-relaxed'}, '$comment': 'TODO: clarify the relationship with ``packages``'}, 'ext-modules': {'description': 'Extension modules to be compiled by setuptools', 'type': 'array', 'items': {'$ref': '#/definitions/ext-module'}}, 'data-files': {'$$description': ['``dict``-like structure where each key represents a directory and', 'the value is a list of glob patterns that should be installed in them.', '**DISCOURAGED**: please notice this might not work as expected with wheels.', 'Whenever possible, consider using data files inside the package directories', '(or create a new namespace package that only contains data files).', 'See `data files support', '<https://setuptools.pypa.io/en/latest/userguide/datafiles.html>`_.'], 'type': 'object', 'patternProperties': {'^.*$': {'type': 'array', 'items': {'type': 'string'}}}}, 'cmdclass': {'$$description': ['Mapping of distutils-style command names to ``setuptools.Command`` subclasses', 'which in turn should be represented by strings with a qualified class name', '(i.e., \"dotted\" form with module), e.g.::\\n\\n', '    cmdclass = {mycmd = \"pkg.subpkg.module.CommandClass\"}\\n\\n', 'The command class should be a directly defined at the top-level of the', 'containing module (no class nesting).'], 'type': 'object', 'patternProperties': {'^.*$': {'type': 'string', 'format': 'python-qualified-identifier'}}}, 'license-files': {'type': 'array', 'items': {'type': 'string'}, '$$description': ['**PROVISIONAL**: list of glob patterns for all license files being distributed.', '(likely to become standard with :pep:`639`).', \"By default: ``['LICEN[CS]E*', 'COPYING*', 'NOTICE*', 'AUTHORS*']``\"], '$comment': 'TODO: revise if PEP 639 is accepted. Probably ``project.license-files``?'}, 'dynamic': {'type': 'object', 'description': 'Instructions for loading :pep:`621`-related metadata dynamically', 'additionalProperties': False, 'properties': {'version': {'$$description': ['A version dynamically loaded via either the ``attr:`` or ``file:``', 'directives. Please make sure the given file or attribute respects :pep:`440`.', 'Also ensure to set ``project.dynamic`` accordingly.'], 'oneOf': [{'$ref': '#/definitions/attr-directive'}, {'$ref': '#/definitions/file-directive'}]}, 'classifiers': {'$ref': '#/definitions/file-directive'}, 'description': {'$ref': '#/definitions/file-directive'}, 'entry-points': {'$ref': '#/definitions/file-directive'}, 'dependencies': {'$ref': '#/definitions/file-directive-for-dependencies'}, 'optional-dependencies': {'type': 'object', 'propertyNames': {'type': 'string', 'format': 'pep508-identifier'}, 'additionalProperties': False, 'patternProperties': {'.+': {'$ref': '#/definitions/file-directive-for-dependencies'}}}, 'readme': {'type': 'object', 'anyOf': [{'$ref': '#/definitions/file-directive'}, {'type': 'object', 'properties': {'content-type': {'type': 'string'}, 'file': {'$ref': '#/definitions/file-directive/properties/file'}}, 'additionalProperties': False}], 'required': ['file']}}}}, 'definitions': {'package-name': {'$id': '#/definitions/package-name', 'title': 'Valid package name', 'description': 'Valid package name (importable or :pep:`561`).', 'type': 'string', 'anyOf': [{'type': 'string', 'format': 'python-module-name-relaxed'}, {'type': 'string', 'format': 'pep561-stub-name'}]}, 'ext-module': {'$id': '#/definitions/ext-module', 'title': 'Extension module', 'description': 'Parameters to construct a :class:`setuptools.Extension` object', 'type': 'object', 'required': ['name', 'sources'], 'additionalProperties': False, 'properties': {'name': {'type': 'string', 'format': 'python-module-name-relaxed'}, 'sources': {'type': 'array', 'items': {'type': 'string'}}, 'include-dirs': {'type': 'array', 'items': {'type': 'string'}}, 'define-macros': {'type': 'array', 'items': {'type': 'array', 'items': [{'description': 'macro name', 'type': 'string'}, {'description': 'macro value', 'oneOf': [{'type': 'string'}, {'type': 'null'}]}], 'additionalItems': False}}, 'undef-macros': {'type': 'array', 'items': {'type': 'string'}}, 'library-dirs': {'type': 'array', 'items': {'type': 'string'}}, 'libraries': {'type': 'array', 'items': {'type': 'string'}}, 'runtime-library-dirs': {'type': 'array', 'items': {'type': 'string'}}, 'extra-objects': {'type': 'array', 'items': {'type': 'string'}}, 'extra-compile-args': {'type': 'array', 'items': {'type': 'string'}}, 'extra-link-args': {'type': 'array', 'items': {'type': 'string'}}, 'export-symbols': {'type': 'array', 'items': {'type': 'string'}}, 'swig-opts': {'type': 'array', 'items': {'type': 'string'}}, 'depends': {'type': 'array', 'items': {'type': 'string'}}, 'language': {'type': 'string'}, 'optional': {'type': 'boolean'}, 'py-limited-api': {'type': 'boolean'}}}, 'file-directive': {'$id': '#/definitions/file-directive', 'title': \"'file:' directive\", 'description': 'Value is read from a file (or list of files and then concatenated)', 'type': 'object', 'additionalProperties': False, 'properties': {'file': {'oneOf': [{'type': 'string'}, {'type': 'array', 'items': {'type': 'string'}}]}}, 'required': ['file']}, 'file-directive-for-dependencies': {'title': \"'file:' directive for dependencies\", 'allOf': [{'$$description': ['**BETA**: subset of the ``requirements.txt`` format', 'without ``pip`` flags and options', '(one :pep:`508`-compliant string per line,', 'lines that are blank or start with ``#`` are excluded).', 'See `dynamic metadata', '<https://setuptools.pypa.io/en/latest/userguide/pyproject_config.html#dynamic-metadata>`_.']}, {'$ref': '#/definitions/file-directive'}]}, 'attr-directive': {'title': \"'attr:' directive\", '$id': '#/definitions/attr-directive', '$$description': ['Value is read from a module attribute. Supports callables and iterables;', 'unsupported types are cast via ``str()``'], 'type': 'object', 'additionalProperties': False, 'properties': {'attr': {'type': 'string', 'format': 'python-qualified-identifier'}}, 'required': ['attr']}, 'find-directive': {'$id': '#/definitions/find-directive', 'title': \"'find:' directive\", 'type': 'object', 'additionalProperties': False, 'properties': {'find': {'type': 'object', '$$description': ['Dynamic `package discovery', '<https://setuptools.pypa.io/en/latest/userguide/package_discovery.html>`_.'], 'additionalProperties': False, 'properties': {'where': {'description': 'Directories to be searched for packages (Unix-style relative path)', 'type': 'array', 'items': {'type': 'string'}}, 'exclude': {'type': 'array', '$$description': ['Exclude packages that match the values listed in this field.', \"Can container shell-style wildcards (e.g. ``'pkg.*'``)\"], 'items': {'type': 'string'}}, 'include': {'type': 'array', '$$description': ['Restrict the found packages to just the ones listed in this field.', \"Can container shell-style wildcards (e.g. ``'pkg.*'``)\"], 'items': {'type': 'string'}}, 'namespaces': {'type': 'boolean', '$$description': ['When ``True``, directories without a ``__init__.py`` file will also', 'be scanned for :pep:`420`-style implicit namespaces']}}}}}}}}}, 'dependency-groups': {'type': 'object', 'description': 'Dependency groups following PEP 735', 'additionalProperties': False, 'patternProperties': {'^([a-zA-Z0-9]|[a-zA-Z0-9][a-zA-Z0-9._-]*[a-zA-Z0-9])$': {'type': 'array', 'items': {'oneOf': [{'type': 'string', 'description': 'Python package specifiers following PEP 508', 'format': 'pep508'}, {'type': 'object', 'additionalProperties': False, 'properties': {'include-group': {'description': 'Another dependency group to include in this one', 'type': 'string', 'pattern': '^([a-zA-Z0-9]|[a-zA-Z0-9][a-zA-Z0-9._-]*[a-zA-Z0-9])$'}}}]}}}}}, 'project': {'$schema': 'http://json-schema.org/draft-07/schema#', '$id': 'https://packaging.python.org/en/latest/specifications/pyproject-toml/', 'title': 'Package metadata stored in the ``project`` table', '$$description': ['Data structure for the **project** table inside ``pyproject.toml``', '(as initially defined in :pep:`621`)'], 'type': 'object', 'properties': {'name': {'type': 'string', 'description': 'The name (primary identifier) of the project. MUST be statically defined.', 'format': 'pep508-identifier'}, 'version': {'type': 'string', 'description': 'The version of the project as supported by :pep:`440`.', 'format': 'pep440'}, 'description': {'type': 'string', '$$description': ['The `summary description of the project', '<https://packaging.python.org/specifications/core-metadata/#summary>`_']}, 'readme': {'$$description': ['`Full/detailed description of the project in the form of a README', '<https://peps.python.org/pep-0621/#readme>`_', \"with meaning similar to the one defined in `core metadata's Description\", '<https://packaging.python.org/specifications/core-metadata/#description>`_'], 'oneOf': [{'type': 'string', '$$description': ['Relative path to a text file (UTF-8) containing the full description', 'of the project. If the file path ends in case-insensitive ``.md`` or', '``.rst`` suffixes, then the content-type is respectively', '``text/markdown`` or ``text/x-rst``']}, {'type': 'object', 'allOf': [{'anyOf': [{'properties': {'file': {'type': 'string', '$$description': ['Relative path to a text file containing the full description', 'of the project.']}}, 'required': ['file']}, {'properties': {'text': {'type': 'string', 'description': 'Full text describing the project.'}}, 'required': ['text']}]}, {'properties': {'content-type': {'type': 'string', '$$description': ['Content-type (:rfc:`1341`) of the full description', '(e.g. ``text/markdown``). The ``charset`` parameter is assumed', 'UTF-8 when not present.'], '$comment': 'TODO: add regex pattern or format?'}}, 'required': ['content-type']}]}]}, 'requires-python': {'type': 'string', 'format': 'pep508-versionspec', '$$description': ['`The Python version requirements of the project', '<https://packaging.python.org/specifications/core-metadata/#requires-python>`_.']}, 'license': {'description': '`Project license <https://peps.python.org/pep-0621/#license>`_.', 'oneOf': [{'type': 'string', 'description': 'An SPDX license identifier', 'format': 'SPDX'}, {'type': 'object', 'properties': {'file': {'type': 'string', '$$description': ['Relative path to the file (UTF-8) which contains the license for the', 'project.']}}, 'required': ['file']}, {'type': 'object', 'properties': {'text': {'type': 'string', '$$description': ['The license of the project whose meaning is that of the', '`License field from the core metadata', '<https://packaging.python.org/specifications/core-metadata/#license>`_.']}}, 'required': ['text']}]}, 'license-files': {'description': 'Paths or globs to paths of license files', 'type': 'array', 'items': {'type': 'string'}}, 'authors': {'type': 'array', 'items': {'$ref': '#/definitions/author'}, '$$description': [\"The people or organizations considered to be the 'authors' of the project.\", 'The exact meaning is open to interpretation (e.g. original or primary authors,', 'current maintainers, or owners of the package).']}, 'maintainers': {'type': 'array', 'items': {'$ref': '#/definitions/author'}, '$$description': [\"The people or organizations considered to be the 'maintainers' of the project.\", 'Similarly to ``authors``, the exact meaning is open to interpretation.']}, 'keywords': {'type': 'array', 'items': {'type': 'string'}, 'description': 'List of keywords to assist searching for the distribution in a larger catalog.'}, 'classifiers': {'type': 'array', 'items': {'type': 'string', 'format': 'trove-classifier', 'description': '`PyPI classifier <https://pypi.org/classifiers/>`_.'}, '$$description': ['`Trove classifiers <https://pypi.org/classifiers/>`_', 'which apply to the project.']}, 'urls': {'type': 'object', 'description': 'URLs associated with the project in the form ``label => value``.', 'additionalProperties': False, 'patternProperties': {'^.+$': {'type': 'string', 'format': 'url'}}}, 'scripts': {'$ref': '#/definitions/entry-point-group', '$$description': ['Instruct the installer to create command-line wrappers for the given', '`entry points <https://packaging.python.org/specifications/entry-points/>`_.']}, 'gui-scripts': {'$ref': '#/definitions/entry-point-group', '$$description': ['Instruct the installer to create GUI wrappers for the given', '`entry points <https://packaging.python.org/specifications/entry-points/>`_.', 'The difference between ``scripts`` and ``gui-scripts`` is only relevant in', 'Windows.']}, 'entry-points': {'$$description': ['Instruct the installer to expose the given modules/functions via', '``entry-point`` discovery mechanism (useful for plugins).', 'More information available in the `Python packaging guide', '<https://packaging.python.org/specifications/entry-points/>`_.'], 'propertyNames': {'format': 'python-entrypoint-group'}, 'additionalProperties': False, 'patternProperties': {'^.+$': {'$ref': '#/definitions/entry-point-group'}}}, 'dependencies': {'type': 'array', 'description': 'Project (mandatory) dependencies.', 'items': {'$ref': '#/definitions/dependency'}}, 'optional-dependencies': {'type': 'object', 'description': 'Optional dependency for the project', 'propertyNames': {'format': 'pep508-identifier'}, 'additionalProperties': False, 'patternProperties': {'^.+$': {'type': 'array', 'items': {'$ref': '#/definitions/dependency'}}}}, 'dynamic': {'type': 'array', '$$description': ['Specifies which fields are intentionally unspecified and expected to be', 'dynamically provided by build tools'], 'items': {'enum': ['version', 'description', 'readme', 'requires-python', 'license', 'license-files', 'authors', 'maintainers', 'keywords', 'classifiers', 'urls', 'scripts', 'gui-scripts', 'entry-points', 'dependencies', 'optional-dependencies']}}}, 'required': ['name'], 'additionalProperties': False, 'allOf': [{'if': {'not': {'required': ['dynamic'], 'properties': {'dynamic': {'contains': {'const': 'version'}, '$$description': ['version is listed in ``dynamic``']}}}, '$$comment': ['According to :pep:`621`:', '    If the core metadata specification lists a field as \"Required\", then', '    the metadata MUST specify the field statically or list it in dynamic', 'In turn, `core metadata`_ defines:', '    The required fields are: Metadata-Version, Name, Version.', '    All the other fields are optional.', 'Since ``Metadata-Version`` is defined by the build back-end, ``name`` and', '``version`` are the only mandatory information in ``pyproject.toml``.', '.. _core metadata: https://packaging.python.org/specifications/core-metadata/']}, 'then': {'required': ['version'], '$$description': ['version should be statically defined in the ``version`` field']}}, {'if': {'required': ['license-files']}, 'then': {'properties': {'license': {'type': 'string'}}}}], 'definitions': {'author': {'$id': '#/definitions/author', 'title': 'Author or Maintainer', '$comment': 'https://peps.python.org/pep-0621/#authors-maintainers', 'type': 'object', 'additionalProperties': False, 'properties': {'name': {'type': 'string', '$$description': ['MUST be a valid email name, i.e. whatever can be put as a name, before an', 'email, in :rfc:`822`.']}, 'email': {'type': 'string', 'format': 'idn-email', 'description': 'MUST be a valid email address'}}}, 'entry-point-group': {'$id': '#/definitions/entry-point-group', 'title': 'Entry-points', 'type': 'object', '$$description': ['Entry-points are grouped together to indicate what sort of capabilities they', 'provide.', 'See the `packaging guides', '<https://packaging.python.org/specifications/entry-points/>`_', 'and `setuptools docs', '<https://setuptools.pypa.io/en/latest/userguide/entry_point.html>`_', 'for more information.'], 'propertyNames': {'format': 'python-entrypoint-name'}, 'additionalProperties': False, 'patternProperties': {'^.+$': {'type': 'string', '$$description': ['Reference to a Python object. It is either in the form', '``importable.module``, or ``importable.module:object.attr``.'], 'format': 'python-entrypoint-reference', '$comment': 'https://packaging.python.org/specifications/entry-points/'}}}, 'dependency': {'$id': '#/definitions/dependency', 'title': 'Dependency', 'type': 'string', 'description': 'Project dependency specification according to PEP 508', 'format': 'pep508'}}}}, rule='type')\n./.venv/lib/python3.13/site-packages/setuptools/config/_validate_pyproject/fastjsonschema_validations.py:                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".build-system must be object\", value=data__buildsystem, name=\"\" + (name_prefix or \"data\") + \".build-system\", definition={'type': 'object', 'description': 'Table used to store build-related data', 'additionalProperties': False, 'properties': {'requires': {'type': 'array', '$$description': ['List of dependencies in the :pep:`508` format required to execute the build', 'system. Please notice that the resulting dependency graph', '**MUST NOT contain cycles**'], 'items': {'type': 'string'}}, 'build-backend': {'type': 'string', 'description': 'Python object that will be used to perform the build according to :pep:`517`', 'format': 'pep517-backend-reference'}, 'backend-path': {'type': 'array', '$$description': ['List of directories to be prepended to ``sys.path`` when loading the', 'back-end, and running its hooks'], 'items': {'type': 'string', '$comment': 'Should be a path (TODO: enforce it with format?)'}}}, 'required': ['requires']}, rule='type')\n./.venv/lib/python3.13/site-packages/setuptools/config/_validate_pyproject/fastjsonschema_validations.py:                    raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".build-system must contain \" + (str(sorted(data__buildsystem__missing_keys)) + \" properties\"), value=data__buildsystem, name=\"\" + (name_prefix or \"data\") + \".build-system\", definition={'type': 'object', 'description': 'Table used to store build-related data', 'additionalProperties': False, 'properties': {'requires': {'type': 'array', '$$description': ['List of dependencies in the :pep:`508` format required to execute the build', 'system. Please notice that the resulting dependency graph', '**MUST NOT contain cycles**'], 'items': {'type': 'string'}}, 'build-backend': {'type': 'string', 'description': 'Python object that will be used to perform the build according to :pep:`517`', 'format': 'pep517-backend-reference'}, 'backend-path': {'type': 'array', '$$description': ['List of directories to be prepended to ``sys.path`` when loading the', 'back-end, and running its hooks'], 'items': {'type': 'string', '$comment': 'Should be a path (TODO: enforce it with format?)'}}}, 'required': ['requires']}, rule='required')\n./.venv/lib/python3.13/site-packages/setuptools/config/_validate_pyproject/fastjsonschema_validations.py:                        raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".build-system.backend-path must be array\", value=data__buildsystem__backendpath, name=\"\" + (name_prefix or \"data\") + \".build-system.backend-path\", definition={'type': 'array', '$$description': ['List of directories to be prepended to ``sys.path`` when loading the', 'back-end, and running its hooks'], 'items': {'type': 'string', '$comment': 'Should be a path (TODO: enforce it with format?)'}}, rule='type')\n./.venv/lib/python3.13/site-packages/setuptools/config/_validate_pyproject/fastjsonschema_validations.py:                                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".build-system.backend-path[{data__buildsystem__backendpath_x}]\".format(**locals()) + \" must be string\", value=data__buildsystem__backendpath_item, name=\"\" + (name_prefix or \"data\") + \".build-system.backend-path[{data__buildsystem__backendpath_x}]\".format(**locals()) + \"\", definition={'type': 'string', '$comment': 'Should be a path (TODO: enforce it with format?)'}, rule='type')\n./.venv/lib/python3.13/site-packages/setuptools/config/_validate_pyproject/fastjsonschema_validations.py:                    raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".build-system must not contain \"+str(data__buildsystem_keys)+\" properties\", value=data__buildsystem, name=\"\" + (name_prefix or \"data\") + \".build-system\", definition={'type': 'object', 'description': 'Table used to store build-related data', 'additionalProperties': False, 'properties': {'requires': {'type': 'array', '$$description': ['List of dependencies in the :pep:`508` format required to execute the build', 'system. Please notice that the resulting dependency graph', '**MUST NOT contain cycles**'], 'items': {'type': 'string'}}, 'build-backend': {'type': 'string', 'description': 'Python object that will be used to perform the build according to :pep:`517`', 'format': 'pep517-backend-reference'}, 'backend-path': {'type': 'array', '$$description': ['List of directories to be prepended to ``sys.path`` when loading the', 'back-end, and running its hooks'], 'items': {'type': 'string', '$comment': 'Should be a path (TODO: enforce it with format?)'}}}, 'required': ['requires']}, rule='additionalProperties')\n./.venv/lib/python3.13/site-packages/setuptools/config/_validate_pyproject/fastjsonschema_validations.py:                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".tool must be object\", value=data__tool, name=\"\" + (name_prefix or \"data\") + \".tool\", definition={'type': 'object', 'properties': {'distutils': {'$schema': 'http://json-schema.org/draft-07/schema#', '$id': 'https://setuptools.pypa.io/en/latest/deprecated/distutils/configfile.html', 'title': '``tool.distutils`` table', '$$description': ['**EXPERIMENTAL** (NOT OFFICIALLY SUPPORTED): Use ``tool.distutils``', 'subtables to configure arguments for ``distutils`` commands.', 'Originally, ``distutils`` allowed developers to configure arguments for', '``setup.py`` commands via `distutils configuration files', '<https://setuptools.pypa.io/en/latest/deprecated/distutils/configfile.html>`_.', 'See also `the old Python docs <https://docs.python.org/3.11/install/>_`.'], 'type': 'object', 'properties': {'global': {'type': 'object', 'description': 'Global options applied to all ``distutils`` commands'}}, 'patternProperties': {'.+': {'type': 'object'}}, '$comment': 'TODO: Is there a practical way of making this schema more specific?'}, 'setuptools': {'$schema': 'http://json-schema.org/draft-07/schema#', '$id': 'https://setuptools.pypa.io/en/latest/userguide/pyproject_config.html', 'title': '``tool.setuptools`` table', '$$description': ['``setuptools``-specific configurations that can be set by users that require', 'customization.', 'These configurations are completely optional and probably can be skipped when', 'creating simple packages. They are equivalent to some of the `Keywords', '<https://setuptools.pypa.io/en/latest/references/keywords.html>`_', 'used by the ``setup.py`` file, and can be set via the ``tool.setuptools`` table.', 'It considers only ``setuptools`` `parameters', '<https://setuptools.pypa.io/en/latest/userguide/pyproject_config.html#setuptools-specific-configuration>`_', 'that are not covered by :pep:`621`; and intentionally excludes ``dependency_links``', 'and ``setup_requires`` (incompatible with modern workflows/standards).'], 'type': 'object', 'additionalProperties': False, 'properties': {'platforms': {'type': 'array', 'items': {'type': 'string'}}, 'provides': {'$$description': ['Package and virtual package names contained within this package', '**(not supported by pip)**'], 'type': 'array', 'items': {'type': 'string', 'format': 'pep508-identifier'}}, 'obsoletes': {'$$description': ['Packages which this package renders obsolete', '**(not supported by pip)**'], 'type': 'array', 'items': {'type': 'string', 'format': 'pep508-identifier'}}, 'zip-safe': {'$$description': ['Whether the project can be safely installed and run from a zip file.', '**OBSOLETE**: only relevant for ``pkg_resources``, ``easy_install`` and', '``setup.py install`` in the context of ``eggs`` (**DEPRECATED**).'], 'type': 'boolean'}, 'script-files': {'$$description': ['Legacy way of defining scripts (entry-points are preferred).', 'Equivalent to the ``script`` keyword in ``setup.py``', '(it was renamed to avoid confusion with entry-point based ``project.scripts``', 'defined in :pep:`621`).', '**DISCOURAGED**: generic script wrappers are tricky and may not work properly.', 'Whenever possible, please use ``project.scripts`` instead.'], 'type': 'array', 'items': {'type': 'string'}, '$comment': 'TODO: is this field deprecated/should be removed?'}, 'eager-resources': {'$$description': ['Resources that should be extracted together, if any of them is needed,', 'or if any C extensions included in the project are imported.', '**OBSOLETE**: only relevant for ``pkg_resources``, ``easy_install`` and', '``setup.py install`` in the context of ``eggs`` (**DEPRECATED**).'], 'type': 'array', 'items': {'type': 'string'}}, 'packages': {'$$description': ['Packages that should be included in the distribution.', 'It can be given either as a list of package identifiers', 'or as a ``dict``-like structure with a single key ``find``', 'which corresponds to a dynamic call to', '``setuptools.config.expand.find_packages`` function.', 'The ``find`` key is associated with a nested ``dict``-like structure that can', 'contain ``where``, ``include``, ``exclude`` and ``namespaces`` keys,', 'mimicking the keyword arguments of the associated function.'], 'oneOf': [{'title': 'Array of Python package identifiers', 'type': 'array', 'items': {'$ref': '#/definitions/package-name'}}, {'$ref': '#/definitions/find-directive'}]}, 'package-dir': {'$$description': [':class:`dict`-like structure mapping from package names to directories where their', 'code can be found.', 'The empty string (as key) means that all packages are contained inside', 'the given directory will be included in the distribution.'], 'type': 'object', 'additionalProperties': False, 'propertyNames': {'anyOf': [{'const': ''}, {'$ref': '#/definitions/package-name'}]}, 'patternProperties': {'^.*$': {'type': 'string'}}}, 'package-data': {'$$description': ['Mapping from package names to lists of glob patterns.', 'Usually this option is not needed when using ``include-package-data = true``', 'For more information on how to include data files, check ``setuptools`` `docs', '<https://setuptools.pypa.io/en/latest/userguide/datafiles.html>`_.'], 'type': 'object', 'additionalProperties': False, 'propertyNames': {'anyOf': [{'type': 'string', 'format': 'python-module-name'}, {'const': '*'}]}, 'patternProperties': {'^.*$': {'type': 'array', 'items': {'type': 'string'}}}}, 'include-package-data': {'$$description': ['Automatically include any data files inside the package directories', 'that are specified by ``MANIFEST.in``', 'For more information on how to include data files, check ``setuptools`` `docs', '<https://setuptools.pypa.io/en/latest/userguide/datafiles.html>`_.'], 'type': 'boolean'}, 'exclude-package-data': {'$$description': ['Mapping from package names to lists of glob patterns that should be excluded', 'For more information on how to include data files, check ``setuptools`` `docs', '<https://setuptools.pypa.io/en/latest/userguide/datafiles.html>`_.'], 'type': 'object', 'additionalProperties': False, 'propertyNames': {'anyOf': [{'type': 'string', 'format': 'python-module-name'}, {'const': '*'}]}, 'patternProperties': {'^.*$': {'type': 'array', 'items': {'type': 'string'}}}}, 'namespace-packages': {'type': 'array', 'items': {'type': 'string', 'format': 'python-module-name-relaxed'}, '$comment': 'https://setuptools.pypa.io/en/latest/userguide/package_discovery.html', 'description': '**DEPRECATED**: use implicit namespaces instead (:pep:`420`).'}, 'py-modules': {'description': 'Modules that setuptools will manipulate', 'type': 'array', 'items': {'type': 'string', 'format': 'python-module-name-relaxed'}, '$comment': 'TODO: clarify the relationship with ``packages``'}, 'ext-modules': {'description': 'Extension modules to be compiled by setuptools', 'type': 'array', 'items': {'$ref': '#/definitions/ext-module'}}, 'data-files': {'$$description': ['``dict``-like structure where each key represents a directory and', 'the value is a list of glob patterns that should be installed in them.', '**DISCOURAGED**: please notice this might not work as expected with wheels.', 'Whenever possible, consider using data files inside the package directories', '(or create a new namespace package that only contains data files).', 'See `data files support', '<https://setuptools.pypa.io/en/latest/userguide/datafiles.html>`_.'], 'type': 'object', 'patternProperties': {'^.*$': {'type': 'array', 'items': {'type': 'string'}}}}, 'cmdclass': {'$$description': ['Mapping of distutils-style command names to ``setuptools.Command`` subclasses', 'which in turn should be represented by strings with a qualified class name', '(i.e., \"dotted\" form with module), e.g.::\\n\\n', '    cmdclass = {mycmd = \"pkg.subpkg.module.CommandClass\"}\\n\\n', 'The command class should be a directly defined at the top-level of the', 'containing module (no class nesting).'], 'type': 'object', 'patternProperties': {'^.*$': {'type': 'string', 'format': 'python-qualified-identifier'}}}, 'license-files': {'type': 'array', 'items': {'type': 'string'}, '$$description': ['**PROVISIONAL**: list of glob patterns for all license files being distributed.', '(likely to become standard with :pep:`639`).', \"By default: ``['LICEN[CS]E*', 'COPYING*', 'NOTICE*', 'AUTHORS*']``\"], '$comment': 'TODO: revise if PEP 639 is accepted. Probably ``project.license-files``?'}, 'dynamic': {'type': 'object', 'description': 'Instructions for loading :pep:`621`-related metadata dynamically', 'additionalProperties': False, 'properties': {'version': {'$$description': ['A version dynamically loaded via either the ``attr:`` or ``file:``', 'directives. Please make sure the given file or attribute respects :pep:`440`.', 'Also ensure to set ``project.dynamic`` accordingly.'], 'oneOf': [{'$ref': '#/definitions/attr-directive'}, {'$ref': '#/definitions/file-directive'}]}, 'classifiers': {'$ref': '#/definitions/file-directive'}, 'description': {'$ref': '#/definitions/file-directive'}, 'entry-points': {'$ref': '#/definitions/file-directive'}, 'dependencies': {'$ref': '#/definitions/file-directive-for-dependencies'}, 'optional-dependencies': {'type': 'object', 'propertyNames': {'type': 'string', 'format': 'pep508-identifier'}, 'additionalProperties': False, 'patternProperties': {'.+': {'$ref': '#/definitions/file-directive-for-dependencies'}}}, 'readme': {'type': 'object', 'anyOf': [{'$ref': '#/definitions/file-directive'}, {'type': 'object', 'properties': {'content-type': {'type': 'string'}, 'file': {'$ref': '#/definitions/file-directive/properties/file'}}, 'additionalProperties': False}], 'required': ['file']}}}}, 'definitions': {'package-name': {'$id': '#/definitions/package-name', 'title': 'Valid package name', 'description': 'Valid package name (importable or :pep:`561`).', 'type': 'string', 'anyOf': [{'type': 'string', 'format': 'python-module-name-relaxed'}, {'type': 'string', 'format': 'pep561-stub-name'}]}, 'ext-module': {'$id': '#/definitions/ext-module', 'title': 'Extension module', 'description': 'Parameters to construct a :class:`setuptools.Extension` object', 'type': 'object', 'required': ['name', 'sources'], 'additionalProperties': False, 'properties': {'name': {'type': 'string', 'format': 'python-module-name-relaxed'}, 'sources': {'type': 'array', 'items': {'type': 'string'}}, 'include-dirs': {'type': 'array', 'items': {'type': 'string'}}, 'define-macros': {'type': 'array', 'items': {'type': 'array', 'items': [{'description': 'macro name', 'type': 'string'}, {'description': 'macro value', 'oneOf': [{'type': 'string'}, {'type': 'null'}]}], 'additionalItems': False}}, 'undef-macros': {'type': 'array', 'items': {'type': 'string'}}, 'library-dirs': {'type': 'array', 'items': {'type': 'string'}}, 'libraries': {'type': 'array', 'items': {'type': 'string'}}, 'runtime-library-dirs': {'type': 'array', 'items': {'type': 'string'}}, 'extra-objects': {'type': 'array', 'items': {'type': 'string'}}, 'extra-compile-args': {'type': 'array', 'items': {'type': 'string'}}, 'extra-link-args': {'type': 'array', 'items': {'type': 'string'}}, 'export-symbols': {'type': 'array', 'items': {'type': 'string'}}, 'swig-opts': {'type': 'array', 'items': {'type': 'string'}}, 'depends': {'type': 'array', 'items': {'type': 'string'}}, 'language': {'type': 'string'}, 'optional': {'type': 'boolean'}, 'py-limited-api': {'type': 'boolean'}}}, 'file-directive': {'$id': '#/definitions/file-directive', 'title': \"'file:' directive\", 'description': 'Value is read from a file (or list of files and then concatenated)', 'type': 'object', 'additionalProperties': False, 'properties': {'file': {'oneOf': [{'type': 'string'}, {'type': 'array', 'items': {'type': 'string'}}]}}, 'required': ['file']}, 'file-directive-for-dependencies': {'title': \"'file:' directive for dependencies\", 'allOf': [{'$$description': ['**BETA**: subset of the ``requirements.txt`` format', 'without ``pip`` flags and options', '(one :pep:`508`-compliant string per line,', 'lines that are blank or start with ``#`` are excluded).', 'See `dynamic metadata', '<https://setuptools.pypa.io/en/latest/userguide/pyproject_config.html#dynamic-metadata>`_.']}, {'$ref': '#/definitions/file-directive'}]}, 'attr-directive': {'title': \"'attr:' directive\", '$id': '#/definitions/attr-directive', '$$description': ['Value is read from a module attribute. Supports callables and iterables;', 'unsupported types are cast via ``str()``'], 'type': 'object', 'additionalProperties': False, 'properties': {'attr': {'type': 'string', 'format': 'python-qualified-identifier'}}, 'required': ['attr']}, 'find-directive': {'$id': '#/definitions/find-directive', 'title': \"'find:' directive\", 'type': 'object', 'additionalProperties': False, 'properties': {'find': {'type': 'object', '$$description': ['Dynamic `package discovery', '<https://setuptools.pypa.io/en/latest/userguide/package_discovery.html>`_.'], 'additionalProperties': False, 'properties': {'where': {'description': 'Directories to be searched for packages (Unix-style relative path)', 'type': 'array', 'items': {'type': 'string'}}, 'exclude': {'type': 'array', '$$description': ['Exclude packages that match the values listed in this field.', \"Can container shell-style wildcards (e.g. ``'pkg.*'``)\"], 'items': {'type': 'string'}}, 'include': {'type': 'array', '$$description': ['Restrict the found packages to just the ones listed in this field.', \"Can container shell-style wildcards (e.g. ``'pkg.*'``)\"], 'items': {'type': 'string'}}, 'namespaces': {'type': 'boolean', '$$description': ['When ``True``, directories without a ``__init__.py`` file will also', 'be scanned for :pep:`420`-style implicit namespaces']}}}}}}}}}, rule='type')\n./.venv/lib/python3.13/site-packages/setuptools/config/_validate_pyproject/fastjsonschema_validations.py:            raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \" must not contain \"+str(data_keys)+\" properties\", value=data, name=\"\" + (name_prefix or \"data\") + \"\", definition={'$schema': 'http://json-schema.org/draft-07/schema#', '$id': 'https://packaging.python.org/en/latest/specifications/declaring-build-dependencies/', 'title': 'Data structure for ``pyproject.toml`` files', '$$description': ['File format containing build-time configurations for the Python ecosystem. ', ':pep:`517` initially defined a build-system independent format for source trees', 'which was complemented by :pep:`518` to provide a way of specifying dependencies ', 'for building Python projects.', 'Please notice the ``project`` table (as initially defined in  :pep:`621`) is not included', 'in this schema and should be considered separately.'], 'type': 'object', 'additionalProperties': False, 'properties': {'build-system': {'type': 'object', 'description': 'Table used to store build-related data', 'additionalProperties': False, 'properties': {'requires': {'type': 'array', '$$description': ['List of dependencies in the :pep:`508` format required to execute the build', 'system. Please notice that the resulting dependency graph', '**MUST NOT contain cycles**'], 'items': {'type': 'string'}}, 'build-backend': {'type': 'string', 'description': 'Python object that will be used to perform the build according to :pep:`517`', 'format': 'pep517-backend-reference'}, 'backend-path': {'type': 'array', '$$description': ['List of directories to be prepended to ``sys.path`` when loading the', 'back-end, and running its hooks'], 'items': {'type': 'string', '$comment': 'Should be a path (TODO: enforce it with format?)'}}}, 'required': ['requires']}, 'project': {'$schema': 'http://json-schema.org/draft-07/schema#', '$id': 'https://packaging.python.org/en/latest/specifications/pyproject-toml/', 'title': 'Package metadata stored in the ``project`` table', '$$description': ['Data structure for the **project** table inside ``pyproject.toml``', '(as initially defined in :pep:`621`)'], 'type': 'object', 'properties': {'name': {'type': 'string', 'description': 'The name (primary identifier) of the project. MUST be statically defined.', 'format': 'pep508-identifier'}, 'version': {'type': 'string', 'description': 'The version of the project as supported by :pep:`440`.', 'format': 'pep440'}, 'description': {'type': 'string', '$$description': ['The `summary description of the project', '<https://packaging.python.org/specifications/core-metadata/#summary>`_']}, 'readme': {'$$description': ['`Full/detailed description of the project in the form of a README', '<https://peps.python.org/pep-0621/#readme>`_', \"with meaning similar to the one defined in `core metadata's Description\", '<https://packaging.python.org/specifications/core-metadata/#description>`_'], 'oneOf': [{'type': 'string', '$$description': ['Relative path to a text file (UTF-8) containing the full description', 'of the project. If the file path ends in case-insensitive ``.md`` or', '``.rst`` suffixes, then the content-type is respectively', '``text/markdown`` or ``text/x-rst``']}, {'type': 'object', 'allOf': [{'anyOf': [{'properties': {'file': {'type': 'string', '$$description': ['Relative path to a text file containing the full description', 'of the project.']}}, 'required': ['file']}, {'properties': {'text': {'type': 'string', 'description': 'Full text describing the project.'}}, 'required': ['text']}]}, {'properties': {'content-type': {'type': 'string', '$$description': ['Content-type (:rfc:`1341`) of the full description', '(e.g. ``text/markdown``). The ``charset`` parameter is assumed', 'UTF-8 when not present.'], '$comment': 'TODO: add regex pattern or format?'}}, 'required': ['content-type']}]}]}, 'requires-python': {'type': 'string', 'format': 'pep508-versionspec', '$$description': ['`The Python version requirements of the project', '<https://packaging.python.org/specifications/core-metadata/#requires-python>`_.']}, 'license': {'description': '`Project license <https://peps.python.org/pep-0621/#license>`_.', 'oneOf': [{'type': 'string', 'description': 'An SPDX license identifier', 'format': 'SPDX'}, {'type': 'object', 'properties': {'file': {'type': 'string', '$$description': ['Relative path to the file (UTF-8) which contains the license for the', 'project.']}}, 'required': ['file']}, {'type': 'object', 'properties': {'text': {'type': 'string', '$$description': ['The license of the project whose meaning is that of the', '`License field from the core metadata', '<https://packaging.python.org/specifications/core-metadata/#license>`_.']}}, 'required': ['text']}]}, 'license-files': {'description': 'Paths or globs to paths of license files', 'type': 'array', 'items': {'type': 'string'}}, 'authors': {'type': 'array', 'items': {'$ref': '#/definitions/author'}, '$$description': [\"The people or organizations considered to be the 'authors' of the project.\", 'The exact meaning is open to interpretation (e.g. original or primary authors,', 'current maintainers, or owners of the package).']}, 'maintainers': {'type': 'array', 'items': {'$ref': '#/definitions/author'}, '$$description': [\"The people or organizations considered to be the 'maintainers' of the project.\", 'Similarly to ``authors``, the exact meaning is open to interpretation.']}, 'keywords': {'type': 'array', 'items': {'type': 'string'}, 'description': 'List of keywords to assist searching for the distribution in a larger catalog.'}, 'classifiers': {'type': 'array', 'items': {'type': 'string', 'format': 'trove-classifier', 'description': '`PyPI classifier <https://pypi.org/classifiers/>`_.'}, '$$description': ['`Trove classifiers <https://pypi.org/classifiers/>`_', 'which apply to the project.']}, 'urls': {'type': 'object', 'description': 'URLs associated with the project in the form ``label => value``.', 'additionalProperties': False, 'patternProperties': {'^.+$': {'type': 'string', 'format': 'url'}}}, 'scripts': {'$ref': '#/definitions/entry-point-group', '$$description': ['Instruct the installer to create command-line wrappers for the given', '`entry points <https://packaging.python.org/specifications/entry-points/>`_.']}, 'gui-scripts': {'$ref': '#/definitions/entry-point-group', '$$description': ['Instruct the installer to create GUI wrappers for the given', '`entry points <https://packaging.python.org/specifications/entry-points/>`_.', 'The difference between ``scripts`` and ``gui-scripts`` is only relevant in', 'Windows.']}, 'entry-points': {'$$description': ['Instruct the installer to expose the given modules/functions via', '``entry-point`` discovery mechanism (useful for plugins).', 'More information available in the `Python packaging guide', '<https://packaging.python.org/specifications/entry-points/>`_.'], 'propertyNames': {'format': 'python-entrypoint-group'}, 'additionalProperties': False, 'patternProperties': {'^.+$': {'$ref': '#/definitions/entry-point-group'}}}, 'dependencies': {'type': 'array', 'description': 'Project (mandatory) dependencies.', 'items': {'$ref': '#/definitions/dependency'}}, 'optional-dependencies': {'type': 'object', 'description': 'Optional dependency for the project', 'propertyNames': {'format': 'pep508-identifier'}, 'additionalProperties': False, 'patternProperties': {'^.+$': {'type': 'array', 'items': {'$ref': '#/definitions/dependency'}}}}, 'dynamic': {'type': 'array', '$$description': ['Specifies which fields are intentionally unspecified and expected to be', 'dynamically provided by build tools'], 'items': {'enum': ['version', 'description', 'readme', 'requires-python', 'license', 'license-files', 'authors', 'maintainers', 'keywords', 'classifiers', 'urls', 'scripts', 'gui-scripts', 'entry-points', 'dependencies', 'optional-dependencies']}}}, 'required': ['name'], 'additionalProperties': False, 'allOf': [{'if': {'not': {'required': ['dynamic'], 'properties': {'dynamic': {'contains': {'const': 'version'}, '$$description': ['version is listed in ``dynamic``']}}}, '$$comment': ['According to :pep:`621`:', '    If the core metadata specification lists a field as \"Required\", then', '    the metadata MUST specify the field statically or list it in dynamic', 'In turn, `core metadata`_ defines:', '    The required fields are: Metadata-Version, Name, Version.', '    All the other fields are optional.', 'Since ``Metadata-Version`` is defined by the build back-end, ``name`` and', '``version`` are the only mandatory information in ``pyproject.toml``.', '.. _core metadata: https://packaging.python.org/specifications/core-metadata/']}, 'then': {'required': ['version'], '$$description': ['version should be statically defined in the ``version`` field']}}, {'if': {'required': ['license-files']}, 'then': {'properties': {'license': {'type': 'string'}}}}], 'definitions': {'author': {'$id': '#/definitions/author', 'title': 'Author or Maintainer', '$comment': 'https://peps.python.org/pep-0621/#authors-maintainers', 'type': 'object', 'additionalProperties': False, 'properties': {'name': {'type': 'string', '$$description': ['MUST be a valid email name, i.e. whatever can be put as a name, before an', 'email, in :rfc:`822`.']}, 'email': {'type': 'string', 'format': 'idn-email', 'description': 'MUST be a valid email address'}}}, 'entry-point-group': {'$id': '#/definitions/entry-point-group', 'title': 'Entry-points', 'type': 'object', '$$description': ['Entry-points are grouped together to indicate what sort of capabilities they', 'provide.', 'See the `packaging guides', '<https://packaging.python.org/specifications/entry-points/>`_', 'and `setuptools docs', '<https://setuptools.pypa.io/en/latest/userguide/entry_point.html>`_', 'for more information.'], 'propertyNames': {'format': 'python-entrypoint-name'}, 'additionalProperties': False, 'patternProperties': {'^.+$': {'type': 'string', '$$description': ['Reference to a Python object. It is either in the form', '``importable.module``, or ``importable.module:object.attr``.'], 'format': 'python-entrypoint-reference', '$comment': 'https://packaging.python.org/specifications/entry-points/'}}}, 'dependency': {'$id': '#/definitions/dependency', 'title': 'Dependency', 'type': 'string', 'description': 'Project dependency specification according to PEP 508', 'format': 'pep508'}}}, 'tool': {'type': 'object', 'properties': {'distutils': {'$schema': 'http://json-schema.org/draft-07/schema#', '$id': 'https://setuptools.pypa.io/en/latest/deprecated/distutils/configfile.html', 'title': '``tool.distutils`` table', '$$description': ['**EXPERIMENTAL** (NOT OFFICIALLY SUPPORTED): Use ``tool.distutils``', 'subtables to configure arguments for ``distutils`` commands.', 'Originally, ``distutils`` allowed developers to configure arguments for', '``setup.py`` commands via `distutils configuration files', '<https://setuptools.pypa.io/en/latest/deprecated/distutils/configfile.html>`_.', 'See also `the old Python docs <https://docs.python.org/3.11/install/>_`.'], 'type': 'object', 'properties': {'global': {'type': 'object', 'description': 'Global options applied to all ``distutils`` commands'}}, 'patternProperties': {'.+': {'type': 'object'}}, '$comment': 'TODO: Is there a practical way of making this schema more specific?'}, 'setuptools': {'$schema': 'http://json-schema.org/draft-07/schema#', '$id': 'https://setuptools.pypa.io/en/latest/userguide/pyproject_config.html', 'title': '``tool.setuptools`` table', '$$description': ['``setuptools``-specific configurations that can be set by users that require', 'customization.', 'These configurations are completely optional and probably can be skipped when', 'creating simple packages. They are equivalent to some of the `Keywords', '<https://setuptools.pypa.io/en/latest/references/keywords.html>`_', 'used by the ``setup.py`` file, and can be set via the ``tool.setuptools`` table.', 'It considers only ``setuptools`` `parameters', '<https://setuptools.pypa.io/en/latest/userguide/pyproject_config.html#setuptools-specific-configuration>`_', 'that are not covered by :pep:`621`; and intentionally excludes ``dependency_links``', 'and ``setup_requires`` (incompatible with modern workflows/standards).'], 'type': 'object', 'additionalProperties': False, 'properties': {'platforms': {'type': 'array', 'items': {'type': 'string'}}, 'provides': {'$$description': ['Package and virtual package names contained within this package', '**(not supported by pip)**'], 'type': 'array', 'items': {'type': 'string', 'format': 'pep508-identifier'}}, 'obsoletes': {'$$description': ['Packages which this package renders obsolete', '**(not supported by pip)**'], 'type': 'array', 'items': {'type': 'string', 'format': 'pep508-identifier'}}, 'zip-safe': {'$$description': ['Whether the project can be safely installed and run from a zip file.', '**OBSOLETE**: only relevant for ``pkg_resources``, ``easy_install`` and', '``setup.py install`` in the context of ``eggs`` (**DEPRECATED**).'], 'type': 'boolean'}, 'script-files': {'$$description': ['Legacy way of defining scripts (entry-points are preferred).', 'Equivalent to the ``script`` keyword in ``setup.py``', '(it was renamed to avoid confusion with entry-point based ``project.scripts``', 'defined in :pep:`621`).', '**DISCOURAGED**: generic script wrappers are tricky and may not work properly.', 'Whenever possible, please use ``project.scripts`` instead.'], 'type': 'array', 'items': {'type': 'string'}, '$comment': 'TODO: is this field deprecated/should be removed?'}, 'eager-resources': {'$$description': ['Resources that should be extracted together, if any of them is needed,', 'or if any C extensions included in the project are imported.', '**OBSOLETE**: only relevant for ``pkg_resources``, ``easy_install`` and', '``setup.py install`` in the context of ``eggs`` (**DEPRECATED**).'], 'type': 'array', 'items': {'type': 'string'}}, 'packages': {'$$description': ['Packages that should be included in the distribution.', 'It can be given either as a list of package identifiers', 'or as a ``dict``-like structure with a single key ``find``', 'which corresponds to a dynamic call to', '``setuptools.config.expand.find_packages`` function.', 'The ``find`` key is associated with a nested ``dict``-like structure that can', 'contain ``where``, ``include``, ``exclude`` and ``namespaces`` keys,', 'mimicking the keyword arguments of the associated function.'], 'oneOf': [{'title': 'Array of Python package identifiers', 'type': 'array', 'items': {'$ref': '#/definitions/package-name'}}, {'$ref': '#/definitions/find-directive'}]}, 'package-dir': {'$$description': [':class:`dict`-like structure mapping from package names to directories where their', 'code can be found.', 'The empty string (as key) means that all packages are contained inside', 'the given directory will be included in the distribution.'], 'type': 'object', 'additionalProperties': False, 'propertyNames': {'anyOf': [{'const': ''}, {'$ref': '#/definitions/package-name'}]}, 'patternProperties': {'^.*$': {'type': 'string'}}}, 'package-data': {'$$description': ['Mapping from package names to lists of glob patterns.', 'Usually this option is not needed when using ``include-package-data = true``', 'For more information on how to include data files, check ``setuptools`` `docs', '<https://setuptools.pypa.io/en/latest/userguide/datafiles.html>`_.'], 'type': 'object', 'additionalProperties': False, 'propertyNames': {'anyOf': [{'type': 'string', 'format': 'python-module-name'}, {'const': '*'}]}, 'patternProperties': {'^.*$': {'type': 'array', 'items': {'type': 'string'}}}}, 'include-package-data': {'$$description': ['Automatically include any data files inside the package directories', 'that are specified by ``MANIFEST.in``', 'For more information on how to include data files, check ``setuptools`` `docs', '<https://setuptools.pypa.io/en/latest/userguide/datafiles.html>`_.'], 'type': 'boolean'}, 'exclude-package-data': {'$$description': ['Mapping from package names to lists of glob patterns that should be excluded', 'For more information on how to include data files, check ``setuptools`` `docs', '<https://setuptools.pypa.io/en/latest/userguide/datafiles.html>`_.'], 'type': 'object', 'additionalProperties': False, 'propertyNames': {'anyOf': [{'type': 'string', 'format': 'python-module-name'}, {'const': '*'}]}, 'patternProperties': {'^.*$': {'type': 'array', 'items': {'type': 'string'}}}}, 'namespace-packages': {'type': 'array', 'items': {'type': 'string', 'format': 'python-module-name-relaxed'}, '$comment': 'https://setuptools.pypa.io/en/latest/userguide/package_discovery.html', 'description': '**DEPRECATED**: use implicit namespaces instead (:pep:`420`).'}, 'py-modules': {'description': 'Modules that setuptools will manipulate', 'type': 'array', 'items': {'type': 'string', 'format': 'python-module-name-relaxed'}, '$comment': 'TODO: clarify the relationship with ``packages``'}, 'ext-modules': {'description': 'Extension modules to be compiled by setuptools', 'type': 'array', 'items': {'$ref': '#/definitions/ext-module'}}, 'data-files': {'$$description': ['``dict``-like structure where each key represents a directory and', 'the value is a list of glob patterns that should be installed in them.', '**DISCOURAGED**: please notice this might not work as expected with wheels.', 'Whenever possible, consider using data files inside the package directories', '(or create a new namespace package that only contains data files).', 'See `data files support', '<https://setuptools.pypa.io/en/latest/userguide/datafiles.html>`_.'], 'type': 'object', 'patternProperties': {'^.*$': {'type': 'array', 'items': {'type': 'string'}}}}, 'cmdclass': {'$$description': ['Mapping of distutils-style command names to ``setuptools.Command`` subclasses', 'which in turn should be represented by strings with a qualified class name', '(i.e., \"dotted\" form with module), e.g.::\\n\\n', '    cmdclass = {mycmd = \"pkg.subpkg.module.CommandClass\"}\\n\\n', 'The command class should be a directly defined at the top-level of the', 'containing module (no class nesting).'], 'type': 'object', 'patternProperties': {'^.*$': {'type': 'string', 'format': 'python-qualified-identifier'}}}, 'license-files': {'type': 'array', 'items': {'type': 'string'}, '$$description': ['**PROVISIONAL**: list of glob patterns for all license files being distributed.', '(likely to become standard with :pep:`639`).', \"By default: ``['LICEN[CS]E*', 'COPYING*', 'NOTICE*', 'AUTHORS*']``\"], '$comment': 'TODO: revise if PEP 639 is accepted. Probably ``project.license-files``?'}, 'dynamic': {'type': 'object', 'description': 'Instructions for loading :pep:`621`-related metadata dynamically', 'additionalProperties': False, 'properties': {'version': {'$$description': ['A version dynamically loaded via either the ``attr:`` or ``file:``', 'directives. Please make sure the given file or attribute respects :pep:`440`.', 'Also ensure to set ``project.dynamic`` accordingly.'], 'oneOf': [{'$ref': '#/definitions/attr-directive'}, {'$ref': '#/definitions/file-directive'}]}, 'classifiers': {'$ref': '#/definitions/file-directive'}, 'description': {'$ref': '#/definitions/file-directive'}, 'entry-points': {'$ref': '#/definitions/file-directive'}, 'dependencies': {'$ref': '#/definitions/file-directive-for-dependencies'}, 'optional-dependencies': {'type': 'object', 'propertyNames': {'type': 'string', 'format': 'pep508-identifier'}, 'additionalProperties': False, 'patternProperties': {'.+': {'$ref': '#/definitions/file-directive-for-dependencies'}}}, 'readme': {'type': 'object', 'anyOf': [{'$ref': '#/definitions/file-directive'}, {'type': 'object', 'properties': {'content-type': {'type': 'string'}, 'file': {'$ref': '#/definitions/file-directive/properties/file'}}, 'additionalProperties': False}], 'required': ['file']}}}}, 'definitions': {'package-name': {'$id': '#/definitions/package-name', 'title': 'Valid package name', 'description': 'Valid package name (importable or :pep:`561`).', 'type': 'string', 'anyOf': [{'type': 'string', 'format': 'python-module-name-relaxed'}, {'type': 'string', 'format': 'pep561-stub-name'}]}, 'ext-module': {'$id': '#/definitions/ext-module', 'title': 'Extension module', 'description': 'Parameters to construct a :class:`setuptools.Extension` object', 'type': 'object', 'required': ['name', 'sources'], 'additionalProperties': False, 'properties': {'name': {'type': 'string', 'format': 'python-module-name-relaxed'}, 'sources': {'type': 'array', 'items': {'type': 'string'}}, 'include-dirs': {'type': 'array', 'items': {'type': 'string'}}, 'define-macros': {'type': 'array', 'items': {'type': 'array', 'items': [{'description': 'macro name', 'type': 'string'}, {'description': 'macro value', 'oneOf': [{'type': 'string'}, {'type': 'null'}]}], 'additionalItems': False}}, 'undef-macros': {'type': 'array', 'items': {'type': 'string'}}, 'library-dirs': {'type': 'array', 'items': {'type': 'string'}}, 'libraries': {'type': 'array', 'items': {'type': 'string'}}, 'runtime-library-dirs': {'type': 'array', 'items': {'type': 'string'}}, 'extra-objects': {'type': 'array', 'items': {'type': 'string'}}, 'extra-compile-args': {'type': 'array', 'items': {'type': 'string'}}, 'extra-link-args': {'type': 'array', 'items': {'type': 'string'}}, 'export-symbols': {'type': 'array', 'items': {'type': 'string'}}, 'swig-opts': {'type': 'array', 'items': {'type': 'string'}}, 'depends': {'type': 'array', 'items': {'type': 'string'}}, 'language': {'type': 'string'}, 'optional': {'type': 'boolean'}, 'py-limited-api': {'type': 'boolean'}}}, 'file-directive': {'$id': '#/definitions/file-directive', 'title': \"'file:' directive\", 'description': 'Value is read from a file (or list of files and then concatenated)', 'type': 'object', 'additionalProperties': False, 'properties': {'file': {'oneOf': [{'type': 'string'}, {'type': 'array', 'items': {'type': 'string'}}]}}, 'required': ['file']}, 'file-directive-for-dependencies': {'title': \"'file:' directive for dependencies\", 'allOf': [{'$$description': ['**BETA**: subset of the ``requirements.txt`` format', 'without ``pip`` flags and options', '(one :pep:`508`-compliant string per line,', 'lines that are blank or start with ``#`` are excluded).', 'See `dynamic metadata', '<https://setuptools.pypa.io/en/latest/userguide/pyproject_config.html#dynamic-metadata>`_.']}, {'$ref': '#/definitions/file-directive'}]}, 'attr-directive': {'title': \"'attr:' directive\", '$id': '#/definitions/attr-directive', '$$description': ['Value is read from a module attribute. Supports callables and iterables;', 'unsupported types are cast via ``str()``'], 'type': 'object', 'additionalProperties': False, 'properties': {'attr': {'type': 'string', 'format': 'python-qualified-identifier'}}, 'required': ['attr']}, 'find-directive': {'$id': '#/definitions/find-directive', 'title': \"'find:' directive\", 'type': 'object', 'additionalProperties': False, 'properties': {'find': {'type': 'object', '$$description': ['Dynamic `package discovery', '<https://setuptools.pypa.io/en/latest/userguide/package_discovery.html>`_.'], 'additionalProperties': False, 'properties': {'where': {'description': 'Directories to be searched for packages (Unix-style relative path)', 'type': 'array', 'items': {'type': 'string'}}, 'exclude': {'type': 'array', '$$description': ['Exclude packages that match the values listed in this field.', \"Can container shell-style wildcards (e.g. ``'pkg.*'``)\"], 'items': {'type': 'string'}}, 'include': {'type': 'array', '$$description': ['Restrict the found packages to just the ones listed in this field.', \"Can container shell-style wildcards (e.g. ``'pkg.*'``)\"], 'items': {'type': 'string'}}, 'namespaces': {'type': 'boolean', '$$description': ['When ``True``, directories without a ``__init__.py`` file will also', 'be scanned for :pep:`420`-style implicit namespaces']}}}}}}}}}, 'dependency-groups': {'type': 'object', 'description': 'Dependency groups following PEP 735', 'additionalProperties': False, 'patternProperties': {'^([a-zA-Z0-9]|[a-zA-Z0-9][a-zA-Z0-9._-]*[a-zA-Z0-9])$': {'type': 'array', 'items': {'oneOf': [{'type': 'string', 'description': 'Python package specifiers following PEP 508', 'format': 'pep508'}, {'type': 'object', 'additionalProperties': False, 'properties': {'include-group': {'description': 'Another dependency group to include in this one', 'type': 'string', 'pattern': '^([a-zA-Z0-9]|[a-zA-Z0-9][a-zA-Z0-9._-]*[a-zA-Z0-9])$'}}}]}}}}}, 'project': {'$schema': 'http://json-schema.org/draft-07/schema#', '$id': 'https://packaging.python.org/en/latest/specifications/pyproject-toml/', 'title': 'Package metadata stored in the ``project`` table', '$$description': ['Data structure for the **project** table inside ``pyproject.toml``', '(as initially defined in :pep:`621`)'], 'type': 'object', 'properties': {'name': {'type': 'string', 'description': 'The name (primary identifier) of the project. MUST be statically defined.', 'format': 'pep508-identifier'}, 'version': {'type': 'string', 'description': 'The version of the project as supported by :pep:`440`.', 'format': 'pep440'}, 'description': {'type': 'string', '$$description': ['The `summary description of the project', '<https://packaging.python.org/specifications/core-metadata/#summary>`_']}, 'readme': {'$$description': ['`Full/detailed description of the project in the form of a README', '<https://peps.python.org/pep-0621/#readme>`_', \"with meaning similar to the one defined in `core metadata's Description\", '<https://packaging.python.org/specifications/core-metadata/#description>`_'], 'oneOf': [{'type': 'string', '$$description': ['Relative path to a text file (UTF-8) containing the full description', 'of the project. If the file path ends in case-insensitive ``.md`` or', '``.rst`` suffixes, then the content-type is respectively', '``text/markdown`` or ``text/x-rst``']}, {'type': 'object', 'allOf': [{'anyOf': [{'properties': {'file': {'type': 'string', '$$description': ['Relative path to a text file containing the full description', 'of the project.']}}, 'required': ['file']}, {'properties': {'text': {'type': 'string', 'description': 'Full text describing the project.'}}, 'required': ['text']}]}, {'properties': {'content-type': {'type': 'string', '$$description': ['Content-type (:rfc:`1341`) of the full description', '(e.g. ``text/markdown``). The ``charset`` parameter is assumed', 'UTF-8 when not present.'], '$comment': 'TODO: add regex pattern or format?'}}, 'required': ['content-type']}]}]}, 'requires-python': {'type': 'string', 'format': 'pep508-versionspec', '$$description': ['`The Python version requirements of the project', '<https://packaging.python.org/specifications/core-metadata/#requires-python>`_.']}, 'license': {'description': '`Project license <https://peps.python.org/pep-0621/#license>`_.', 'oneOf': [{'type': 'string', 'description': 'An SPDX license identifier', 'format': 'SPDX'}, {'type': 'object', 'properties': {'file': {'type': 'string', '$$description': ['Relative path to the file (UTF-8) which contains the license for the', 'project.']}}, 'required': ['file']}, {'type': 'object', 'properties': {'text': {'type': 'string', '$$description': ['The license of the project whose meaning is that of the', '`License field from the core metadata', '<https://packaging.python.org/specifications/core-metadata/#license>`_.']}}, 'required': ['text']}]}, 'license-files': {'description': 'Paths or globs to paths of license files', 'type': 'array', 'items': {'type': 'string'}}, 'authors': {'type': 'array', 'items': {'$ref': '#/definitions/author'}, '$$description': [\"The people or organizations considered to be the 'authors' of the project.\", 'The exact meaning is open to interpretation (e.g. original or primary authors,', 'current maintainers, or owners of the package).']}, 'maintainers': {'type': 'array', 'items': {'$ref': '#/definitions/author'}, '$$description': [\"The people or organizations considered to be the 'maintainers' of the project.\", 'Similarly to ``authors``, the exact meaning is open to interpretation.']}, 'keywords': {'type': 'array', 'items': {'type': 'string'}, 'description': 'List of keywords to assist searching for the distribution in a larger catalog.'}, 'classifiers': {'type': 'array', 'items': {'type': 'string', 'format': 'trove-classifier', 'description': '`PyPI classifier <https://pypi.org/classifiers/>`_.'}, '$$description': ['`Trove classifiers <https://pypi.org/classifiers/>`_', 'which apply to the project.']}, 'urls': {'type': 'object', 'description': 'URLs associated with the project in the form ``label => value``.', 'additionalProperties': False, 'patternProperties': {'^.+$': {'type': 'string', 'format': 'url'}}}, 'scripts': {'$ref': '#/definitions/entry-point-group', '$$description': ['Instruct the installer to create command-line wrappers for the given', '`entry points <https://packaging.python.org/specifications/entry-points/>`_.']}, 'gui-scripts': {'$ref': '#/definitions/entry-point-group', '$$description': ['Instruct the installer to create GUI wrappers for the given', '`entry points <https://packaging.python.org/specifications/entry-points/>`_.', 'The difference between ``scripts`` and ``gui-scripts`` is only relevant in', 'Windows.']}, 'entry-points': {'$$description': ['Instruct the installer to expose the given modules/functions via', '``entry-point`` discovery mechanism (useful for plugins).', 'More information available in the `Python packaging guide', '<https://packaging.python.org/specifications/entry-points/>`_.'], 'propertyNames': {'format': 'python-entrypoint-group'}, 'additionalProperties': False, 'patternProperties': {'^.+$': {'$ref': '#/definitions/entry-point-group'}}}, 'dependencies': {'type': 'array', 'description': 'Project (mandatory) dependencies.', 'items': {'$ref': '#/definitions/dependency'}}, 'optional-dependencies': {'type': 'object', 'description': 'Optional dependency for the project', 'propertyNames': {'format': 'pep508-identifier'}, 'additionalProperties': False, 'patternProperties': {'^.+$': {'type': 'array', 'items': {'$ref': '#/definitions/dependency'}}}}, 'dynamic': {'type': 'array', '$$description': ['Specifies which fields are intentionally unspecified and expected to be', 'dynamically provided by build tools'], 'items': {'enum': ['version', 'description', 'readme', 'requires-python', 'license', 'license-files', 'authors', 'maintainers', 'keywords', 'classifiers', 'urls', 'scripts', 'gui-scripts', 'entry-points', 'dependencies', 'optional-dependencies']}}}, 'required': ['name'], 'additionalProperties': False, 'allOf': [{'if': {'not': {'required': ['dynamic'], 'properties': {'dynamic': {'contains': {'const': 'version'}, '$$description': ['version is listed in ``dynamic``']}}}, '$$comment': ['According to :pep:`621`:', '    If the core metadata specification lists a field as \"Required\", then', '    the metadata MUST specify the field statically or list it in dynamic', 'In turn, `core metadata`_ defines:', '    The required fields are: Metadata-Version, Name, Version.', '    All the other fields are optional.', 'Since ``Metadata-Version`` is defined by the build back-end, ``name`` and', '``version`` are the only mandatory information in ``pyproject.toml``.', '.. _core metadata: https://packaging.python.org/specifications/core-metadata/']}, 'then': {'required': ['version'], '$$description': ['version should be statically defined in the ``version`` field']}}, {'if': {'required': ['license-files']}, 'then': {'properties': {'license': {'type': 'string'}}}}], 'definitions': {'author': {'$id': '#/definitions/author', 'title': 'Author or Maintainer', '$comment': 'https://peps.python.org/pep-0621/#authors-maintainers', 'type': 'object', 'additionalProperties': False, 'properties': {'name': {'type': 'string', '$$description': ['MUST be a valid email name, i.e. whatever can be put as a name, before an', 'email, in :rfc:`822`.']}, 'email': {'type': 'string', 'format': 'idn-email', 'description': 'MUST be a valid email address'}}}, 'entry-point-group': {'$id': '#/definitions/entry-point-group', 'title': 'Entry-points', 'type': 'object', '$$description': ['Entry-points are grouped together to indicate what sort of capabilities they', 'provide.', 'See the `packaging guides', '<https://packaging.python.org/specifications/entry-points/>`_', 'and `setuptools docs', '<https://setuptools.pypa.io/en/latest/userguide/entry_point.html>`_', 'for more information.'], 'propertyNames': {'format': 'python-entrypoint-name'}, 'additionalProperties': False, 'patternProperties': {'^.+$': {'type': 'string', '$$description': ['Reference to a Python object. It is either in the form', '``importable.module``, or ``importable.module:object.attr``.'], 'format': 'python-entrypoint-reference', '$comment': 'https://packaging.python.org/specifications/entry-points/'}}}, 'dependency': {'$id': '#/definitions/dependency', 'title': 'Dependency', 'type': 'string', 'description': 'Project dependency specification according to PEP 508', 'format': 'pep508'}}}}, rule='additionalProperties')\n./.venv/lib/python3.13/site-packages/setuptools/config/_validate_pyproject/fastjsonschema_validations.py:        raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \" must be object\", value=data, name=\"\" + (name_prefix or \"data\") + \"\", definition={'$schema': 'http://json-schema.org/draft-07/schema#', '$id': 'https://setuptools.pypa.io/en/latest/userguide/pyproject_config.html', 'title': '``tool.setuptools`` table', '$$description': ['``setuptools``-specific configurations that can be set by users that require', 'customization.', 'These configurations are completely optional and probably can be skipped when', 'creating simple packages. They are equivalent to some of the `Keywords', '<https://setuptools.pypa.io/en/latest/references/keywords.html>`_', 'used by the ``setup.py`` file, and can be set via the ``tool.setuptools`` table.', 'It considers only ``setuptools`` `parameters', '<https://setuptools.pypa.io/en/latest/userguide/pyproject_config.html#setuptools-specific-configuration>`_', 'that are not covered by :pep:`621`; and intentionally excludes ``dependency_links``', 'and ``setup_requires`` (incompatible with modern workflows/standards).'], 'type': 'object', 'additionalProperties': False, 'properties': {'platforms': {'type': 'array', 'items': {'type': 'string'}}, 'provides': {'$$description': ['Package and virtual package names contained within this package', '**(not supported by pip)**'], 'type': 'array', 'items': {'type': 'string', 'format': 'pep508-identifier'}}, 'obsoletes': {'$$description': ['Packages which this package renders obsolete', '**(not supported by pip)**'], 'type': 'array', 'items': {'type': 'string', 'format': 'pep508-identifier'}}, 'zip-safe': {'$$description': ['Whether the project can be safely installed and run from a zip file.', '**OBSOLETE**: only relevant for ``pkg_resources``, ``easy_install`` and', '``setup.py install`` in the context of ``eggs`` (**DEPRECATED**).'], 'type': 'boolean'}, 'script-files': {'$$description': ['Legacy way of defining scripts (entry-points are preferred).', 'Equivalent to the ``script`` keyword in ``setup.py``', '(it was renamed to avoid confusion with entry-point based ``project.scripts``', 'defined in :pep:`621`).', '**DISCOURAGED**: generic script wrappers are tricky and may not work properly.', 'Whenever possible, please use ``project.scripts`` instead.'], 'type': 'array', 'items': {'type': 'string'}, '$comment': 'TODO: is this field deprecated/should be removed?'}, 'eager-resources': {'$$description': ['Resources that should be extracted together, if any of them is needed,', 'or if any C extensions included in the project are imported.', '**OBSOLETE**: only relevant for ``pkg_resources``, ``easy_install`` and', '``setup.py install`` in the context of ``eggs`` (**DEPRECATED**).'], 'type': 'array', 'items': {'type': 'string'}}, 'packages': {'$$description': ['Packages that should be included in the distribution.', 'It can be given either as a list of package identifiers', 'or as a ``dict``-like structure with a single key ``find``', 'which corresponds to a dynamic call to', '``setuptools.config.expand.find_packages`` function.', 'The ``find`` key is associated with a nested ``dict``-like structure that can', 'contain ``where``, ``include``, ``exclude`` and ``namespaces`` keys,', 'mimicking the keyword arguments of the associated function.'], 'oneOf': [{'title': 'Array of Python package identifiers', 'type': 'array', 'items': {'$id': '#/definitions/package-name', 'title': 'Valid package name', 'description': 'Valid package name (importable or :pep:`561`).', 'type': 'string', 'anyOf': [{'type': 'string', 'format': 'python-module-name-relaxed'}, {'type': 'string', 'format': 'pep561-stub-name'}]}}, {'$id': '#/definitions/find-directive', 'title': \"'find:' directive\", 'type': 'object', 'additionalProperties': False, 'properties': {'find': {'type': 'object', '$$description': ['Dynamic `package discovery', '<https://setuptools.pypa.io/en/latest/userguide/package_discovery.html>`_.'], 'additionalProperties': False, 'properties': {'where': {'description': 'Directories to be searched for packages (Unix-style relative path)', 'type': 'array', 'items': {'type': 'string'}}, 'exclude': {'type': 'array', '$$description': ['Exclude packages that match the values listed in this field.', \"Can container shell-style wildcards (e.g. ``'pkg.*'``)\"], 'items': {'type': 'string'}}, 'include': {'type': 'array', '$$description': ['Restrict the found packages to just the ones listed in this field.', \"Can container shell-style wildcards (e.g. ``'pkg.*'``)\"], 'items': {'type': 'string'}}, 'namespaces': {'type': 'boolean', '$$description': ['When ``True``, directories without a ``__init__.py`` file will also', 'be scanned for :pep:`420`-style implicit namespaces']}}}}}]}, 'package-dir': {'$$description': [':class:`dict`-like structure mapping from package names to directories where their', 'code can be found.', 'The empty string (as key) means that all packages are contained inside', 'the given directory will be included in the distribution.'], 'type': 'object', 'additionalProperties': False, 'propertyNames': {'anyOf': [{'const': ''}, {'$id': '#/definitions/package-name', 'title': 'Valid package name', 'description': 'Valid package name (importable or :pep:`561`).', 'type': 'string', 'anyOf': [{'type': 'string', 'format': 'python-module-name-relaxed'}, {'type': 'string', 'format': 'pep561-stub-name'}]}]}, 'patternProperties': {'^.*$': {'type': 'string'}}}, 'package-data': {'$$description': ['Mapping from package names to lists of glob patterns.', 'Usually this option is not needed when using ``include-package-data = true``', 'For more information on how to include data files, check ``setuptools`` `docs', '<https://setuptools.pypa.io/en/latest/userguide/datafiles.html>`_.'], 'type': 'object', 'additionalProperties': False, 'propertyNames': {'anyOf': [{'type': 'string', 'format': 'python-module-name'}, {'const': '*'}]}, 'patternProperties': {'^.*$': {'type': 'array', 'items': {'type': 'string'}}}}, 'include-package-data': {'$$description': ['Automatically include any data files inside the package directories', 'that are specified by ``MANIFEST.in``', 'For more information on how to include data files, check ``setuptools`` `docs', '<https://setuptools.pypa.io/en/latest/userguide/datafiles.html>`_.'], 'type': 'boolean'}, 'exclude-package-data': {'$$description': ['Mapping from package names to lists of glob patterns that should be excluded', 'For more information on how to include data files, check ``setuptools`` `docs', '<https://setuptools.pypa.io/en/latest/userguide/datafiles.html>`_.'], 'type': 'object', 'additionalProperties': False, 'propertyNames': {'anyOf': [{'type': 'string', 'format': 'python-module-name'}, {'const': '*'}]}, 'patternProperties': {'^.*$': {'type': 'array', 'items': {'type': 'string'}}}}, 'namespace-packages': {'type': 'array', 'items': {'type': 'string', 'format': 'python-module-name-relaxed'}, '$comment': 'https://setuptools.pypa.io/en/latest/userguide/package_discovery.html', 'description': '**DEPRECATED**: use implicit namespaces instead (:pep:`420`).'}, 'py-modules': {'description': 'Modules that setuptools will manipulate', 'type': 'array', 'items': {'type': 'string', 'format': 'python-module-name-relaxed'}, '$comment': 'TODO: clarify the relationship with ``packages``'}, 'ext-modules': {'description': 'Extension modules to be compiled by setuptools', 'type': 'array', 'items': {'$id': '#/definitions/ext-module', 'title': 'Extension module', 'description': 'Parameters to construct a :class:`setuptools.Extension` object', 'type': 'object', 'required': ['name', 'sources'], 'additionalProperties': False, 'properties': {'name': {'type': 'string', 'format': 'python-module-name-relaxed'}, 'sources': {'type': 'array', 'items': {'type': 'string'}}, 'include-dirs': {'type': 'array', 'items': {'type': 'string'}}, 'define-macros': {'type': 'array', 'items': {'type': 'array', 'items': [{'description': 'macro name', 'type': 'string'}, {'description': 'macro value', 'oneOf': [{'type': 'string'}, {'type': 'null'}]}], 'additionalItems': False}}, 'undef-macros': {'type': 'array', 'items': {'type': 'string'}}, 'library-dirs': {'type': 'array', 'items': {'type': 'string'}}, 'libraries': {'type': 'array', 'items': {'type': 'string'}}, 'runtime-library-dirs': {'type': 'array', 'items': {'type': 'string'}}, 'extra-objects': {'type': 'array', 'items': {'type': 'string'}}, 'extra-compile-args': {'type': 'array', 'items': {'type': 'string'}}, 'extra-link-args': {'type': 'array', 'items': {'type': 'string'}}, 'export-symbols': {'type': 'array', 'items': {'type': 'string'}}, 'swig-opts': {'type': 'array', 'items': {'type': 'string'}}, 'depends': {'type': 'array', 'items': {'type': 'string'}}, 'language': {'type': 'string'}, 'optional': {'type': 'boolean'}, 'py-limited-api': {'type': 'boolean'}}}}, 'data-files': {'$$description': ['``dict``-like structure where each key represents a directory and', 'the value is a list of glob patterns that should be installed in them.', '**DISCOURAGED**: please notice this might not work as expected with wheels.', 'Whenever possible, consider using data files inside the package directories', '(or create a new namespace package that only contains data files).', 'See `data files support', '<https://setuptools.pypa.io/en/latest/userguide/datafiles.html>`_.'], 'type': 'object', 'patternProperties': {'^.*$': {'type': 'array', 'items': {'type': 'string'}}}}, 'cmdclass': {'$$description': ['Mapping of distutils-style command names to ``setuptools.Command`` subclasses', 'which in turn should be represented by strings with a qualified class name', '(i.e., \"dotted\" form with module), e.g.::\\n\\n', '    cmdclass = {mycmd = \"pkg.subpkg.module.CommandClass\"}\\n\\n', 'The command class should be a directly defined at the top-level of the', 'containing module (no class nesting).'], 'type': 'object', 'patternProperties': {'^.*$': {'type': 'string', 'format': 'python-qualified-identifier'}}}, 'license-files': {'type': 'array', 'items': {'type': 'string'}, '$$description': ['**PROVISIONAL**: list of glob patterns for all license files being distributed.', '(likely to become standard with :pep:`639`).', \"By default: ``['LICEN[CS]E*', 'COPYING*', 'NOTICE*', 'AUTHORS*']``\"], '$comment': 'TODO: revise if PEP 639 is accepted. Probably ``project.license-files``?'}, 'dynamic': {'type': 'object', 'description': 'Instructions for loading :pep:`621`-related metadata dynamically', 'additionalProperties': False, 'properties': {'version': {'$$description': ['A version dynamically loaded via either the ``attr:`` or ``file:``', 'directives. Please make sure the given file or attribute respects :pep:`440`.', 'Also ensure to set ``project.dynamic`` accordingly.'], 'oneOf': [{'title': \"'attr:' directive\", '$id': '#/definitions/attr-directive', '$$description': ['Value is read from a module attribute. Supports callables and iterables;', 'unsupported types are cast via ``str()``'], 'type': 'object', 'additionalProperties': False, 'properties': {'attr': {'type': 'string', 'format': 'python-qualified-identifier'}}, 'required': ['attr']}, {'$id': '#/definitions/file-directive', 'title': \"'file:' directive\", 'description': 'Value is read from a file (or list of files and then concatenated)', 'type': 'object', 'additionalProperties': False, 'properties': {'file': {'oneOf': [{'type': 'string'}, {'type': 'array', 'items': {'type': 'string'}}]}}, 'required': ['file']}]}, 'classifiers': {'$id': '#/definitions/file-directive', 'title': \"'file:' directive\", 'description': 'Value is read from a file (or list of files and then concatenated)', 'type': 'object', 'additionalProperties': False, 'properties': {'file': {'oneOf': [{'type': 'string'}, {'type': 'array', 'items': {'type': 'string'}}]}}, 'required': ['file']}, 'description': {'$id': '#/definitions/file-directive', 'title': \"'file:' directive\", 'description': 'Value is read from a file (or list of files and then concatenated)', 'type': 'object', 'additionalProperties': False, 'properties': {'file': {'oneOf': [{'type': 'string'}, {'type': 'array', 'items': {'type': 'string'}}]}}, 'required': ['file']}, 'entry-points': {'$id': '#/definitions/file-directive', 'title': \"'file:' directive\", 'description': 'Value is read from a file (or list of files and then concatenated)', 'type': 'object', 'additionalProperties': False, 'properties': {'file': {'oneOf': [{'type': 'string'}, {'type': 'array', 'items': {'type': 'string'}}]}}, 'required': ['file']}, 'dependencies': {'title': \"'file:' directive for dependencies\", 'allOf': [{'$$description': ['**BETA**: subset of the ``requirements.txt`` format', 'without ``pip`` flags and options', '(one :pep:`508`-compliant string per line,', 'lines that are blank or start with ``#`` are excluded).', 'See `dynamic metadata', '<https://setuptools.pypa.io/en/latest/userguide/pyproject_config.html#dynamic-metadata>`_.']}, {'$ref': '#/definitions/file-directive'}]}, 'optional-dependencies': {'type': 'object', 'propertyNames': {'type': 'string', 'format': 'pep508-identifier'}, 'additionalProperties': False, 'patternProperties': {'.+': {'title': \"'file:' directive for dependencies\", 'allOf': [{'$$description': ['**BETA**: subset of the ``requirements.txt`` format', 'without ``pip`` flags and options', '(one :pep:`508`-compliant string per line,', 'lines that are blank or start with ``#`` are excluded).', 'See `dynamic metadata', '<https://setuptools.pypa.io/en/latest/userguide/pyproject_config.html#dynamic-metadata>`_.']}, {'$ref': '#/definitions/file-directive'}]}}}, 'readme': {'type': 'object', 'anyOf': [{'$id': '#/definitions/file-directive', 'title': \"'file:' directive\", 'description': 'Value is read from a file (or list of files and then concatenated)', 'type': 'object', 'additionalProperties': False, 'properties': {'file': {'oneOf': [{'type': 'string'}, {'type': 'array', 'items': {'type': 'string'}}]}}, 'required': ['file']}, {'type': 'object', 'properties': {'content-type': {'type': 'string'}, 'file': {'oneOf': [{'type': 'string'}, {'type': 'array', 'items': {'type': 'string'}}]}}, 'additionalProperties': False}], 'required': ['file']}}}}, 'definitions': {'package-name': {'$id': '#/definitions/package-name', 'title': 'Valid package name', 'description': 'Valid package name (importable or :pep:`561`).', 'type': 'string', 'anyOf': [{'type': 'string', 'format': 'python-module-name-relaxed'}, {'type': 'string', 'format': 'pep561-stub-name'}]}, 'ext-module': {'$id': '#/definitions/ext-module', 'title': 'Extension module', 'description': 'Parameters to construct a :class:`setuptools.Extension` object', 'type': 'object', 'required': ['name', 'sources'], 'additionalProperties': False, 'properties': {'name': {'type': 'string', 'format': 'python-module-name-relaxed'}, 'sources': {'type': 'array', 'items': {'type': 'string'}}, 'include-dirs': {'type': 'array', 'items': {'type': 'string'}}, 'define-macros': {'type': 'array', 'items': {'type': 'array', 'items': [{'description': 'macro name', 'type': 'string'}, {'description': 'macro value', 'oneOf': [{'type': 'string'}, {'type': 'null'}]}], 'additionalItems': False}}, 'undef-macros': {'type': 'array', 'items': {'type': 'string'}}, 'library-dirs': {'type': 'array', 'items': {'type': 'string'}}, 'libraries': {'type': 'array', 'items': {'type': 'string'}}, 'runtime-library-dirs': {'type': 'array', 'items': {'type': 'string'}}, 'extra-objects': {'type': 'array', 'items': {'type': 'string'}}, 'extra-compile-args': {'type': 'array', 'items': {'type': 'string'}}, 'extra-link-args': {'type': 'array', 'items': {'type': 'string'}}, 'export-symbols': {'type': 'array', 'items': {'type': 'string'}}, 'swig-opts': {'type': 'array', 'items': {'type': 'string'}}, 'depends': {'type': 'array', 'items': {'type': 'string'}}, 'language': {'type': 'string'}, 'optional': {'type': 'boolean'}, 'py-limited-api': {'type': 'boolean'}}}, 'file-directive': {'$id': '#/definitions/file-directive', 'title': \"'file:' directive\", 'description': 'Value is read from a file (or list of files and then concatenated)', 'type': 'object', 'additionalProperties': False, 'properties': {'file': {'oneOf': [{'type': 'string'}, {'type': 'array', 'items': {'type': 'string'}}]}}, 'required': ['file']}, 'file-directive-for-dependencies': {'title': \"'file:' directive for dependencies\", 'allOf': [{'$$description': ['**BETA**: subset of the ``requirements.txt`` format', 'without ``pip`` flags and options', '(one :pep:`508`-compliant string per line,', 'lines that are blank or start with ``#`` are excluded).', 'See `dynamic metadata', '<https://setuptools.pypa.io/en/latest/userguide/pyproject_config.html#dynamic-metadata>`_.']}, {'$id': '#/definitions/file-directive', 'title': \"'file:' directive\", 'description': 'Value is read from a file (or list of files and then concatenated)', 'type': 'object', 'additionalProperties': False, 'properties': {'file': {'oneOf': [{'type': 'string'}, {'type': 'array', 'items': {'type': 'string'}}]}}, 'required': ['file']}]}, 'attr-directive': {'title': \"'attr:' directive\", '$id': '#/definitions/attr-directive', '$$description': ['Value is read from a module attribute. Supports callables and iterables;', 'unsupported types are cast via ``str()``'], 'type': 'object', 'additionalProperties': False, 'properties': {'attr': {'type': 'string', 'format': 'python-qualified-identifier'}}, 'required': ['attr']}, 'find-directive': {'$id': '#/definitions/find-directive', 'title': \"'find:' directive\", 'type': 'object', 'additionalProperties': False, 'properties': {'find': {'type': 'object', '$$description': ['Dynamic `package discovery', '<https://setuptools.pypa.io/en/latest/userguide/package_discovery.html>`_.'], 'additionalProperties': False, 'properties': {'where': {'description': 'Directories to be searched for packages (Unix-style relative path)', 'type': 'array', 'items': {'type': 'string'}}, 'exclude': {'type': 'array', '$$description': ['Exclude packages that match the values listed in this field.', \"Can container shell-style wildcards (e.g. ``'pkg.*'``)\"], 'items': {'type': 'string'}}, 'include': {'type': 'array', '$$description': ['Restrict the found packages to just the ones listed in this field.', \"Can container shell-style wildcards (e.g. ``'pkg.*'``)\"], 'items': {'type': 'string'}}, 'namespaces': {'type': 'boolean', '$$description': ['When ``True``, directories without a ``__init__.py`` file will also', 'be scanned for :pep:`420`-style implicit namespaces']}}}}}}}, rule='type')\n./.venv/lib/python3.13/site-packages/setuptools/config/_validate_pyproject/fastjsonschema_validations.py:                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".script-files must be array\", value=data__scriptfiles, name=\"\" + (name_prefix or \"data\") + \".script-files\", definition={'$$description': ['Legacy way of defining scripts (entry-points are preferred).', 'Equivalent to the ``script`` keyword in ``setup.py``', '(it was renamed to avoid confusion with entry-point based ``project.scripts``', 'defined in :pep:`621`).', '**DISCOURAGED**: generic script wrappers are tricky and may not work properly.', 'Whenever possible, please use ``project.scripts`` instead.'], 'type': 'array', 'items': {'type': 'string'}, '$comment': 'TODO: is this field deprecated/should be removed?'}, rule='type')\n./.venv/lib/python3.13/site-packages/setuptools/config/_validate_pyproject/fastjsonschema_validations.py:                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".py-modules must be array\", value=data__pymodules, name=\"\" + (name_prefix or \"data\") + \".py-modules\", definition={'description': 'Modules that setuptools will manipulate', 'type': 'array', 'items': {'type': 'string', 'format': 'python-module-name-relaxed'}, '$comment': 'TODO: clarify the relationship with ``packages``'}, rule='type')\n./.venv/lib/python3.13/site-packages/setuptools/config/_validate_pyproject/fastjsonschema_validations.py:                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".license-files must be array\", value=data__licensefiles, name=\"\" + (name_prefix or \"data\") + \".license-files\", definition={'type': 'array', 'items': {'type': 'string'}, '$$description': ['**PROVISIONAL**: list of glob patterns for all license files being distributed.', '(likely to become standard with :pep:`639`).', \"By default: ``['LICEN[CS]E*', 'COPYING*', 'NOTICE*', 'AUTHORS*']``\"], '$comment': 'TODO: revise if PEP 639 is accepted. Probably ``project.license-files``?'}, rule='type')\n./.venv/lib/python3.13/site-packages/setuptools/config/_validate_pyproject/fastjsonschema_validations.py:            raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \" must not contain \"+str(data_keys)+\" properties\", value=data, name=\"\" + (name_prefix or \"data\") + \"\", definition={'$schema': 'http://json-schema.org/draft-07/schema#', '$id': 'https://setuptools.pypa.io/en/latest/userguide/pyproject_config.html', 'title': '``tool.setuptools`` table', '$$description': ['``setuptools``-specific configurations that can be set by users that require', 'customization.', 'These configurations are completely optional and probably can be skipped when', 'creating simple packages. They are equivalent to some of the `Keywords', '<https://setuptools.pypa.io/en/latest/references/keywords.html>`_', 'used by the ``setup.py`` file, and can be set via the ``tool.setuptools`` table.', 'It considers only ``setuptools`` `parameters', '<https://setuptools.pypa.io/en/latest/userguide/pyproject_config.html#setuptools-specific-configuration>`_', 'that are not covered by :pep:`621`; and intentionally excludes ``dependency_links``', 'and ``setup_requires`` (incompatible with modern workflows/standards).'], 'type': 'object', 'additionalProperties': False, 'properties': {'platforms': {'type': 'array', 'items': {'type': 'string'}}, 'provides': {'$$description': ['Package and virtual package names contained within this package', '**(not supported by pip)**'], 'type': 'array', 'items': {'type': 'string', 'format': 'pep508-identifier'}}, 'obsoletes': {'$$description': ['Packages which this package renders obsolete', '**(not supported by pip)**'], 'type': 'array', 'items': {'type': 'string', 'format': 'pep508-identifier'}}, 'zip-safe': {'$$description': ['Whether the project can be safely installed and run from a zip file.', '**OBSOLETE**: only relevant for ``pkg_resources``, ``easy_install`` and', '``setup.py install`` in the context of ``eggs`` (**DEPRECATED**).'], 'type': 'boolean'}, 'script-files': {'$$description': ['Legacy way of defining scripts (entry-points are preferred).', 'Equivalent to the ``script`` keyword in ``setup.py``', '(it was renamed to avoid confusion with entry-point based ``project.scripts``', 'defined in :pep:`621`).', '**DISCOURAGED**: generic script wrappers are tricky and may not work properly.', 'Whenever possible, please use ``project.scripts`` instead.'], 'type': 'array', 'items': {'type': 'string'}, '$comment': 'TODO: is this field deprecated/should be removed?'}, 'eager-resources': {'$$description': ['Resources that should be extracted together, if any of them is needed,', 'or if any C extensions included in the project are imported.', '**OBSOLETE**: only relevant for ``pkg_resources``, ``easy_install`` and', '``setup.py install`` in the context of ``eggs`` (**DEPRECATED**).'], 'type': 'array', 'items': {'type': 'string'}}, 'packages': {'$$description': ['Packages that should be included in the distribution.', 'It can be given either as a list of package identifiers', 'or as a ``dict``-like structure with a single key ``find``', 'which corresponds to a dynamic call to', '``setuptools.config.expand.find_packages`` function.', 'The ``find`` key is associated with a nested ``dict``-like structure that can', 'contain ``where``, ``include``, ``exclude`` and ``namespaces`` keys,', 'mimicking the keyword arguments of the associated function.'], 'oneOf': [{'title': 'Array of Python package identifiers', 'type': 'array', 'items': {'$id': '#/definitions/package-name', 'title': 'Valid package name', 'description': 'Valid package name (importable or :pep:`561`).', 'type': 'string', 'anyOf': [{'type': 'string', 'format': 'python-module-name-relaxed'}, {'type': 'string', 'format': 'pep561-stub-name'}]}}, {'$id': '#/definitions/find-directive', 'title': \"'find:' directive\", 'type': 'object', 'additionalProperties': False, 'properties': {'find': {'type': 'object', '$$description': ['Dynamic `package discovery', '<https://setuptools.pypa.io/en/latest/userguide/package_discovery.html>`_.'], 'additionalProperties': False, 'properties': {'where': {'description': 'Directories to be searched for packages (Unix-style relative path)', 'type': 'array', 'items': {'type': 'string'}}, 'exclude': {'type': 'array', '$$description': ['Exclude packages that match the values listed in this field.', \"Can container shell-style wildcards (e.g. ``'pkg.*'``)\"], 'items': {'type': 'string'}}, 'include': {'type': 'array', '$$description': ['Restrict the found packages to just the ones listed in this field.', \"Can container shell-style wildcards (e.g. ``'pkg.*'``)\"], 'items': {'type': 'string'}}, 'namespaces': {'type': 'boolean', '$$description': ['When ``True``, directories without a ``__init__.py`` file will also', 'be scanned for :pep:`420`-style implicit namespaces']}}}}}]}, 'package-dir': {'$$description': [':class:`dict`-like structure mapping from package names to directories where their', 'code can be found.', 'The empty string (as key) means that all packages are contained inside', 'the given directory will be included in the distribution.'], 'type': 'object', 'additionalProperties': False, 'propertyNames': {'anyOf': [{'const': ''}, {'$id': '#/definitions/package-name', 'title': 'Valid package name', 'description': 'Valid package name (importable or :pep:`561`).', 'type': 'string', 'anyOf': [{'type': 'string', 'format': 'python-module-name-relaxed'}, {'type': 'string', 'format': 'pep561-stub-name'}]}]}, 'patternProperties': {'^.*$': {'type': 'string'}}}, 'package-data': {'$$description': ['Mapping from package names to lists of glob patterns.', 'Usually this option is not needed when using ``include-package-data = true``', 'For more information on how to include data files, check ``setuptools`` `docs', '<https://setuptools.pypa.io/en/latest/userguide/datafiles.html>`_.'], 'type': 'object', 'additionalProperties': False, 'propertyNames': {'anyOf': [{'type': 'string', 'format': 'python-module-name'}, {'const': '*'}]}, 'patternProperties': {'^.*$': {'type': 'array', 'items': {'type': 'string'}}}}, 'include-package-data': {'$$description': ['Automatically include any data files inside the package directories', 'that are specified by ``MANIFEST.in``', 'For more information on how to include data files, check ``setuptools`` `docs', '<https://setuptools.pypa.io/en/latest/userguide/datafiles.html>`_.'], 'type': 'boolean'}, 'exclude-package-data': {'$$description': ['Mapping from package names to lists of glob patterns that should be excluded', 'For more information on how to include data files, check ``setuptools`` `docs', '<https://setuptools.pypa.io/en/latest/userguide/datafiles.html>`_.'], 'type': 'object', 'additionalProperties': False, 'propertyNames': {'anyOf': [{'type': 'string', 'format': 'python-module-name'}, {'const': '*'}]}, 'patternProperties': {'^.*$': {'type': 'array', 'items': {'type': 'string'}}}}, 'namespace-packages': {'type': 'array', 'items': {'type': 'string', 'format': 'python-module-name-relaxed'}, '$comment': 'https://setuptools.pypa.io/en/latest/userguide/package_discovery.html', 'description': '**DEPRECATED**: use implicit namespaces instead (:pep:`420`).'}, 'py-modules': {'description': 'Modules that setuptools will manipulate', 'type': 'array', 'items': {'type': 'string', 'format': 'python-module-name-relaxed'}, '$comment': 'TODO: clarify the relationship with ``packages``'}, 'ext-modules': {'description': 'Extension modules to be compiled by setuptools', 'type': 'array', 'items': {'$id': '#/definitions/ext-module', 'title': 'Extension module', 'description': 'Parameters to construct a :class:`setuptools.Extension` object', 'type': 'object', 'required': ['name', 'sources'], 'additionalProperties': False, 'properties': {'name': {'type': 'string', 'format': 'python-module-name-relaxed'}, 'sources': {'type': 'array', 'items': {'type': 'string'}}, 'include-dirs': {'type': 'array', 'items': {'type': 'string'}}, 'define-macros': {'type': 'array', 'items': {'type': 'array', 'items': [{'description': 'macro name', 'type': 'string'}, {'description': 'macro value', 'oneOf': [{'type': 'string'}, {'type': 'null'}]}], 'additionalItems': False}}, 'undef-macros': {'type': 'array', 'items': {'type': 'string'}}, 'library-dirs': {'type': 'array', 'items': {'type': 'string'}}, 'libraries': {'type': 'array', 'items': {'type': 'string'}}, 'runtime-library-dirs': {'type': 'array', 'items': {'type': 'string'}}, 'extra-objects': {'type': 'array', 'items': {'type': 'string'}}, 'extra-compile-args': {'type': 'array', 'items': {'type': 'string'}}, 'extra-link-args': {'type': 'array', 'items': {'type': 'string'}}, 'export-symbols': {'type': 'array', 'items': {'type': 'string'}}, 'swig-opts': {'type': 'array', 'items': {'type': 'string'}}, 'depends': {'type': 'array', 'items': {'type': 'string'}}, 'language': {'type': 'string'}, 'optional': {'type': 'boolean'}, 'py-limited-api': {'type': 'boolean'}}}}, 'data-files': {'$$description': ['``dict``-like structure where each key represents a directory and', 'the value is a list of glob patterns that should be installed in them.', '**DISCOURAGED**: please notice this might not work as expected with wheels.', 'Whenever possible, consider using data files inside the package directories', '(or create a new namespace package that only contains data files).', 'See `data files support', '<https://setuptools.pypa.io/en/latest/userguide/datafiles.html>`_.'], 'type': 'object', 'patternProperties': {'^.*$': {'type': 'array', 'items': {'type': 'string'}}}}, 'cmdclass': {'$$description': ['Mapping of distutils-style command names to ``setuptools.Command`` subclasses', 'which in turn should be represented by strings with a qualified class name', '(i.e., \"dotted\" form with module), e.g.::\\n\\n', '    cmdclass = {mycmd = \"pkg.subpkg.module.CommandClass\"}\\n\\n', 'The command class should be a directly defined at the top-level of the', 'containing module (no class nesting).'], 'type': 'object', 'patternProperties': {'^.*$': {'type': 'string', 'format': 'python-qualified-identifier'}}}, 'license-files': {'type': 'array', 'items': {'type': 'string'}, '$$description': ['**PROVISIONAL**: list of glob patterns for all license files being distributed.', '(likely to become standard with :pep:`639`).', \"By default: ``['LICEN[CS]E*', 'COPYING*', 'NOTICE*', 'AUTHORS*']``\"], '$comment': 'TODO: revise if PEP 639 is accepted. Probably ``project.license-files``?'}, 'dynamic': {'type': 'object', 'description': 'Instructions for loading :pep:`621`-related metadata dynamically', 'additionalProperties': False, 'properties': {'version': {'$$description': ['A version dynamically loaded via either the ``attr:`` or ``file:``', 'directives. Please make sure the given file or attribute respects :pep:`440`.', 'Also ensure to set ``project.dynamic`` accordingly.'], 'oneOf': [{'title': \"'attr:' directive\", '$id': '#/definitions/attr-directive', '$$description': ['Value is read from a module attribute. Supports callables and iterables;', 'unsupported types are cast via ``str()``'], 'type': 'object', 'additionalProperties': False, 'properties': {'attr': {'type': 'string', 'format': 'python-qualified-identifier'}}, 'required': ['attr']}, {'$id': '#/definitions/file-directive', 'title': \"'file:' directive\", 'description': 'Value is read from a file (or list of files and then concatenated)', 'type': 'object', 'additionalProperties': False, 'properties': {'file': {'oneOf': [{'type': 'string'}, {'type': 'array', 'items': {'type': 'string'}}]}}, 'required': ['file']}]}, 'classifiers': {'$id': '#/definitions/file-directive', 'title': \"'file:' directive\", 'description': 'Value is read from a file (or list of files and then concatenated)', 'type': 'object', 'additionalProperties': False, 'properties': {'file': {'oneOf': [{'type': 'string'}, {'type': 'array', 'items': {'type': 'string'}}]}}, 'required': ['file']}, 'description': {'$id': '#/definitions/file-directive', 'title': \"'file:' directive\", 'description': 'Value is read from a file (or list of files and then concatenated)', 'type': 'object', 'additionalProperties': False, 'properties': {'file': {'oneOf': [{'type': 'string'}, {'type': 'array', 'items': {'type': 'string'}}]}}, 'required': ['file']}, 'entry-points': {'$id': '#/definitions/file-directive', 'title': \"'file:' directive\", 'description': 'Value is read from a file (or list of files and then concatenated)', 'type': 'object', 'additionalProperties': False, 'properties': {'file': {'oneOf': [{'type': 'string'}, {'type': 'array', 'items': {'type': 'string'}}]}}, 'required': ['file']}, 'dependencies': {'title': \"'file:' directive for dependencies\", 'allOf': [{'$$description': ['**BETA**: subset of the ``requirements.txt`` format', 'without ``pip`` flags and options', '(one :pep:`508`-compliant string per line,', 'lines that are blank or start with ``#`` are excluded).', 'See `dynamic metadata', '<https://setuptools.pypa.io/en/latest/userguide/pyproject_config.html#dynamic-metadata>`_.']}, {'$ref': '#/definitions/file-directive'}]}, 'optional-dependencies': {'type': 'object', 'propertyNames': {'type': 'string', 'format': 'pep508-identifier'}, 'additionalProperties': False, 'patternProperties': {'.+': {'title': \"'file:' directive for dependencies\", 'allOf': [{'$$description': ['**BETA**: subset of the ``requirements.txt`` format', 'without ``pip`` flags and options', '(one :pep:`508`-compliant string per line,', 'lines that are blank or start with ``#`` are excluded).', 'See `dynamic metadata', '<https://setuptools.pypa.io/en/latest/userguide/pyproject_config.html#dynamic-metadata>`_.']}, {'$ref': '#/definitions/file-directive'}]}}}, 'readme': {'type': 'object', 'anyOf': [{'$id': '#/definitions/file-directive', 'title': \"'file:' directive\", 'description': 'Value is read from a file (or list of files and then concatenated)', 'type': 'object', 'additionalProperties': False, 'properties': {'file': {'oneOf': [{'type': 'string'}, {'type': 'array', 'items': {'type': 'string'}}]}}, 'required': ['file']}, {'type': 'object', 'properties': {'content-type': {'type': 'string'}, 'file': {'oneOf': [{'type': 'string'}, {'type': 'array', 'items': {'type': 'string'}}]}}, 'additionalProperties': False}], 'required': ['file']}}}}, 'definitions': {'package-name': {'$id': '#/definitions/package-name', 'title': 'Valid package name', 'description': 'Valid package name (importable or :pep:`561`).', 'type': 'string', 'anyOf': [{'type': 'string', 'format': 'python-module-name-relaxed'}, {'type': 'string', 'format': 'pep561-stub-name'}]}, 'ext-module': {'$id': '#/definitions/ext-module', 'title': 'Extension module', 'description': 'Parameters to construct a :class:`setuptools.Extension` object', 'type': 'object', 'required': ['name', 'sources'], 'additionalProperties': False, 'properties': {'name': {'type': 'string', 'format': 'python-module-name-relaxed'}, 'sources': {'type': 'array', 'items': {'type': 'string'}}, 'include-dirs': {'type': 'array', 'items': {'type': 'string'}}, 'define-macros': {'type': 'array', 'items': {'type': 'array', 'items': [{'description': 'macro name', 'type': 'string'}, {'description': 'macro value', 'oneOf': [{'type': 'string'}, {'type': 'null'}]}], 'additionalItems': False}}, 'undef-macros': {'type': 'array', 'items': {'type': 'string'}}, 'library-dirs': {'type': 'array', 'items': {'type': 'string'}}, 'libraries': {'type': 'array', 'items': {'type': 'string'}}, 'runtime-library-dirs': {'type': 'array', 'items': {'type': 'string'}}, 'extra-objects': {'type': 'array', 'items': {'type': 'string'}}, 'extra-compile-args': {'type': 'array', 'items': {'type': 'string'}}, 'extra-link-args': {'type': 'array', 'items': {'type': 'string'}}, 'export-symbols': {'type': 'array', 'items': {'type': 'string'}}, 'swig-opts': {'type': 'array', 'items': {'type': 'string'}}, 'depends': {'type': 'array', 'items': {'type': 'string'}}, 'language': {'type': 'string'}, 'optional': {'type': 'boolean'}, 'py-limited-api': {'type': 'boolean'}}}, 'file-directive': {'$id': '#/definitions/file-directive', 'title': \"'file:' directive\", 'description': 'Value is read from a file (or list of files and then concatenated)', 'type': 'object', 'additionalProperties': False, 'properties': {'file': {'oneOf': [{'type': 'string'}, {'type': 'array', 'items': {'type': 'string'}}]}}, 'required': ['file']}, 'file-directive-for-dependencies': {'title': \"'file:' directive for dependencies\", 'allOf': [{'$$description': ['**BETA**: subset of the ``requirements.txt`` format', 'without ``pip`` flags and options', '(one :pep:`508`-compliant string per line,', 'lines that are blank or start with ``#`` are excluded).', 'See `dynamic metadata', '<https://setuptools.pypa.io/en/latest/userguide/pyproject_config.html#dynamic-metadata>`_.']}, {'$id': '#/definitions/file-directive', 'title': \"'file:' directive\", 'description': 'Value is read from a file (or list of files and then concatenated)', 'type': 'object', 'additionalProperties': False, 'properties': {'file': {'oneOf': [{'type': 'string'}, {'type': 'array', 'items': {'type': 'string'}}]}}, 'required': ['file']}]}, 'attr-directive': {'title': \"'attr:' directive\", '$id': '#/definitions/attr-directive', '$$description': ['Value is read from a module attribute. Supports callables and iterables;', 'unsupported types are cast via ``str()``'], 'type': 'object', 'additionalProperties': False, 'properties': {'attr': {'type': 'string', 'format': 'python-qualified-identifier'}}, 'required': ['attr']}, 'find-directive': {'$id': '#/definitions/find-directive', 'title': \"'find:' directive\", 'type': 'object', 'additionalProperties': False, 'properties': {'find': {'type': 'object', '$$description': ['Dynamic `package discovery', '<https://setuptools.pypa.io/en/latest/userguide/package_discovery.html>`_.'], 'additionalProperties': False, 'properties': {'where': {'description': 'Directories to be searched for packages (Unix-style relative path)', 'type': 'array', 'items': {'type': 'string'}}, 'exclude': {'type': 'array', '$$description': ['Exclude packages that match the values listed in this field.', \"Can container shell-style wildcards (e.g. ``'pkg.*'``)\"], 'items': {'type': 'string'}}, 'include': {'type': 'array', '$$description': ['Restrict the found packages to just the ones listed in this field.', \"Can container shell-style wildcards (e.g. ``'pkg.*'``)\"], 'items': {'type': 'string'}}, 'namespaces': {'type': 'boolean', '$$description': ['When ``True``, directories without a ``__init__.py`` file will also', 'be scanned for :pep:`420`-style implicit namespaces']}}}}}}}, rule='additionalProperties')\n./.venv/lib/python3.13/site-packages/setuptools/config/_validate_pyproject/fastjsonschema_validations.py:        raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \" must be object\", value=data, name=\"\" + (name_prefix or \"data\") + \"\", definition={'$schema': 'http://json-schema.org/draft-07/schema#', '$id': 'https://setuptools.pypa.io/en/latest/deprecated/distutils/configfile.html', 'title': '``tool.distutils`` table', '$$description': ['**EXPERIMENTAL** (NOT OFFICIALLY SUPPORTED): Use ``tool.distutils``', 'subtables to configure arguments for ``distutils`` commands.', 'Originally, ``distutils`` allowed developers to configure arguments for', '``setup.py`` commands via `distutils configuration files', '<https://setuptools.pypa.io/en/latest/deprecated/distutils/configfile.html>`_.', 'See also `the old Python docs <https://docs.python.org/3.11/install/>_`.'], 'type': 'object', 'properties': {'global': {'type': 'object', 'description': 'Global options applied to all ``distutils`` commands'}}, 'patternProperties': {'.+': {'type': 'object'}}, '$comment': 'TODO: Is there a practical way of making this schema more specific?'}, rule='type')\n./.venv/lib/python3.13/site-packages/setuptools/config/_validate_pyproject/fastjsonschema_validations.py:        raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \" must be object\", value=data, name=\"\" + (name_prefix or \"data\") + \"\", definition={'$schema': 'http://json-schema.org/draft-07/schema#', '$id': 'https://packaging.python.org/en/latest/specifications/pyproject-toml/', 'title': 'Package metadata stored in the ``project`` table', '$$description': ['Data structure for the **project** table inside ``pyproject.toml``', '(as initially defined in :pep:`621`)'], 'type': 'object', 'properties': {'name': {'type': 'string', 'description': 'The name (primary identifier) of the project. MUST be statically defined.', 'format': 'pep508-identifier'}, 'version': {'type': 'string', 'description': 'The version of the project as supported by :pep:`440`.', 'format': 'pep440'}, 'description': {'type': 'string', '$$description': ['The `summary description of the project', '<https://packaging.python.org/specifications/core-metadata/#summary>`_']}, 'readme': {'$$description': ['`Full/detailed description of the project in the form of a README', '<https://peps.python.org/pep-0621/#readme>`_', \"with meaning similar to the one defined in `core metadata's Description\", '<https://packaging.python.org/specifications/core-metadata/#description>`_'], 'oneOf': [{'type': 'string', '$$description': ['Relative path to a text file (UTF-8) containing the full description', 'of the project. If the file path ends in case-insensitive ``.md`` or', '``.rst`` suffixes, then the content-type is respectively', '``text/markdown`` or ``text/x-rst``']}, {'type': 'object', 'allOf': [{'anyOf': [{'properties': {'file': {'type': 'string', '$$description': ['Relative path to a text file containing the full description', 'of the project.']}}, 'required': ['file']}, {'properties': {'text': {'type': 'string', 'description': 'Full text describing the project.'}}, 'required': ['text']}]}, {'properties': {'content-type': {'type': 'string', '$$description': ['Content-type (:rfc:`1341`) of the full description', '(e.g. ``text/markdown``). The ``charset`` parameter is assumed', 'UTF-8 when not present.'], '$comment': 'TODO: add regex pattern or format?'}}, 'required': ['content-type']}]}]}, 'requires-python': {'type': 'string', 'format': 'pep508-versionspec', '$$description': ['`The Python version requirements of the project', '<https://packaging.python.org/specifications/core-metadata/#requires-python>`_.']}, 'license': {'description': '`Project license <https://peps.python.org/pep-0621/#license>`_.', 'oneOf': [{'type': 'string', 'description': 'An SPDX license identifier', 'format': 'SPDX'}, {'type': 'object', 'properties': {'file': {'type': 'string', '$$description': ['Relative path to the file (UTF-8) which contains the license for the', 'project.']}}, 'required': ['file']}, {'type': 'object', 'properties': {'text': {'type': 'string', '$$description': ['The license of the project whose meaning is that of the', '`License field from the core metadata', '<https://packaging.python.org/specifications/core-metadata/#license>`_.']}}, 'required': ['text']}]}, 'license-files': {'description': 'Paths or globs to paths of license files', 'type': 'array', 'items': {'type': 'string'}}, 'authors': {'type': 'array', 'items': {'$id': '#/definitions/author', 'title': 'Author or Maintainer', '$comment': 'https://peps.python.org/pep-0621/#authors-maintainers', 'type': 'object', 'additionalProperties': False, 'properties': {'name': {'type': 'string', '$$description': ['MUST be a valid email name, i.e. whatever can be put as a name, before an', 'email, in :rfc:`822`.']}, 'email': {'type': 'string', 'format': 'idn-email', 'description': 'MUST be a valid email address'}}}, '$$description': [\"The people or organizations considered to be the 'authors' of the project.\", 'The exact meaning is open to interpretation (e.g. original or primary authors,', 'current maintainers, or owners of the package).']}, 'maintainers': {'type': 'array', 'items': {'$id': '#/definitions/author', 'title': 'Author or Maintainer', '$comment': 'https://peps.python.org/pep-0621/#authors-maintainers', 'type': 'object', 'additionalProperties': False, 'properties': {'name': {'type': 'string', '$$description': ['MUST be a valid email name, i.e. whatever can be put as a name, before an', 'email, in :rfc:`822`.']}, 'email': {'type': 'string', 'format': 'idn-email', 'description': 'MUST be a valid email address'}}}, '$$description': [\"The people or organizations considered to be the 'maintainers' of the project.\", 'Similarly to ``authors``, the exact meaning is open to interpretation.']}, 'keywords': {'type': 'array', 'items': {'type': 'string'}, 'description': 'List of keywords to assist searching for the distribution in a larger catalog.'}, 'classifiers': {'type': 'array', 'items': {'type': 'string', 'format': 'trove-classifier', 'description': '`PyPI classifier <https://pypi.org/classifiers/>`_.'}, '$$description': ['`Trove classifiers <https://pypi.org/classifiers/>`_', 'which apply to the project.']}, 'urls': {'type': 'object', 'description': 'URLs associated with the project in the form ``label => value``.', 'additionalProperties': False, 'patternProperties': {'^.+$': {'type': 'string', 'format': 'url'}}}, 'scripts': {'$id': '#/definitions/entry-point-group', 'title': 'Entry-points', 'type': 'object', '$$description': ['Entry-points are grouped together to indicate what sort of capabilities they', 'provide.', 'See the `packaging guides', '<https://packaging.python.org/specifications/entry-points/>`_', 'and `setuptools docs', '<https://setuptools.pypa.io/en/latest/userguide/entry_point.html>`_', 'for more information.'], 'propertyNames': {'format': 'python-entrypoint-name'}, 'additionalProperties': False, 'patternProperties': {'^.+$': {'type': 'string', '$$description': ['Reference to a Python object. It is either in the form', '``importable.module``, or ``importable.module:object.attr``.'], 'format': 'python-entrypoint-reference', '$comment': 'https://packaging.python.org/specifications/entry-points/'}}}, 'gui-scripts': {'$id': '#/definitions/entry-point-group', 'title': 'Entry-points', 'type': 'object', '$$description': ['Entry-points are grouped together to indicate what sort of capabilities they', 'provide.', 'See the `packaging guides', '<https://packaging.python.org/specifications/entry-points/>`_', 'and `setuptools docs', '<https://setuptools.pypa.io/en/latest/userguide/entry_point.html>`_', 'for more information.'], 'propertyNames': {'format': 'python-entrypoint-name'}, 'additionalProperties': False, 'patternProperties': {'^.+$': {'type': 'string', '$$description': ['Reference to a Python object. It is either in the form', '``importable.module``, or ``importable.module:object.attr``.'], 'format': 'python-entrypoint-reference', '$comment': 'https://packaging.python.org/specifications/entry-points/'}}}, 'entry-points': {'$$description': ['Instruct the installer to expose the given modules/functions via', '``entry-point`` discovery mechanism (useful for plugins).', 'More information available in the `Python packaging guide', '<https://packaging.python.org/specifications/entry-points/>`_.'], 'propertyNames': {'format': 'python-entrypoint-group'}, 'additionalProperties': False, 'patternProperties': {'^.+$': {'$id': '#/definitions/entry-point-group', 'title': 'Entry-points', 'type': 'object', '$$description': ['Entry-points are grouped together to indicate what sort of capabilities they', 'provide.', 'See the `packaging guides', '<https://packaging.python.org/specifications/entry-points/>`_', 'and `setuptools docs', '<https://setuptools.pypa.io/en/latest/userguide/entry_point.html>`_', 'for more information.'], 'propertyNames': {'format': 'python-entrypoint-name'}, 'additionalProperties': False, 'patternProperties': {'^.+$': {'type': 'string', '$$description': ['Reference to a Python object. It is either in the form', '``importable.module``, or ``importable.module:object.attr``.'], 'format': 'python-entrypoint-reference', '$comment': 'https://packaging.python.org/specifications/entry-points/'}}}}}, 'dependencies': {'type': 'array', 'description': 'Project (mandatory) dependencies.', 'items': {'$id': '#/definitions/dependency', 'title': 'Dependency', 'type': 'string', 'description': 'Project dependency specification according to PEP 508', 'format': 'pep508'}}, 'optional-dependencies': {'type': 'object', 'description': 'Optional dependency for the project', 'propertyNames': {'format': 'pep508-identifier'}, 'additionalProperties': False, 'patternProperties': {'^.+$': {'type': 'array', 'items': {'$id': '#/definitions/dependency', 'title': 'Dependency', 'type': 'string', 'description': 'Project dependency specification according to PEP 508', 'format': 'pep508'}}}}, 'dynamic': {'type': 'array', '$$description': ['Specifies which fields are intentionally unspecified and expected to be', 'dynamically provided by build tools'], 'items': {'enum': ['version', 'description', 'readme', 'requires-python', 'license', 'license-files', 'authors', 'maintainers', 'keywords', 'classifiers', 'urls', 'scripts', 'gui-scripts', 'entry-points', 'dependencies', 'optional-dependencies']}}}, 'required': ['name'], 'additionalProperties': False, 'allOf': [{'if': {'not': {'required': ['dynamic'], 'properties': {'dynamic': {'contains': {'const': 'version'}, '$$description': ['version is listed in ``dynamic``']}}}, '$$comment': ['According to :pep:`621`:', '    If the core metadata specification lists a field as \"Required\", then', '    the metadata MUST specify the field statically or list it in dynamic', 'In turn, `core metadata`_ defines:', '    The required fields are: Metadata-Version, Name, Version.', '    All the other fields are optional.', 'Since ``Metadata-Version`` is defined by the build back-end, ``name`` and', '``version`` are the only mandatory information in ``pyproject.toml``.', '.. _core metadata: https://packaging.python.org/specifications/core-metadata/']}, 'then': {'required': ['version'], '$$description': ['version should be statically defined in the ``version`` field']}}, {'if': {'required': ['license-files']}, 'then': {'properties': {'license': {'type': 'string'}}}}], 'definitions': {'author': {'$id': '#/definitions/author', 'title': 'Author or Maintainer', '$comment': 'https://peps.python.org/pep-0621/#authors-maintainers', 'type': 'object', 'additionalProperties': False, 'properties': {'name': {'type': 'string', '$$description': ['MUST be a valid email name, i.e. whatever can be put as a name, before an', 'email, in :rfc:`822`.']}, 'email': {'type': 'string', 'format': 'idn-email', 'description': 'MUST be a valid email address'}}}, 'entry-point-group': {'$id': '#/definitions/entry-point-group', 'title': 'Entry-points', 'type': 'object', '$$description': ['Entry-points are grouped together to indicate what sort of capabilities they', 'provide.', 'See the `packaging guides', '<https://packaging.python.org/specifications/entry-points/>`_', 'and `setuptools docs', '<https://setuptools.pypa.io/en/latest/userguide/entry_point.html>`_', 'for more information.'], 'propertyNames': {'format': 'python-entrypoint-name'}, 'additionalProperties': False, 'patternProperties': {'^.+$': {'type': 'string', '$$description': ['Reference to a Python object. It is either in the form', '``importable.module``, or ``importable.module:object.attr``.'], 'format': 'python-entrypoint-reference', '$comment': 'https://packaging.python.org/specifications/entry-points/'}}}, 'dependency': {'$id': '#/definitions/dependency', 'title': 'Dependency', 'type': 'string', 'description': 'Project dependency specification according to PEP 508', 'format': 'pep508'}}}, rule='type')\n./.venv/lib/python3.13/site-packages/setuptools/config/_validate_pyproject/fastjsonschema_validations.py:            raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \" must contain \" + (str(sorted(data__missing_keys)) + \" properties\"), value=data, name=\"\" + (name_prefix or \"data\") + \"\", definition={'$schema': 'http://json-schema.org/draft-07/schema#', '$id': 'https://packaging.python.org/en/latest/specifications/pyproject-toml/', 'title': 'Package metadata stored in the ``project`` table', '$$description': ['Data structure for the **project** table inside ``pyproject.toml``', '(as initially defined in :pep:`621`)'], 'type': 'object', 'properties': {'name': {'type': 'string', 'description': 'The name (primary identifier) of the project. MUST be statically defined.', 'format': 'pep508-identifier'}, 'version': {'type': 'string', 'description': 'The version of the project as supported by :pep:`440`.', 'format': 'pep440'}, 'description': {'type': 'string', '$$description': ['The `summary description of the project', '<https://packaging.python.org/specifications/core-metadata/#summary>`_']}, 'readme': {'$$description': ['`Full/detailed description of the project in the form of a README', '<https://peps.python.org/pep-0621/#readme>`_', \"with meaning similar to the one defined in `core metadata's Description\", '<https://packaging.python.org/specifications/core-metadata/#description>`_'], 'oneOf': [{'type': 'string', '$$description': ['Relative path to a text file (UTF-8) containing the full description', 'of the project. If the file path ends in case-insensitive ``.md`` or', '``.rst`` suffixes, then the content-type is respectively', '``text/markdown`` or ``text/x-rst``']}, {'type': 'object', 'allOf': [{'anyOf': [{'properties': {'file': {'type': 'string', '$$description': ['Relative path to a text file containing the full description', 'of the project.']}}, 'required': ['file']}, {'properties': {'text': {'type': 'string', 'description': 'Full text describing the project.'}}, 'required': ['text']}]}, {'properties': {'content-type': {'type': 'string', '$$description': ['Content-type (:rfc:`1341`) of the full description', '(e.g. ``text/markdown``). The ``charset`` parameter is assumed', 'UTF-8 when not present.'], '$comment': 'TODO: add regex pattern or format?'}}, 'required': ['content-type']}]}]}, 'requires-python': {'type': 'string', 'format': 'pep508-versionspec', '$$description': ['`The Python version requirements of the project', '<https://packaging.python.org/specifications/core-metadata/#requires-python>`_.']}, 'license': {'description': '`Project license <https://peps.python.org/pep-0621/#license>`_.', 'oneOf': [{'type': 'string', 'description': 'An SPDX license identifier', 'format': 'SPDX'}, {'type': 'object', 'properties': {'file': {'type': 'string', '$$description': ['Relative path to the file (UTF-8) which contains the license for the', 'project.']}}, 'required': ['file']}, {'type': 'object', 'properties': {'text': {'type': 'string', '$$description': ['The license of the project whose meaning is that of the', '`License field from the core metadata', '<https://packaging.python.org/specifications/core-metadata/#license>`_.']}}, 'required': ['text']}]}, 'license-files': {'description': 'Paths or globs to paths of license files', 'type': 'array', 'items': {'type': 'string'}}, 'authors': {'type': 'array', 'items': {'$id': '#/definitions/author', 'title': 'Author or Maintainer', '$comment': 'https://peps.python.org/pep-0621/#authors-maintainers', 'type': 'object', 'additionalProperties': False, 'properties': {'name': {'type': 'string', '$$description': ['MUST be a valid email name, i.e. whatever can be put as a name, before an', 'email, in :rfc:`822`.']}, 'email': {'type': 'string', 'format': 'idn-email', 'description': 'MUST be a valid email address'}}}, '$$description': [\"The people or organizations considered to be the 'authors' of the project.\", 'The exact meaning is open to interpretation (e.g. original or primary authors,', 'current maintainers, or owners of the package).']}, 'maintainers': {'type': 'array', 'items': {'$id': '#/definitions/author', 'title': 'Author or Maintainer', '$comment': 'https://peps.python.org/pep-0621/#authors-maintainers', 'type': 'object', 'additionalProperties': False, 'properties': {'name': {'type': 'string', '$$description': ['MUST be a valid email name, i.e. whatever can be put as a name, before an', 'email, in :rfc:`822`.']}, 'email': {'type': 'string', 'format': 'idn-email', 'description': 'MUST be a valid email address'}}}, '$$description': [\"The people or organizations considered to be the 'maintainers' of the project.\", 'Similarly to ``authors``, the exact meaning is open to interpretation.']}, 'keywords': {'type': 'array', 'items': {'type': 'string'}, 'description': 'List of keywords to assist searching for the distribution in a larger catalog.'}, 'classifiers': {'type': 'array', 'items': {'type': 'string', 'format': 'trove-classifier', 'description': '`PyPI classifier <https://pypi.org/classifiers/>`_.'}, '$$description': ['`Trove classifiers <https://pypi.org/classifiers/>`_', 'which apply to the project.']}, 'urls': {'type': 'object', 'description': 'URLs associated with the project in the form ``label => value``.', 'additionalProperties': False, 'patternProperties': {'^.+$': {'type': 'string', 'format': 'url'}}}, 'scripts': {'$id': '#/definitions/entry-point-group', 'title': 'Entry-points', 'type': 'object', '$$description': ['Entry-points are grouped together to indicate what sort of capabilities they', 'provide.', 'See the `packaging guides', '<https://packaging.python.org/specifications/entry-points/>`_', 'and `setuptools docs', '<https://setuptools.pypa.io/en/latest/userguide/entry_point.html>`_', 'for more information.'], 'propertyNames': {'format': 'python-entrypoint-name'}, 'additionalProperties': False, 'patternProperties': {'^.+$': {'type': 'string', '$$description': ['Reference to a Python object. It is either in the form', '``importable.module``, or ``importable.module:object.attr``.'], 'format': 'python-entrypoint-reference', '$comment': 'https://packaging.python.org/specifications/entry-points/'}}}, 'gui-scripts': {'$id': '#/definitions/entry-point-group', 'title': 'Entry-points', 'type': 'object', '$$description': ['Entry-points are grouped together to indicate what sort of capabilities they', 'provide.', 'See the `packaging guides', '<https://packaging.python.org/specifications/entry-points/>`_', 'and `setuptools docs', '<https://setuptools.pypa.io/en/latest/userguide/entry_point.html>`_', 'for more information.'], 'propertyNames': {'format': 'python-entrypoint-name'}, 'additionalProperties': False, 'patternProperties': {'^.+$': {'type': 'string', '$$description': ['Reference to a Python object. It is either in the form', '``importable.module``, or ``importable.module:object.attr``.'], 'format': 'python-entrypoint-reference', '$comment': 'https://packaging.python.org/specifications/entry-points/'}}}, 'entry-points': {'$$description': ['Instruct the installer to expose the given modules/functions via', '``entry-point`` discovery mechanism (useful for plugins).', 'More information available in the `Python packaging guide', '<https://packaging.python.org/specifications/entry-points/>`_.'], 'propertyNames': {'format': 'python-entrypoint-group'}, 'additionalProperties': False, 'patternProperties': {'^.+$': {'$id': '#/definitions/entry-point-group', 'title': 'Entry-points', 'type': 'object', '$$description': ['Entry-points are grouped together to indicate what sort of capabilities they', 'provide.', 'See the `packaging guides', '<https://packaging.python.org/specifications/entry-points/>`_', 'and `setuptools docs', '<https://setuptools.pypa.io/en/latest/userguide/entry_point.html>`_', 'for more information.'], 'propertyNames': {'format': 'python-entrypoint-name'}, 'additionalProperties': False, 'patternProperties': {'^.+$': {'type': 'string', '$$description': ['Reference to a Python object. It is either in the form', '``importable.module``, or ``importable.module:object.attr``.'], 'format': 'python-entrypoint-reference', '$comment': 'https://packaging.python.org/specifications/entry-points/'}}}}}, 'dependencies': {'type': 'array', 'description': 'Project (mandatory) dependencies.', 'items': {'$id': '#/definitions/dependency', 'title': 'Dependency', 'type': 'string', 'description': 'Project dependency specification according to PEP 508', 'format': 'pep508'}}, 'optional-dependencies': {'type': 'object', 'description': 'Optional dependency for the project', 'propertyNames': {'format': 'pep508-identifier'}, 'additionalProperties': False, 'patternProperties': {'^.+$': {'type': 'array', 'items': {'$id': '#/definitions/dependency', 'title': 'Dependency', 'type': 'string', 'description': 'Project dependency specification according to PEP 508', 'format': 'pep508'}}}}, 'dynamic': {'type': 'array', '$$description': ['Specifies which fields are intentionally unspecified and expected to be', 'dynamically provided by build tools'], 'items': {'enum': ['version', 'description', 'readme', 'requires-python', 'license', 'license-files', 'authors', 'maintainers', 'keywords', 'classifiers', 'urls', 'scripts', 'gui-scripts', 'entry-points', 'dependencies', 'optional-dependencies']}}}, 'required': ['name'], 'additionalProperties': False, 'allOf': [{'if': {'not': {'required': ['dynamic'], 'properties': {'dynamic': {'contains': {'const': 'version'}, '$$description': ['version is listed in ``dynamic``']}}}, '$$comment': ['According to :pep:`621`:', '    If the core metadata specification lists a field as \"Required\", then', '    the metadata MUST specify the field statically or list it in dynamic', 'In turn, `core metadata`_ defines:', '    The required fields are: Metadata-Version, Name, Version.', '    All the other fields are optional.', 'Since ``Metadata-Version`` is defined by the build back-end, ``name`` and', '``version`` are the only mandatory information in ``pyproject.toml``.', '.. _core metadata: https://packaging.python.org/specifications/core-metadata/']}, 'then': {'required': ['version'], '$$description': ['version should be statically defined in the ``version`` field']}}, {'if': {'required': ['license-files']}, 'then': {'properties': {'license': {'type': 'string'}}}}], 'definitions': {'author': {'$id': '#/definitions/author', 'title': 'Author or Maintainer', '$comment': 'https://peps.python.org/pep-0621/#authors-maintainers', 'type': 'object', 'additionalProperties': False, 'properties': {'name': {'type': 'string', '$$description': ['MUST be a valid email name, i.e. whatever can be put as a name, before an', 'email, in :rfc:`822`.']}, 'email': {'type': 'string', 'format': 'idn-email', 'description': 'MUST be a valid email address'}}}, 'entry-point-group': {'$id': '#/definitions/entry-point-group', 'title': 'Entry-points', 'type': 'object', '$$description': ['Entry-points are grouped together to indicate what sort of capabilities they', 'provide.', 'See the `packaging guides', '<https://packaging.python.org/specifications/entry-points/>`_', 'and `setuptools docs', '<https://setuptools.pypa.io/en/latest/userguide/entry_point.html>`_', 'for more information.'], 'propertyNames': {'format': 'python-entrypoint-name'}, 'additionalProperties': False, 'patternProperties': {'^.+$': {'type': 'string', '$$description': ['Reference to a Python object. It is either in the form', '``importable.module``, or ``importable.module:object.attr``.'], 'format': 'python-entrypoint-reference', '$comment': 'https://packaging.python.org/specifications/entry-points/'}}}, 'dependency': {'$id': '#/definitions/dependency', 'title': 'Dependency', 'type': 'string', 'description': 'Project dependency specification according to PEP 508', 'format': 'pep508'}}}, rule='required')\n./.venv/lib/python3.13/site-packages/setuptools/config/_validate_pyproject/fastjsonschema_validations.py:                        raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".readme must be object\", value=data__readme, name=\"\" + (name_prefix or \"data\") + \".readme\", definition={'type': 'object', 'allOf': [{'anyOf': [{'properties': {'file': {'type': 'string', '$$description': ['Relative path to a text file containing the full description', 'of the project.']}}, 'required': ['file']}, {'properties': {'text': {'type': 'string', 'description': 'Full text describing the project.'}}, 'required': ['text']}]}, {'properties': {'content-type': {'type': 'string', '$$description': ['Content-type (:rfc:`1341`) of the full description', '(e.g. ``text/markdown``). The ``charset`` parameter is assumed', 'UTF-8 when not present.'], '$comment': 'TODO: add regex pattern or format?'}}, 'required': ['content-type']}]}, rule='type')\n./.venv/lib/python3.13/site-packages/setuptools/config/_validate_pyproject/fastjsonschema_validations.py:                            raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".readme must contain \" + (str(sorted(data__readme__missing_keys)) + \" properties\"), value=data__readme, name=\"\" + (name_prefix or \"data\") + \".readme\", definition={'properties': {'content-type': {'type': 'string', '$$description': ['Content-type (:rfc:`1341`) of the full description', '(e.g. ``text/markdown``). The ``charset`` parameter is assumed', 'UTF-8 when not present.'], '$comment': 'TODO: add regex pattern or format?'}}, 'required': ['content-type']}, rule='required')\n./.venv/lib/python3.13/site-packages/setuptools/config/_validate_pyproject/fastjsonschema_validations.py:                                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".readme.content-type must be string\", value=data__readme__contenttype, name=\"\" + (name_prefix or \"data\") + \".readme.content-type\", definition={'type': 'string', '$$description': ['Content-type (:rfc:`1341`) of the full description', '(e.g. ``text/markdown``). The ``charset`` parameter is assumed', 'UTF-8 when not present.'], '$comment': 'TODO: add regex pattern or format?'}, rule='type')\n./.venv/lib/python3.13/site-packages/setuptools/config/_validate_pyproject/fastjsonschema_validations.py:                raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \".readme must be valid exactly by one definition\" + (\" (\" + str(data__readme_one_of_count12) + \" matches found)\"), value=data__readme, name=\"\" + (name_prefix or \"data\") + \".readme\", definition={'$$description': ['`Full/detailed description of the project in the form of a README', '<https://peps.python.org/pep-0621/#readme>`_', \"with meaning similar to the one defined in `core metadata's Description\", '<https://packaging.python.org/specifications/core-metadata/#description>`_'], 'oneOf': [{'type': 'string', '$$description': ['Relative path to a text file (UTF-8) containing the full description', 'of the project. If the file path ends in case-insensitive ``.md`` or', '``.rst`` suffixes, then the content-type is respectively', '``text/markdown`` or ``text/x-rst``']}, {'type': 'object', 'allOf': [{'anyOf': [{'properties': {'file': {'type': 'string', '$$description': ['Relative path to a text file containing the full description', 'of the project.']}}, 'required': ['file']}, {'properties': {'text': {'type': 'string', 'description': 'Full text describing the project.'}}, 'required': ['text']}]}, {'properties': {'content-type': {'type': 'string', '$$description': ['Content-type (:rfc:`1341`) of the full description', '(e.g. ``text/markdown``). The ``charset`` parameter is assumed', 'UTF-8 when not present.'], '$comment': 'TODO: add regex pattern or format?'}}, 'required': ['content-type']}]}]}, rule='oneOf')\n./.venv/lib/python3.13/site-packages/setuptools/config/_validate_pyproject/fastjsonschema_validations.py:            raise JsonSchemaValueException(\"\" + (name_prefix or \"data\") + \" must not contain \"+str(data_keys)+\" properties\", value=data, name=\"\" + (name_prefix or \"data\") + \"\", definition={'$schema': 'http://json-schema.org/draft-07/schema#', '$id': 'https://packaging.python.org/en/latest/specifications/pyproject-toml/', 'title': 'Package metadata stored in the ``project`` table', '$$description': ['Data structure for the **project** table inside ``pyproject.toml``', '(as initially defined in :pep:`621`)'], 'type': 'object', 'properties': {'name': {'type': 'string', 'description': 'The name (primary identifier) of the project. MUST be statically defined.', 'format': 'pep508-identifier'}, 'version': {'type': 'string', 'description': 'The version of the project as supported by :pep:`440`.', 'format': 'pep440'}, 'description': {'type': 'string', '$$description': ['The `summary description of the project', '<https://packaging.python.org/specifications/core-metadata/#summary>`_']}, 'readme': {'$$description': ['`Full/detailed description of the project in the form of a README', '<https://peps.python.org/pep-0621/#readme>`_', \"with meaning similar to the one defined in `core metadata's Description\", '<https://packaging.python.org/specifications/core-metadata/#description>`_'], 'oneOf': [{'type': 'string', '$$description': ['Relative path to a text file (UTF-8) containing the full description', 'of the project. If the file path ends in case-insensitive ``.md`` or', '``.rst`` suffixes, then the content-type is respectively', '``text/markdown`` or ``text/x-rst``']}, {'type': 'object', 'allOf': [{'anyOf': [{'properties': {'file': {'type': 'string', '$$description': ['Relative path to a text file containing the full description', 'of the project.']}}, 'required': ['file']}, {'properties': {'text': {'type': 'string', 'description': 'Full text describing the project.'}}, 'required': ['text']}]}, {'properties': {'content-type': {'type': 'string', '$$description': ['Content-type (:rfc:`1341`) of the full description', '(e.g. ``text/markdown``). The ``charset`` parameter is assumed', 'UTF-8 when not present.'], '$comment': 'TODO: add regex pattern or format?'}}, 'required': ['content-type']}]}]}, 'requires-python': {'type': 'string', 'format': 'pep508-versionspec', '$$description': ['`The Python version requirements of the project', '<https://packaging.python.org/specifications/core-metadata/#requires-python>`_.']}, 'license': {'description': '`Project license <https://peps.python.org/pep-0621/#license>`_.', 'oneOf': [{'type': 'string', 'description': 'An SPDX license identifier', 'format': 'SPDX'}, {'type': 'object', 'properties': {'file': {'type': 'string', '$$description': ['Relative path to the file (UTF-8) which contains the license for the', 'project.']}}, 'required': ['file']}, {'type': 'object', 'properties': {'text': {'type': 'string', '$$description': ['The license of the project whose meaning is that of the', '`License field from the core metadata', '<https://packaging.python.org/specifications/core-metadata/#license>`_.']}}, 'required': ['text']}]}, 'license-files': {'description': 'Paths or globs to paths of license files', 'type': 'array', 'items': {'type': 'string'}}, 'authors': {'type': 'array', 'items': {'$id': '#/definitions/author', 'title': 'Author or Maintainer', '$comment': 'https://peps.python.org/pep-0621/#authors-maintainers', 'type': 'object', 'additionalProperties': False, 'properties': {'name': {'type': 'string', '$$description': ['MUST be a valid email name, i.e. whatever can be put as a name, before an', 'email, in :rfc:`822`.']}, 'email': {'type': 'string', 'format': 'idn-email', 'description': 'MUST be a valid email address'}}}, '$$description': [\"The people or organizations considered to be the 'authors' of the project.\", 'The exact meaning is open to interpretation (e.g. original or primary authors,', 'current maintainers, or owners of the package).']}, 'maintainers': {'type': 'array', 'items': {'$id': '#/definitions/author', 'title': 'Author or Maintainer', '$comment': 'https://peps.python.org/pep-0621/#authors-maintainers', 'type': 'object', 'additionalProperties': False, 'properties': {'name': {'type': 'string', '$$description': ['MUST be a valid email name, i.e. whatever can be put as a name, before an', 'email, in :rfc:`822`.']}, 'email': {'type': 'string', 'format': 'idn-email', 'description': 'MUST be a valid email address'}}}, '$$description': [\"The people or organizations considered to be the 'maintainers' of the project.\", 'Similarly to ``authors``, the exact meaning is open to interpretation.']}, 'keywords': {'type': 'array', 'items': {'type': 'string'}, 'description': 'List of keywords to assist searching for the distribution in a larger catalog.'}, 'classifiers': {'type': 'array', 'items': {'type': 'string', 'format': 'trove-classifier', 'description': '`PyPI classifier <https://pypi.org/classifiers/>`_.'}, '$$description': ['`Trove classifiers <https://pypi.org/classifiers/>`_', 'which apply to the project.']}, 'urls': {'type': 'object', 'description': 'URLs associated with the project in the form ``label => value``.', 'additionalProperties': False, 'patternProperties': {'^.+$': {'type': 'string', 'format': 'url'}}}, 'scripts': {'$id': '#/definitions/entry-point-group', 'title': 'Entry-points', 'type': 'object', '$$description': ['Entry-points are grouped together to indicate what sort of capabilities they', 'provide.', 'See the `packaging guides', '<https://packaging.python.org/specifications/entry-points/>`_', 'and `setuptools docs', '<https://setuptools.pypa.io/en/latest/userguide/entry_point.html>`_', 'for more information.'], 'propertyNames': {'format': 'python-entrypoint-name'}, 'additionalProperties': False, 'patternProperties': {'^.+$': {'type': 'string', '$$description': ['Reference to a Python object. It is either in the form', '``importable.module``, or ``importable.module:object.attr``.'], 'format': 'python-entrypoint-reference', '$comment': 'https://packaging.python.org/specifications/entry-points/'}}}, 'gui-scripts': {'$id': '#/definitions/entry-point-group', 'title': 'Entry-points', 'type': 'object', '$$description': ['Entry-points are grouped together to indicate what sort of capabilities they', 'provide.', 'See the `packaging guides', '<https://packaging.python.org/specifications/entry-points/>`_', 'and `setuptools docs', '<https://setuptools.pypa.io/en/latest/userguide/entry_point.html>`_', 'for more information.'], 'propertyNames': {'format': 'python-entrypoint-name'}, 'additionalProperties': False, 'patternProperties': {'^.+$': {'type': 'string', '$$description': ['Reference to a Python object. It is either in the form', '``importable.module``, or ``importable.module:object.attr``.'], 'format': 'python-entrypoint-reference', '$comment': 'https://packaging.python.org/specifications/entry-points/'}}}, 'entry-points': {'$$description': ['Instruct the installer to expose the given modules/functions via', '``entry-point`` discovery mechanism (useful for plugins).', 'More information available in the `Python packaging guide', '<https://packaging.python.org/specifications/entry-points/>`_.'], 'propertyNames': {'format': 'python-entrypoint-group'}, 'additionalProperties': False, 'patternProperties': {'^.+$': {'$id': '#/definitions/entry-point-group', 'title': 'Entry-points', 'type': 'object', '$$description': ['Entry-points are grouped together to indicate what sort of capabilities they', 'provide.', 'See the `packaging guides', '<https://packaging.python.org/specifications/entry-points/>`_', 'and `setuptools docs', '<https://setuptools.pypa.io/en/latest/userguide/entry_point.html>`_', 'for more information.'], 'propertyNames': {'format': 'python-entrypoint-name'}, 'additionalProperties': False, 'patternProperties': {'^.+$': {'type': 'string', '$$description': ['Reference to a Python object. It is either in the form', '``importable.module``, or ``importable.module:object.attr``.'], 'format': 'python-entrypoint-reference', '$comment': 'https://packaging.python.org/specifications/entry-points/'}}}}}, 'dependencies': {'type': 'array', 'description': 'Project (mandatory) dependencies.', 'items': {'$id': '#/definitions/dependency', 'title': 'Dependency', 'type': 'string', 'description': 'Project dependency specification according to PEP 508', 'format': 'pep508'}}, 'optional-dependencies': {'type': 'object', 'description': 'Optional dependency for the project', 'propertyNames': {'format': 'pep508-identifier'}, 'additionalProperties': False, 'patternProperties': {'^.+$': {'type': 'array', 'items': {'$id': '#/definitions/dependency', 'title': 'Dependency', 'type': 'string', 'description': 'Project dependency specification according to PEP 508', 'format': 'pep508'}}}}, 'dynamic': {'type': 'array', '$$description': ['Specifies which fields are intentionally unspecified and expected to be', 'dynamically provided by build tools'], 'items': {'enum': ['version', 'description', 'readme', 'requires-python', 'license', 'license-files', 'authors', 'maintainers', 'keywords', 'classifiers', 'urls', 'scripts', 'gui-scripts', 'entry-points', 'dependencies', 'optional-dependencies']}}}, 'required': ['name'], 'additionalProperties': False, 'allOf': [{'if': {'not': {'required': ['dynamic'], 'properties': {'dynamic': {'contains': {'const': 'version'}, '$$description': ['version is listed in ``dynamic``']}}}, '$$comment': ['According to :pep:`621`:', '    If the core metadata specification lists a field as \"Required\", then', '    the metadata MUST specify the field statically or list it in dynamic', 'In turn, `core metadata`_ defines:', '    The required fields are: Metadata-Version, Name, Version.', '    All the other fields are optional.', 'Since ``Metadata-Version`` is defined by the build back-end, ``name`` and', '``version`` are the only mandatory information in ``pyproject.toml``.', '.. _core metadata: https://packaging.python.org/specifications/core-metadata/']}, 'then': {'required': ['version'], '$$description': ['version should be statically defined in the ``version`` field']}}, {'if': {'required': ['license-files']}, 'then': {'properties': {'license': {'type': 'string'}}}}], 'definitions': {'author': {'$id': '#/definitions/author', 'title': 'Author or Maintainer', '$comment': 'https://peps.python.org/pep-0621/#authors-maintainers', 'type': 'object', 'additionalProperties': False, 'properties': {'name': {'type': 'string', '$$description': ['MUST be a valid email name, i.e. whatever can be put as a name, before an', 'email, in :rfc:`822`.']}, 'email': {'type': 'string', 'format': 'idn-email', 'description': 'MUST be a valid email address'}}}, 'entry-point-group': {'$id': '#/definitions/entry-point-group', 'title': 'Entry-points', 'type': 'object', '$$description': ['Entry-points are grouped together to indicate what sort of capabilities they', 'provide.', 'See the `packaging guides', '<https://packaging.python.org/specifications/entry-points/>`_', 'and `setuptools docs', '<https://setuptools.pypa.io/en/latest/userguide/entry_point.html>`_', 'for more information.'], 'propertyNames': {'format': 'python-entrypoint-name'}, 'additionalProperties': False, 'patternProperties': {'^.+$': {'type': 'string', '$$description': ['Reference to a Python object. It is either in the form', '``importable.module``, or ``importable.module:object.attr``.'], 'format': 'python-entrypoint-reference', '$comment': 'https://packaging.python.org/specifications/entry-points/'}}}, 'dependency': {'$id': '#/definitions/dependency', 'title': 'Dependency', 'type': 'string', 'description': 'Project dependency specification according to PEP 508', 'format': 'pep508'}}}, rule='additionalProperties')\n./.venv/lib/python3.13/site-packages/setuptools/config/_validate_pyproject/extra_validations.py:    # TODO: check for `include-group` cycles (can be conditional to graphlib)\n./.venv/lib/python3.13/site-packages/setuptools/config/distutils.schema.json:  \"$comment\": \"TODO: Is there a practical way of making this schema more specific?\"\n./.venv/lib/python3.13/site-packages/setuptools/config/setuptools.schema.json:      \"$comment\": \"TODO: is this field deprecated/should be removed?\"\n./.venv/lib/python3.13/site-packages/setuptools/config/setuptools.schema.json:      \"$comment\": \"TODO: clarify the relationship with ``packages``\"\n./.venv/lib/python3.13/site-packages/setuptools/config/setuptools.schema.json:      \"$comment\": \"TODO: revise if PEP 639 is accepted. Probably ``project.license-files``?\"\n./.venv/lib/python3.13/site-packages/setuptools/command/egg_info.py:    TODO: Remove this function in a version sufficiently > 68.\n./.venv/lib/python3.13/site-packages/setuptools/command/install_lib.py:        # TODO: is it necessary to short-circuit here? i.e. what's the cost\n./.venv/lib/python3.13/site-packages/setuptools/command/dist_info.py:        # TODO: if bdist_wheel if merged into setuptools, just add \"keep_egg_info\" there\n./.venv/lib/python3.13/site-packages/setuptools/command/bdist_wheel.py:            # TODO armv8l, packaging pull request #690 => this did not land\n./.venv/lib/python3.13/site-packages/setuptools/command/bdist_wheel.py:                    # TODO armv8l, packaging pull request #690 => this did not land\n./.venv/lib/python3.13/site-packages/setuptools/command/editable_wheel.py:    COMPAT = \"compat\"  # TODO: Remove `compat` after Dec/2022.\n./.venv/lib/python3.13/site-packages/setuptools/command/editable_wheel.py:                # TODO: define due_date\n./.venv/lib/python3.13/site-packages/setuptools/command/editable_wheel.py:        # TODO: Once plugins/customizations had the chance to catch up, replace\n./.venv/lib/python3.13/site-packages/setuptools/command/editable_wheel.py:                # TODO: define due_date\n./.venv/lib/python3.13/site-packages/setuptools/command/editable_wheel.py:        # TODO: Python 3.13 replace the whole function with `bytes(content, \"utf-8\")`\n./.venv/lib/python3.13/site-packages/setuptools/_static.py:    _mutated_: bool = False  # TODO: Remove after deprecation warning is solved\n./.venv/lib/python3.13/site-packages/setuptools/_static.py:        # TODO: After deprecation period raise NotImplementedError instead of warning\n./.venv/lib/python3.13/site-packages/setuptools/dist.py:            # TODO: define due_date, it may break old packages that are no longer\n./.venv/lib/python3.13/site-packages/setuptools/dist.py:        # TODO: should there be a `due_date` here?\n./.venv/lib/python3.13/site-packages/setuptools/dist.py:                # TODO: Should we add a due date? It may affect old/unmaintained\n./.venv/lib/python3.13/site-packages/setuptools/_distutils/tests/test_build_ext.py:    # TODO: can the file be scheduled for deletion?\n./.venv/lib/python3.13/site-packages/setuptools/tests/fixtures.py:    # TODO: Use `--no-wheel` when setuptools implements its own bdist_wheel\n./.venv/lib/python3.13/site-packages/setuptools/tests/test_core_metadata.py:    # TODO: Handle lack of PEP 643 implementation in pypa/wheel?\n./.venv/lib/python3.13/site-packages/setuptools/tests/test_build_py.py:        # TODO: To fix #3260 we need some transition period to deprecate the\n./.venv/lib/python3.13/site-packages/setuptools/tests/test_build_py.py:            # TODO: Enforce the following assertion once #3260 is fixed\n./.venv/lib/python3.13/site-packages/setuptools/tests/test_setuptools.py:        # TODO: Evaluate if this code is needed at all.\n./.venv/lib/python3.13/site-packages/setuptools/tests/test_bdist_wheel.py:    # TODO: Remove this test after deprecation period is over\n./.venv/lib/python3.13/site-packages/setuptools/tests/test_editable_install.py:    # TODO: Remove `compat` after Dec/2022.\n./.venv/lib/python3.13/site-packages/setuptools/tests/test_editable_install.py:    # TODO: Remove tests after _run_build_steps is removed.\n./.venv/lib/python3.13/site-packages/setuptools/unicode_utils.py:    # TODO: Add a deadline?\n./.venv/lib/python3.13/site-packages/setuptools/_core_metadata.py:        # TODO: Replace with `raise ValueError(\"newlines not allowed\")`\n./.venv/lib/python3.13/site-packages/setuptools/_normalization.py:    TODO: replace this with filename_component after pip 24 is\n./.venv/lib/python3.13/site-packages/setuptools/_normalization.py:    # TODO: Replace with only safe_version in the future (no need for best effort)\n./.venv/lib/python3.13/site-packages/setuptools/_vendor/typing_extensions.py:                # TODO: Use inspect.VALUE here, and make the annotations lazily evaluated\n./.venv/lib/python3.13/site-packages/setuptools/_vendor/typing_extensions.py:                # TODO: Use inspect.VALUE here, and make the annotations lazily evaluated\n./.venv/lib/python3.13/site-packages/setuptools/_vendor/packaging/metadata.py:        # TODO: The spec doesn't say anything about if the keys should be\n./.venv/lib/python3.13/site-packages/setuptools/_vendor/packaging/metadata.py:    description: _Validator[str | None] = _Validator()  # TODO 2.1: can be in body\n./.venv/lib/python3.13/site-packages/setuptools/_vendor/packaging/tags.py:        # TODO: Need to care about 32-bit PPC for ppc64 through 10.2?\n./.venv/lib/python3.13/site-packages/setuptools/_vendor/packaging/requirements.py:    # TODO: Can we test whether something is contained within a requirement?\n./.venv/lib/python3.13/site-packages/setuptools/_vendor/packaging/requirements.py:    # TODO: Can we normalize the name and extra name?\n./.venv/lib/python3.13/site-packages/setuptools/_vendor/typeguard/_checkers.py:            # TODO: raise exception on added keyword-only arguments without defaults\n./.venv/lib/python3.13/site-packages/setuptools/_vendor/typeguard/_checkers.py:            # TODO: implement assignability checks for non-callable members\n./.venv/lib/python3.13/site-packages/setuptools/_vendor/inflect/__init__.py:    ),  # TODO: isn't ue$ -> u encompassed in the following rule?\n./.venv/lib/python3.13/site-packages/setuptools/_vendor/inflect/__init__.py:            )  # TODO: what if 2 spaces between these words?\n./.venv/lib/python3.13/site-packages/setuptools/_vendor/inflect/__init__.py:            )  # TODO: what if 2 spaces between these words?\n./.venv/lib/python3.13/site-packages/setuptools/_vendor/autocommand/autoparse.py:        # TODO: special case for list type.\n./.venv/lib/python3.13/site-packages/setuptools/_vendor/autocommand/autoparse.py:        # TODO: consider depluralizing metavar/name here.\n./.venv/lib/python3.13/site-packages/setuptools/_vendor/autocommand/autoparse.py:    # TODO: attach an updated __signature__ to autoparse_wrapper, just in case.\n./.venv/lib/python3.13/site-packages/setuptools/_vendor/inflect-7.3.1.dist-info/METADATA:    # TODO: I get no comma before plus: check perl\n./.venv/lib/python3.13/site-packages/setuptools/_vendor/inflect-7.3.1.dist-info/METADATA:    # TODO: DOCBUG: perl gives commas here as do I\n./.venv/lib/python3.13/site-packages/setuptools/_vendor/wheel/_bdist_wheel.py:            # TODO armv8l, packaging pull request #690 => this did not land\n./.venv/lib/python3.13/site-packages/setuptools/_vendor/wheel/_bdist_wheel.py:                    # TODO armv8l, packaging pull request #690 => this did not land\n./.venv/lib/python3.13/site-packages/setuptools/_vendor/wheel/vendored/packaging/tags.py:        # TODO: Need to care about 32-bit PPC for ppc64 through 10.2?\n./.venv/lib/python3.13/site-packages/setuptools/_vendor/wheel/vendored/packaging/requirements.py:    # TODO: Can we test whether something is contained within a requirement?\n./.venv/lib/python3.13/site-packages/setuptools/_vendor/wheel/vendored/packaging/requirements.py:    # TODO: Can we normalize the name and extra name?\n./.venv/lib/python3.13/site-packages/jsonschema/_format.py:    # TODO: I don't want to maintain this, so it\n./.venv/lib/python3.13/site-packages/jsonschema/validators.py:        # TODO: include new meta-schemas added at runtime\n./.venv/lib/python3.13/site-packages/jsonschema/tests/test_validators.py:    # TODO: These really need unit tests for each individual keyword, rather\n./.venv/lib/python3.13/site-packages/jsonschema/tests/test_validators.py:    # TODO: These all belong upstream\n./.venv/lib/python3.13/site-packages/pkg_resources/__init__.py:_ResourceStream: TypeAlias = Any  # TODO / Incomplete: A readable file-like object\n./.venv/lib/python3.13/site-packages/pkg_resources/__init__.py:            # TODO: remove this except clause when python/cpython#103632 is fixed.\n./.venv/lib/python3.13/site-packages/pkg_resources/__init__.py:        # TODO: Add a deadline?\n./.venv/lib/python3.13/site-packages/pyasn1_modules/rfc2985.py:# TODO:\n./.venv/lib/python3.13/site-packages/pyasn1_modules/rfc2985.py:# TODO: Once PKCS15Token can be imported, this can be included\n./.venv/lib/python3.13/site-packages/pyasn1_modules/rfc2985.py:    # TODO: Once PKCS15Token can be imported, this can be included\n./.venv/lib/python3.13/site-packages/pyasn1_modules/rfc2459.py:# Updated by Russ Housley to resolve the TODO regarding the Certificate\n./.venv/lib/python3.13/site-packages/flake8/options/config.py:    # TODO: remove this and replace it with configuration modifying plugins\n./.venv/lib/python3.13/site-packages/flake8/options/manager.py:    # TODO: maybe make this a free function to reduce api surface area\n./.venv/lib/python3.13/site-packages/flake8/violation.py:        # TODO(sigmavirus24): Determine how to handle stdin with linecache\n./.venv/lib/python3.13/site-packages/flake8/main/options.py:    # TODO(sigmavirus24): Figure out --first/--repeat\n./.venv/lib/python3.13/site-packages/flake8/main/options.py:    # TODO(sigmavirus24): Decide what to do about --show-pep8\n./.venv/lib/python3.13/site-packages/oauthlib/oauth1/rfc5849/__init__.py:        # TODO what if there are body params on a header-type auth?\n./.venv/lib/python3.13/site-packages/oauthlib/oauth1/rfc5849/__init__.py:        # TODO what if there are query params on a body-type auth?\n./.venv/lib/python3.13/site-packages/oauthlib/oauth1/rfc5849/__init__.py:        # TODO: right now these prepare_* methods are very narrow in scope--they\n./.venv/lib/python3.13/site-packages/oauthlib/oauth1/rfc5849/parameters.py:# TODO: do we need filter_params now that oauth_params are handled by Request?\n./.venv/lib/python3.13/site-packages/oauthlib/oauth1/rfc5849/signature.py:    # TODO: enforce header param inclusion conditions\n./.venv/lib/python3.13/site-packages/oauthlib/oauth1/rfc5849/endpoints/base.py:        # TODO: move into oauthlib.common from oauth2.utils\n./.venv/lib/python3.13/site-packages/oauthlib/oauth2/rfc8628/endpoints/device_authorization.py:        # TODO: extract client_id an helper validation function.\n./.venv/lib/python3.13/site-packages/oauthlib/oauth2/rfc6749/utils.py:    TODO: verify whether this can in fact be used for OAuth 2\n./.venv/lib/python3.13/site-packages/oauthlib/oauth2/rfc6749/endpoints/authorization.py:        # TODO: decide whether this should be a required argument\n./.venv/lib/python3.13/site-packages/oauthlib/oauth2/rfc6749/endpoints/authorization.py:        request.user = None     # TODO: explain this in docs\n./.venv/lib/python3.13/site-packages/charset_normalizer/legacy.py:# TODO: remove this check when dropping Python 3.7 support\n./.venv/lib/python3.13/site-packages/requests/adapters.py:                # TODO: Remove this in 3.0.0: see #2811\n./.venv/lib/python3.13/site-packages/requests/hooks.py:# TODO: response is the only one\n./.venv/lib/python3.13/site-packages/flatbuffers/encode.py:        # TODO: could set .flags.writeable = False to make users jump through\n./.venv/lib/python3.13/site-packages/flatbuffers/table.py:        length = self.VectorLen(off) # TODO: length accounts for bytewidth, right?\n./.venv/lib/python3.13/site-packages/flatbuffers/flexbuffers.py:# TODO(dkovalev): Add type hints everywhere, so tools like pytypes could work.\n./.venv/lib/python3.13/site-packages/rich/text.py:        # TODO: This is a little inefficient, it is only used by full justify\n./.venv/lib/python3.13/site-packages/faiss/array_conversions.py:# TODO: once deprecated classes are removed, remove the dict and just use .lower() below\n./.venv/lib/python3.13/site-packages/faiss/array_conversions.py:    # TODO check class name\n./.venv/lib/python3.13/site-packages/faiss/swigfaiss.i:// TODO: Remove the need for this hack.\n./.venv/lib/python3.13/site-packages/faiss/contrib/big_batch_search.py:            # TODO implement blockwise to avoid mem blowup\n./.venv/lib/python3.13/site-packages/faiss/contrib/exhaustive_search.py:    # TODO: all result manipulations are in python, should move to C++ if perf\n./.venv/lib/python3.13/site-packages/nvidia/cudnn/include/cudnn_ops_v9.h:/* TODO: move these enums out to the appropriate submodule */\n./.venv/lib/python3.13/site-packages/nvidia/cudnn/include/cudnn_ops.h:/* TODO: move these enums out to the appropriate submodule */\n./.venv/lib/python3.13/site-packages/nvidia/nvtx/include/nvtx3/nvtxDetail/nvtxInit.h:/* TODO */\n./.venv/lib/python3.13/site-packages/nvidia/nvtx/include/nvtx3/nvtxDetail/nvtxInit.h:/* TODO: Detect UWP, a.k.a. Windows Store app, and set this to 0. */\n./.venv/lib/python3.13/site-packages/nvidia/cuda_runtime/include/cuda_runtime.h: * TODO detail\n./.venv/lib/python3.13/site-packages/nvidia/cuda_runtime/include/cooperative_groups.h:// TODO: Use a static dispatch to determine appropriate return type\n./.venv/lib/python3.13/site-packages/pydantic/alias_generators.py:# TODO: in V3, change the argument names to be more descriptive\n./.venv/lib/python3.13/site-packages/pydantic/mypy.py:                    # TODO: Only do this if the first argument of the decorated function is `cls`\n./.venv/lib/python3.13/site-packages/pydantic/mypy.py:                # TODO: We shouldn't be performing type operations during the main\n./.venv/lib/python3.13/site-packages/pydantic/mypy.py:            # TODO this path should be removed (see https://github.com/pydantic/pydantic/issues/11119)\n./.venv/lib/python3.13/site-packages/pydantic/json_schema.py:        TODO: the nested function definitions here seem like bad practice, I'd like to unpack these\n./.venv/lib/python3.13/site-packages/pydantic/json_schema.py:        # TODO: I dislike that we have to wrap these basic dict updates in callables, is there any way around this?\n./.venv/lib/python3.13/site-packages/pydantic/json_schema.py:            # TODO: should we add regex flags to the pattern?\n./.venv/lib/python3.13/site-packages/pydantic/json_schema.py:        # TODO: improvements along with https://github.com/pydantic/pydantic/issues/8208\n./.venv/lib/python3.13/site-packages/pydantic/json_schema.py:        # This reflects the v1 behavior; TODO: we should make it possible to exclude OpenAPI stuff from the JSON schema\n./.venv/lib/python3.13/site-packages/pydantic/json_schema.py:                        # TODO: fixme - this is a workaround for the fact that we can't always resolve refs\n./.venv/lib/python3.13/site-packages/pydantic/json_schema.py:        # TODO: Need to read the default value off of model config or whatever\n./.venv/lib/python3.13/site-packages/pydantic/json_schema.py:        use_strict = schema.get('strict', False)  # TODO: replace this default False\n./.venv/lib/python3.13/site-packages/pydantic/deprecated/json.py:# TODO: Add a suggested migration path once there is a way to use custom encoders\n./.venv/lib/python3.13/site-packages/pydantic/type_adapter.py:            # TODO: we don't go through the rebuild logic here directly because we don't want\n./.venv/lib/python3.13/site-packages/pydantic/experimental/pipeline.py:# TODO: ultimately, make this public, see https://github.com/pydantic/pydantic/pull/9459#discussion_r1628197626\n./.venv/lib/python3.13/site-packages/pydantic/experimental/pipeline.py:            # TODO: is there a better way? should we just not do this?\n./.venv/lib/python3.13/site-packages/pydantic/main.py:# TODO v3 fallback to `dict` when the deprecated `dict` method gets removed.\n./.venv/lib/python3.13/site-packages/pydantic/main.py:                    # TODO - matching error\n./.venv/lib/python3.13/site-packages/pydantic/main.py:    # TODO PEP 747: replace `Any` by the TypeForm:\n./.venv/lib/python3.13/site-packages/pydantic/fields.py:    # TODO PEP 747: use TypeForm:\n./.venv/lib/python3.13/site-packages/pydantic/fields.py:        # TODO check for classvar and error?\n./.venv/lib/python3.13/site-packages/pydantic/fields.py:        # TODO check for classvar and error?\n./.venv/lib/python3.13/site-packages/pydantic/fields.py:        # TODO infer from the default, this can be done in v3 once we treat final fields with\n./.venv/lib/python3.13/site-packages/pydantic/fields.py:            # TODO: properly make use of the protocol (https://rich.readthedocs.io/en/stable/pretty.html#rich-repr-protocol)\n./.venv/lib/python3.13/site-packages/pydantic/fields.py:    default: ellipsis,  # noqa: F821  # TODO: use `_typing_extra.EllipsisType` when we drop Py3.9\n./.venv/lib/python3.13/site-packages/pydantic/_internal/_model_construction.py:                # TODO we can also stop there if `__pydantic_fields_complete__` is False.\n./.venv/lib/python3.13/site-packages/pydantic/_internal/_validators.py:    # TODO: refactor sequence validation to validate with either a list or a tuple\n./.venv/lib/python3.13/site-packages/pydantic/_internal/_known_annotated_metadata.py:    # TODO: this is a bit redundant, we could probably avoid some of these\n./.venv/lib/python3.13/site-packages/pydantic/_internal/_namespace_utils.py:            # TODO: should we merge the parent namespace here?\n./.venv/lib/python3.13/site-packages/pydantic/_internal/_namespace_utils.py:        # TODO `typ.__type_params__` when we drop support for Python 3.11:\n./.venv/lib/python3.13/site-packages/pydantic/_internal/_core_metadata.py:    TODO: Perhaps we should move this structure to pydantic-core. At the moment, though,\n./.venv/lib/python3.13/site-packages/pydantic/_internal/_core_metadata.py:    TODO: It's unfortunate how functionally oriented JSON schema generation is, especially that which occurs during\n./.venv/lib/python3.13/site-packages/pydantic/_internal/_schema_gather.py:    # TODO When we drop 3.9, use a match statement to get better type checking and remove\n./.venv/lib/python3.13/site-packages/pydantic/_internal/_schema_gather.py:        # TODO duplicate schema types for serializers and validators, needs to be deduplicated.\n./.venv/lib/python3.13/site-packages/pydantic/_internal/_schema_gather.py:        # TODO duplicate schema types for serializers and validators, needs to be deduplicated.\n./.venv/lib/python3.13/site-packages/pydantic/_internal/_fields.py:                    # TODO: We should probably do something with this so that validate_assignment behaves properly\n./.venv/lib/python3.13/site-packages/pydantic/_internal/_fields.py:                        # TODO: same note as above re validate_assignment\n./.venv/lib/python3.13/site-packages/pydantic/_internal/_fields.py:            # was already evaluated. TODO: is this method relevant?\n./.venv/lib/python3.13/site-packages/pydantic/_internal/_generate_schema.py:        # TODO: in theory we should check that the schema accepts a serialization key\n./.venv/lib/python3.13/site-packages/pydantic/_internal/_generate_schema.py:            # TODO this is an ugly hack, how do we trigger an Any schema for serialization?\n./.venv/lib/python3.13/site-packages/pydantic/_internal/_generate_schema.py:        # TODO: note, this is a fairly common pattern, re lax / strict for attempted type coercion,\n./.venv/lib/python3.13/site-packages/pydantic/_internal/_generate_schema.py:        # TODO: do we really need to resolve type vars here?\n./.venv/lib/python3.13/site-packages/pydantic/_internal/_generate_schema.py:                # TODO: something like https://github.com/pydantic/pydantic/issues/5952\n./.venv/lib/python3.13/site-packages/pydantic/_internal/_generate_schema.py:        TODO support functional validators once we support them in Config\n./.venv/lib/python3.13/site-packages/pydantic/_internal/_generate_schema.py:# TODO V3: this function is only used for deprecated decorators. It should\n./.venv/lib/python3.13/site-packages/pydantic/_internal/_generics.py:    # TODO: This could be unified with `get_standard_typevars_map` if we stored the generic metadata\n./.venv/lib/python3.13/site-packages/pydantic/_internal/_generics.py:        # TODO remove parentheses when we drop support for Python 3.10:\n./.venv/lib/python3.13/site-packages/pydantic/_internal/_typing_extra.py:# TODO implement `is_finalvar_annotation` as Final can be wrapped with other special forms:\n./.venv/lib/python3.13/site-packages/pydantic/_internal/_typing_extra.py:# TODO In 2.12, delete this export. It is currently defined only to not break\n./.venv/lib/python3.13/site-packages/pydantic/_internal/_typing_extra.py:# TODO: Ideally, we should avoid relying on the private `typing` constructs:\n./.venv/lib/python3.13/site-packages/pydantic/_internal/_typing_extra.py:        # TODO ideally recursion errors should be checked in `eval_type` above, but `eval_type_backport`\n./.venv/lib/python3.13/site-packages/pydantic/functional_validators.py:            # TODO if `schema['serialization']` is one of `'include-exclude-dict/sequence',\n./.venv/lib/python3.13/site-packages/pydantic/dataclasses.py:        # TODO `parent_namespace` is currently None, but we could do the same thing as Pydantic models:\n./.venv/lib/python3.13/site-packages/pydantic/v1/utils.py:            # TODO: replace annotation with actual expected types once #1055 solved\n./.venv/lib/python3.13/site-packages/pydantic/v1/networks.py:    # TODO: Needed to generic \"Parts\" for \"Replica Set\", \"Sharded Cluster\", and other mongodb deployment modes\n./.venv/lib/python3.13/site-packages/attr/__init__.pyi:# TODO: add support for returning a proper attrs class from the mypy plugin\n./.venv/lib/python3.13/site-packages/attr/__init__.pyi:# TODO: add support for returning TypedDict from the mypy plugin\n./.venv/lib/python3.13/site-packages/attr/__init__.pyi:# TODO: add support for returning NamedTuple from the mypy plugin\n./.venv/lib/python3.13/site-packages/dateutil/zoneinfo/__init__.py:    except IOError as e:  # TODO  switch to FileNotFoundError?\n./.venv/lib/python3.13/site-packages/dateutil/zoneinfo/__init__.py:# TODO: Remove after deprecation period.\n./.venv/lib/python3.13/site-packages/dateutil/parser/_parser.py:# TODO: pandas.core.tools.datetimes imports this explicitly.  Might be worth\n./.venv/lib/python3.13/site-packages/dateutil/parser/_parser.py:                (\"Tue\", \"Tuesday\"),     # TODO: \"Tues\"\n./.venv/lib/python3.13/site-packages/dateutil/parser/_parser.py:                (\"Thu\", \"Thursday\"),    # TODO: \"Thurs\"\n./.venv/lib/python3.13/site-packages/dateutil/parser/_parser.py:              (\"Feb\", \"February\"),      # TODO: \"Febr\"\n./.venv/lib/python3.13/site-packages/dateutil/parser/_parser.py:    # TODO: ERA = [\"AD\", \"BC\", \"CE\", \"BCE\", \"Stardate\",\n./.venv/lib/python3.13/site-packages/dateutil/parser/_parser.py:                                # TODO: not hit in tests\n./.venv/lib/python3.13/site-packages/dateutil/parser/_parser.py:                    # TODO: check that l[i + 1] is integer?\n./.venv/lib/python3.13/site-packages/dateutil/parser/_parser.py:                        min_offset = int(l[i + 3])  # TODO: Check that l[i+3] is minute-like?\n./.venv/lib/python3.13/site-packages/dateutil/parser/_parser.py:                # TODO: Check if res attributes already set.\n./.venv/lib/python3.13/site-packages/dateutil/parser/_parser.py:                # TODO: checking that hour/minute/second are not\n./.venv/lib/python3.13/site-packages/dateutil/parser/_parser.py:            value = self._to_decimal(tokens[idx + 2])  # TODO: try/except for this?\n./.venv/lib/python3.13/site-packages/dateutil/parser/_parser.py:            # TODO: Are we sure this is the right condition here?\n./.venv/lib/python3.13/site-packages/dateutil/parser/_parser.py:        # TODO: Every usage of this function sets res.second to the return\n./.venv/lib/python3.13/site-packages/dateutil/parser/_parser.py:        # TODO: Is this going to admit a lot of false-positives for when we\n./.venv/lib/python3.13/site-packages/dateutil/rrule.py:                    # TODO: Check -numweeks for next year.\n./.venv/lib/python3.13/site-packages/urllib3/util/request.py:    # File-like object, TODO: use seek() and tell() for length?\n./.venv/lib/python3.13/site-packages/urllib3/util/url.py:    # TODO: Remove this when we break backwards compatibility.\n./.venv/lib/python3.13/site-packages/urllib3/exceptions.py:    # TODO(t-8ch): Stop inheriting from AssertionError in v2.0.\n./.venv/lib/python3.13/site-packages/urllib3/connectionpool.py:        # TODO: Add optional support for socket.gethostbyname checking.\n./.venv/lib/python3.13/site-packages/urllib3/connectionpool.py:        # TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\n./.venv/lib/python3.13/site-packages/urllib3/response.py:                # TODO make sure to initially read enough data to get past the headers\n./.venv/lib/python3.13/site-packages/urllib3/connection.py:            # TODO: Fix tunnel so it doesn't depend on self.sock state.\n./.venv/lib/python3.13/site-packages/urllib3/connection.py:        # object later. TODO: Remove this in favor of a real\n./.venv/lib/python3.13/site-packages/urllib3/connection.py:        # TODO should we implement it everywhere?\n./.venv/lib/python3.13/site-packages/urllib3/_base_connection.py:    # TODO: Remove this in favor of a better\n./.venv/lib/python3.13/site-packages/urllib3/http2/__init__.py:    # TODO: Offer 'http/1.1' as well, but for testing purposes this is handy.\n./.venv/lib/python3.13/site-packages/urllib3/http2/connection.py:        # TODO SKIPPABLE_HEADERS from urllib3 are ignored.\n./.venv/lib/python3.13/site-packages/urllib3/http2/connection.py:                # TODO: Arbitrary read value.\n./.venv/lib/python3.13/site-packages/urllib3/http2/connection.py:            # TODO this is often present from upstream.\n./.venv/lib/python3.13/site-packages/urllib3/http2/connection.py:    # TODO: This is a woefully incomplete response object, but works for non-streaming.\n./.venv/lib/python3.13/site-packages/urllib3/http2/connection.py:        decode_content: bool = False,  # TODO: support decoding\n./.venv/lib/python3.13/site-packages/mmh3/mmh3module.c:        if (self->shift == 0) {  // TODO: use bit ops\n./.venv/lib/python3.13/site-packages/mmh3/mmh3module.c:        if (self->shift == 0) {  // TODO: use bit ops\n./.venv/lib/python3.13/site-packages/mmh3/mmh3module.c:        if (self->shift < 64) {  // TODO: use bit ops\n./.venv/lib/python3.13/site-packages/mmh3/mmh3module.c:        if (self->shift < 32) {  // TODO: use bit ops\n./.venv/lib/python3.13/site-packages/mdurl/_parse.py:            # v0.12 TODO(isaacs): This is not quite how Chrome does things.\n./.venv/lib/python3.13/site-packages/sklearn/ensemble/_stacking.py:            # TODO(SLEP6): remove when metadata routing cannot be disabled.\n./.venv/lib/python3.13/site-packages/sklearn/ensemble/_stacking.py:            # TODO(SLEP6): remove when metadata routing cannot be disabled.\n./.venv/lib/python3.13/site-packages/sklearn/ensemble/_weight_boosting.py:    # TODO(1.8): remove \"algorithm\" entry\n./.venv/lib/python3.13/site-packages/sklearn/ensemble/_gb.py:    # TODO: Use loss.fit_intercept_only where appropriate instead of\n./.venv/lib/python3.13/site-packages/sklearn/ensemble/_gb.py:            # TODO: Multiply here by learning rate instead of everywhere else.\n./.venv/lib/python3.13/site-packages/sklearn/ensemble/_gb.py:        # TODO: Without oob, i.e. with self.subsample = 1.0, we could call\n./.venv/lib/python3.13/site-packages/sklearn/ensemble/_gb.py:        y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n./.venv/lib/python3.13/site-packages/sklearn/ensemble/tests/test_weight_boosting.py:# TODO(1.8): remove\n./.venv/lib/python3.13/site-packages/sklearn/ensemble/_hist_gradient_boosting/splitting.pyx:            # LightGBM. TODO: Make this user adjustable?\n./.venv/lib/python3.13/site-packages/sklearn/ensemble/_hist_gradient_boosting/predictor.py:        # TODO: consider always using platform agnostic dtypes for fitted\n./.venv/lib/python3.13/site-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:    # TODO: Ideally this should be computed in parallel over the leaves using something\n./.venv/lib/python3.13/site-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:    TODO: in the future, we could explore the possibility to extend the scorer\n./.venv/lib/python3.13/site-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:            # TODO: remove when PDP supports sample weights\n./.venv/lib/python3.13/site-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:                # TODO: incorporate sample_weight in sampling here, as well as\n./.venv/lib/python3.13/site-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:        # TODO: incorporate sample_weights here in `resample`\n./.venv/lib/python3.13/site-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:        # TODO: This could be done in parallel\n./.venv/lib/python3.13/site-packages/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py:# TODO(1.8) remove the filterwarnings decorator\n./.venv/lib/python3.13/site-packages/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py:        # TODO: We are not entirely satisfied with this lax comparison, but the root\n./.venv/lib/python3.13/site-packages/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py:# TODO(1.8) remove the filterwarnings decorator\n./.venv/lib/python3.13/site-packages/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py:# TODO(1.8) remove the filterwarnings decorator\n./.venv/lib/python3.13/site-packages/sklearn/ensemble/_hist_gradient_boosting/binning.py:        # TODO: complexity is O(n_categorical_features * 255). Maybe this is\n./.venv/lib/python3.13/site-packages/sklearn/ensemble/_base.py:    # TODO(SLEP6): remove if-condition for unrouted sample_weight when metadata\n./.venv/lib/python3.13/site-packages/sklearn/ensemble/_bagging.py:    # TODO: (slep6) remove if condition for unrouted sample_weight when metadata\n./.venv/lib/python3.13/site-packages/sklearn/ensemble/_bagging.py:        # TODO(SLEP6): remove if condition for unrouted sample_weight when metadata\n./.venv/lib/python3.13/site-packages/sklearn/cluster/_hdbscan/hdbscan.py:                # TODO: Support np.nan in Cython implementation for precomputed\n./.venv/lib/python3.13/site-packages/sklearn/cluster/_hdbscan/hdbscan.py:                # TODO: Benchmark KD vs Ball Tree efficiency\n./.venv/lib/python3.13/site-packages/sklearn/cluster/_hdbscan/hdbscan.py:                # TODO: Implement weighted argmin PWD backend\n./.venv/lib/python3.13/site-packages/sklearn/cluster/_hdbscan/_reachability.pyx:        # TODO: Update w/ prange with thread count based on\n./.venv/lib/python3.13/site-packages/sklearn/cluster/tests/test_birch.py:# TODO(1.8): Remove\n./.venv/lib/python3.13/site-packages/sklearn/cluster/tests/test_affinity_propagation.py:# TODO: AffinityPropagation must preserve dtype for its fitted attributes\n./.venv/lib/python3.13/site-packages/sklearn/cluster/_optics.py:    # TODO: handle working_memory somehow?\n./.venv/lib/python3.13/site-packages/sklearn/isotonic.py:        # TODO: remove this branch when Scipy 1.12 is the minimum supported version\n./.venv/lib/python3.13/site-packages/sklearn/manifold/_mds.py:# TODO(1.9): change default `n_init` to 1, see PR #31117\n./.venv/lib/python3.13/site-packages/sklearn/manifold/_mds.py:# TODO(1.9): change default `n_init` to 1, see PR #31117\n./.venv/lib/python3.13/site-packages/sklearn/manifold/_utils.pyx:# TODO: have this function support float32 and float64 and preserve inputs' dtypes.\n./.venv/lib/python3.13/site-packages/sklearn/manifold/tests/test_locally_linear.py:    # TODO: rewrite this test to make less sensitive to the random seed,\n./.venv/lib/python3.13/site-packages/sklearn/manifold/tests/test_locally_linear.py:    # TODO check that it actually does something useful\n./.venv/lib/python3.13/site-packages/sklearn/manifold/tests/test_t_sne.py:    # TODO: compare results on dense and sparse data as proposed in:\n./.venv/lib/python3.13/site-packages/sklearn/manifold/tests/test_t_sne.py:        # TODO: re-enable this test if/when `manhattan_distances` is refactored to\n./.venv/lib/python3.13/site-packages/sklearn/manifold/tests/test_isomap.py:    # TODO check that it actually does something useful\n./.venv/lib/python3.13/site-packages/sklearn/manifold/tests/test_isomap.py:    # TODO: compare results on dense and sparse data as proposed in:\n./.venv/lib/python3.13/site-packages/sklearn/manifold/tests/test_spectral_embedding.py:# TODO: investigate why this test is seed-sensitive on 32-bit Python\n./.venv/lib/python3.13/site-packages/sklearn/manifold/tests/test_mds.py:# TODO(1.9): remove warning filter\n./.venv/lib/python3.13/site-packages/sklearn/manifold/tests/test_mds.py:# TODO(1.9): remove warning filter\n./.venv/lib/python3.13/site-packages/sklearn/manifold/tests/test_mds.py:# TODO(1.9): remove warning filter\n./.venv/lib/python3.13/site-packages/sklearn/manifold/tests/test_mds.py:# TODO(1.9): remove warning filter\n./.venv/lib/python3.13/site-packages/sklearn/manifold/tests/test_mds.py:# TODO(1.9): delete this test\n./.venv/lib/python3.13/site-packages/sklearn/manifold/_spectral_embedding.py:        # TODO(jjerphan): Once SciPy 1.11.3 is the minimum supported version, use\n./.venv/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:        # TODO: better names for these variables: z\n./.venv/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:                # TODO: this could be updated\n./.venv/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:                # Cov_n = Cov_j + x_j * X + increment(betas) TODO:\n./.venv/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:                # TODO: this could be updated\n./.venv/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:        # TODO(1.9): remove \"warn\" and None options.\n./.venv/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:        # TODO(1.9): remove n_alphas and alphas={\"warn\", None}; set alphas=100 by\n./.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:        # TODO(1.8) remove multi_class\n./.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:        # TODO: Refactor this to avoid joblib parallelism entirely when doing binary\n./.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:        # TODO(1.8) remove multi_class\n./.venv/lib/python3.13/site-packages/sklearn/linear_model/_glm/glm.py:        # TODO: if alpha=0 check that X is not rank deficient\n./.venv/lib/python3.13/site-packages/sklearn/linear_model/_glm/glm.py:        # TODO: Adapt link to User Guide in the docstring, once\n./.venv/lib/python3.13/site-packages/sklearn/linear_model/_glm/glm.py:        # TODO: make D^2 a score function in module metrics (and thereby get\n./.venv/lib/python3.13/site-packages/sklearn/linear_model/_glm/_newton_solver.py:            # TODO:\n./.venv/lib/python3.13/site-packages/sklearn/linear_model/_omp.py:            # TODO(SLEP6): remove when metadata routing cannot be disabled.\n./.venv/lib/python3.13/site-packages/sklearn/linear_model/_cd_fast.pyx:                # TODO: use squared L2 norm directly\n./.venv/lib/python3.13/site-packages/sklearn/linear_model/tests/test_common.py:        # TODO: FIx SAGA which fails badly with sample_weights.\n./.venv/lib/python3.13/site-packages/sklearn/linear_model/tests/test_least_angle.py:# TODO: use another dataset that has multiple drops\n./.venv/lib/python3.13/site-packages/sklearn/linear_model/tests/test_least_angle.py:# TODO: remove warning filter when numpy min version >= 2.0.0\n./.venv/lib/python3.13/site-packages/sklearn/linear_model/tests/test_least_angle.py:# TODO: remove warning filter when numpy min version >= 2.0.0\n./.venv/lib/python3.13/site-packages/sklearn/linear_model/tests/test_logistic.py:# TODO(1.8): remove filterwarnings after the deprecation of multi_class\n./.venv/lib/python3.13/site-packages/sklearn/linear_model/tests/test_logistic.py:# TODO(1.8): remove filterwarnings after the deprecation of multi_class\n./.venv/lib/python3.13/site-packages/sklearn/linear_model/tests/test_logistic.py:# TODO(1.8): remove whole test with deprecation of multi_class\n./.venv/lib/python3.13/site-packages/sklearn/linear_model/tests/test_logistic.py:# TODO(1.8): remove filterwarnings after the deprecation of multi_class\n./.venv/lib/python3.13/site-packages/sklearn/linear_model/tests/test_logistic.py:# TODO(1.8): remove filterwarnings after the deprecation of multi_class\n./.venv/lib/python3.13/site-packages/sklearn/linear_model/tests/test_logistic.py:# TODO(1.8): remove filterwarnings after the deprecation of multi_class\n./.venv/lib/python3.13/site-packages/sklearn/linear_model/tests/test_logistic.py:# TODO(1.8): remove filterwarnings after the deprecation of multi_class\n./.venv/lib/python3.13/site-packages/sklearn/linear_model/tests/test_logistic.py:# TODO(1.8): remove filterwarnings after the deprecation of multi_class\n./.venv/lib/python3.13/site-packages/sklearn/linear_model/tests/test_logistic.py:# TODO(1.8): remove filterwarnings after the deprecation of multi_class\n./.venv/lib/python3.13/site-packages/sklearn/linear_model/tests/test_logistic.py:# TODO(1.8): remove filterwarnings after the deprecation of multi_class\n./.venv/lib/python3.13/site-packages/sklearn/linear_model/tests/test_logistic.py:# TODO(1.8): remove filterwarnings after the deprecation of multi_class\n./.venv/lib/python3.13/site-packages/sklearn/linear_model/tests/test_logistic.py:# TODO(1.8): remove filterwarnings after the deprecation of multi_class\n./.venv/lib/python3.13/site-packages/sklearn/linear_model/tests/test_logistic.py:# TODO(1.8): remove filterwarnings after the deprecation of multi_class\n./.venv/lib/python3.13/site-packages/sklearn/linear_model/tests/test_logistic.py:# TODO(1.8): remove filterwarnings after the deprecation of multi_class\n./.venv/lib/python3.13/site-packages/sklearn/linear_model/tests/test_logistic.py:# TODO(1.8): remove filterwarnings after the deprecation of multi_class\n./.venv/lib/python3.13/site-packages/sklearn/linear_model/tests/test_logistic.py:# TODO(1.8): remove\n./.venv/lib/python3.13/site-packages/sklearn/linear_model/tests/test_logistic.py:# TODO(1.8): check for an error instead\n./.venv/lib/python3.13/site-packages/sklearn/linear_model/tests/test_theil_sen.py:# TODO(1.8): Remove\n./.venv/lib/python3.13/site-packages/sklearn/linear_model/tests/test_coordinate_descent.py:# TODO(1.9): remove\n./.venv/lib/python3.13/site-packages/sklearn/linear_model/tests/test_coordinate_descent.py:# TODO(1.9): remove\n./.venv/lib/python3.13/site-packages/sklearn/linear_model/tests/test_coordinate_descent.py:# TODO(1.9): remove\n./.venv/lib/python3.13/site-packages/sklearn/linear_model/tests/test_coordinate_descent.py:# TODO(1.9): remove\n./.venv/lib/python3.13/site-packages/sklearn/linear_model/tests/test_sag.py:    # TODO: uncomment when sparse Ridge with intercept will be fixed (#4710)\n./.venv/lib/python3.13/site-packages/sklearn/linear_model/_linear_loss.py:    # TODO: This \"sandwich product\" is the main computational bottleneck for solvers\n./.venv/lib/python3.13/site-packages/sklearn/linear_model/_base.py:# TODO: bayesian_ridge_regression and bayesian_regression_ard\n./.venv/lib/python3.13/site-packages/sklearn/linear_model/_base.py:        Always an array of ones. TODO: refactor the code base to make it\n./.venv/lib/python3.13/site-packages/sklearn/linear_model/_base.py:# TODO: _rescale_data should be factored into _preprocess_data.\n./.venv/lib/python3.13/site-packages/sklearn/linear_model/_base.py:            # TODO: instead of warning and recomputing, we could just center\n./.venv/lib/python3.13/site-packages/sklearn/linear_model/meson.build:    # TODO in principle this should go in py.exension_module below. This is\n./.venv/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:                # TODO: incorporate sample_weight in sampling here.\n./.venv/lib/python3.13/site-packages/sklearn/neural_network/_base.py:    # TODO: Decide what to do with the term `xlogy(y_true, y_true) - y_true`. For now,\n./.venv/lib/python3.13/site-packages/sklearn/discriminant_analysis.py:        # TODO: Explore the choice of using bincount + add.at as it seems sub optimal\n./.venv/lib/python3.13/site-packages/sklearn/feature_selection/_from_model.py:                # TODO(SLEP6): remove when metadata routing cannot be disabled.\n./.venv/lib/python3.13/site-packages/sklearn/feature_selection/_from_model.py:            # TODO(SLEP6): remove when metadata routing cannot be disabled.\n./.venv/lib/python3.13/site-packages/sklearn/feature_selection/_univariate_selection.py:        # TODO: for Scipy <= 1.10, `isspmatrix(X)` returns `True` for sparse arrays.\n./.venv/lib/python3.13/site-packages/sklearn/feature_selection/_univariate_selection.py:# TODO this class should fit on either p-values or scores,\n./.venv/lib/python3.13/site-packages/sklearn/feature_selection/_rfe.py:    # TODO(1.8) remove this property\n./.venv/lib/python3.13/site-packages/sklearn/feature_selection/_rfe.py:    # TODO(1.8): remove `groups` from the signature after deprecation cycle.\n./.venv/lib/python3.13/site-packages/sklearn/conftest.py:        # TODO: configure numpy to output scalar arrays as regular Python scalars\n./.venv/lib/python3.13/site-packages/sklearn/metrics/cluster/_supervised.py:    # TODO(1.9): remove the sparse parameter\n./.venv/lib/python3.13/site-packages/sklearn/metrics/cluster/tests/test_supervised.py:# TODO(1.9): remove\n./.venv/lib/python3.13/site-packages/sklearn/metrics/_pairwise_distances_reduction/_datasets_pair.pyx.tp:        # TODO: add \"with gil: raise\" here when supporting Cython 3.0\n./.venv/lib/python3.13/site-packages/sklearn/metrics/_pairwise_distances_reduction/_datasets_pair.pyx.tp:        # TODO: add \"with gil: raise\" here when supporting Cython 3.0\n./.venv/lib/python3.13/site-packages/sklearn/metrics/_pairwise_distances_reduction/_datasets_pair.pyx.tp:        # TODO: add \"with gil: raise\" here when supporting Cython 3.0\n./.venv/lib/python3.13/site-packages/sklearn/metrics/_pairwise_distances_reduction/_argkmin_classmode.pyx.tp:            # TODO: Implement other WeightingStrategy values\n./.venv/lib/python3.13/site-packages/sklearn/metrics/_pairwise_distances_reduction/_argkmin_classmode.pyx.tp:            # TODO: inspect if it worth permuting this condition\n./.venv/lib/python3.13/site-packages/sklearn/metrics/_pairwise_distances_reduction/_classmode.pxd:    # TODO: Implement the following options in weighted_histogram_mode\n./.venv/lib/python3.13/site-packages/sklearn/metrics/_pairwise_distances_reduction/_dispatcher.py:            # TODO: implement a stable simultaneous_sort.\n./.venv/lib/python3.13/site-packages/sklearn/metrics/_pairwise_distances_reduction/_dispatcher.py:                # TODO: support CSR matrices without non-zeros elements\n./.venv/lib/python3.13/site-packages/sklearn/metrics/_pairwise_distances_reduction/_dispatcher.py:                # TODO: support CSR matrices with int64 indices and indptr\n./.venv/lib/python3.13/site-packages/sklearn/metrics/_pairwise_distances_reduction/_dispatcher.py:            # TODO: implement Euclidean specialization using GEMM.\n./.venv/lib/python3.13/site-packages/sklearn/metrics/_pairwise_distances_reduction/_dispatcher.py:            # TODO: implement Euclidean specialization using GEMM.\n./.venv/lib/python3.13/site-packages/sklearn/metrics/_pairwise_distances_reduction/_base.pyx.tp:        # TODO: remove this instruction which is a cast in the float32 case\n./.venv/lib/python3.13/site-packages/sklearn/metrics/_pairwise_distances_reduction/meson.build:  # TODO in principle this should go in py.exension_module below. This is\n./.venv/lib/python3.13/site-packages/sklearn/metrics/_pairwise_distances_reduction/meson.build:  # TODO in principle this should go in py.exension_module below. This is\n./.venv/lib/python3.13/site-packages/sklearn/metrics/_pairwise_distances_reduction/meson.build:  # TODO in principle this should go in py.exension_module below. This is\n./.venv/lib/python3.13/site-packages/sklearn/metrics/_pairwise_distances_reduction/meson.build:    # TODO in principle this should go in py.exension_module below. This is\n./.venv/lib/python3.13/site-packages/sklearn/metrics/_pairwise_distances_reduction/meson.build:    # TODO in principle this should go in py.exension_module below. This is\n./.venv/lib/python3.13/site-packages/sklearn/metrics/_pairwise_distances_reduction/meson.build:  # TODO in principle this should go in py.exension_module below. This is\n./.venv/lib/python3.13/site-packages/sklearn/metrics/_pairwise_distances_reduction/meson.build:  # TODO in principle this should go in py.exension_module below. This is\n./.venv/lib/python3.13/site-packages/sklearn/metrics/_dist_metrics.pyx.tp:    # TODO: remove the explicit cast to cnp.intp_t* when cython min version >= 3.0\n./.venv/lib/python3.13/site-packages/sklearn/metrics/pairwise.py:    TODO: use a float64 accumulator in row_norms to avoid the latter.\n./.venv/lib/python3.13/site-packages/sklearn/metrics/pairwise.py:        # TODO: do it also for other norms.\n./.venv/lib/python3.13/site-packages/sklearn/metrics/tests/test_common.py:# TODO: Handle multi_class metrics that has a labels argument as well as a\n./.venv/lib/python3.13/site-packages/sklearn/metrics/tests/test_common.py:            # TODO those metrics doesn't support string label yet\n./.venv/lib/python3.13/site-packages/sklearn/metrics/tests/test_dist_metrics.py:            # TODO: Inspect slight numerical discrepancy\n./.venv/lib/python3.13/site-packages/sklearn/metrics/tests/test_dist_metrics.py:            # TODO: Inspect slight numerical discrepancy\n./.venv/lib/python3.13/site-packages/sklearn/metrics/tests/test_score_objects.py:    # TODO: remove when enable_metadata_routing is deprecated\n./.venv/lib/python3.13/site-packages/sklearn/metrics/tests/test_score_objects.py:# TODO(1.8): remove\n./.venv/lib/python3.13/site-packages/sklearn/metrics/tests/test_pairwise_distances_reduction.py:    # TODO: support CSR matrices without non-zeros elements\n./.venv/lib/python3.13/site-packages/sklearn/metrics/tests/test_pairwise_distances_reduction.py:    # TODO: support CSR matrices with int64 indices and indptr\n./.venv/lib/python3.13/site-packages/sklearn/metrics/tests/test_pairwise_distances_reduction.py:    # TODO: introduce assertions on UserWarnings once the Euclidean specialisation\n./.venv/lib/python3.13/site-packages/sklearn/metrics/tests/test_classification.py:# TODO(1.9): remove test\n./.venv/lib/python3.13/site-packages/sklearn/metrics/tests/test_pairwise.py:        # TODO Fix manhattan_distances to preserve dtype.\n./.venv/lib/python3.13/site-packages/sklearn/metrics/tests/test_pairwise.py:        # TODO Fix manhattan_distances to preserve dtype.\n./.venv/lib/python3.13/site-packages/sklearn/metrics/tests/test_pairwise.py:# TODO(1.8): remove\n./.venv/lib/python3.13/site-packages/sklearn/metrics/_scorer.py:        # TODO(slep006): remove when metadata routing is the only way\n./.venv/lib/python3.13/site-packages/sklearn/metrics/_scorer.py:        # TODO (1.8): remove in 1.8 (scoring=\"max_error\" has been deprecated in 1.6)\n./.venv/lib/python3.13/site-packages/sklearn/metrics/_scorer.py:        # TODO(slep006): remove when metadata routing is the only way\n./.venv/lib/python3.13/site-packages/sklearn/metrics/_scorer.py:        # TODO (1.8): remove in 1.8 (scoring=\"max_error\" has been deprecated in 1.6)\n./.venv/lib/python3.13/site-packages/sklearn/metrics/_scorer.py:                # TODO (1.8): scoring=\"max_error\" has been deprecated in 1.6,\n./.venv/lib/python3.13/site-packages/sklearn/metrics/_scorer.py:        # TODO(slep006): remove when metadata routing is the only way\n./.venv/lib/python3.13/site-packages/sklearn/metrics/_scorer.py:# TODO (1.8): remove in 1.8 (scoring=\"max_error\" has been deprecated in 1.6)\n./.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:    # TODO(1.9): When `raise_warning` is removed, the following changes need to be made:\n./.venv/lib/python3.13/site-packages/sklearn/metrics/_plot/roc_curve.py:        # TODO(1.9): remove after the end of the deprecation period of `y_pred`\n./.venv/lib/python3.13/site-packages/sklearn/metrics/_plot/tests/test_common_curve_display.py:# TODO: remove this test once classes moved to using `name` instead of\n./.venv/lib/python3.13/site-packages/sklearn/metrics/_plot/tests/test_roc_curve_display.py:# TODO(1.9): Remove in 1.9\n./.venv/lib/python3.13/site-packages/sklearn/metrics/_plot/tests/test_roc_curve_display.py:# TODO(1.9): Remove in 1.9\n./.venv/lib/python3.13/site-packages/sklearn/metrics/_plot/tests/test_roc_curve_display.py:# TODO(1.9): remove\n./.venv/lib/python3.13/site-packages/sklearn/metrics/_plot/tests/test_roc_curve_display.py:# TODO(1.9): remove\n./.venv/lib/python3.13/site-packages/sklearn/metrics/meson.build:  # TODO in principle this should go in py.exension_module below. This is\n./.venv/lib/python3.13/site-packages/sklearn/base.py:    # TODO(1.8): Remove this attribute\n./.venv/lib/python3.13/site-packages/sklearn/base.py:    # TODO(1.8): Remove this attribute\n./.venv/lib/python3.13/site-packages/sklearn/base.py:    # TODO(1.8): Remove this attribute\n./.venv/lib/python3.13/site-packages/sklearn/base.py:    # TODO(1.8): Remove this attribute\n./.venv/lib/python3.13/site-packages/sklearn/base.py:    # TODO(1.8): Remove this attribute\n./.venv/lib/python3.13/site-packages/sklearn/base.py:    # TODO(1.8): Remove this check\n./.venv/lib/python3.13/site-packages/sklearn/base.py:    # TODO(1.8): Remove this check\n./.venv/lib/python3.13/site-packages/sklearn/base.py:    # TODO(1.8): Remove this check\n./.venv/lib/python3.13/site-packages/sklearn/base.py:    # TODO(1.8): Remove this check\n./.venv/lib/python3.13/site-packages/sklearn/externals/array_api_compat/cupy/_info.py:        # TODO: Does this depend on device?\n./.venv/lib/python3.13/site-packages/sklearn/externals/array_api_compat/cupy/_info.py:        # TODO: Does this depend on device?\n./.venv/lib/python3.13/site-packages/sklearn/externals/array_api_compat/dask/array/_aliases.py:    # TODO: respect device keyword?\n./.venv/lib/python3.13/site-packages/sklearn/externals/array_api_compat/dask/array/_aliases.py:    # TODO: respect device keyword?\n./.venv/lib/python3.13/site-packages/sklearn/externals/array_api_compat/dask/array/_aliases.py:    # TODO: respect device keyword?\n./.venv/lib/python3.13/site-packages/sklearn/externals/array_api_compat/dask/array/_aliases.py:    # TODO: This won't handle dask unknown shapes\n./.venv/lib/python3.13/site-packages/sklearn/externals/array_api_compat/dask/array/linalg.py:# TODO: use the QR wrapper once dask\n./.venv/lib/python3.13/site-packages/sklearn/externals/array_api_compat/dask/array/linalg.py:    # TODO: can't avoid computing U or V for dask\n./.venv/lib/python3.13/site-packages/sklearn/externals/array_api_compat/torch/_aliases.py:    # TODO: is the return type a list or a tuple\n./.venv/lib/python3.13/site-packages/sklearn/externals/array_api_compat/common/_aliases.py:    # TODO: import from typing (requires Python >=3.13)\n./.venv/lib/python3.13/site-packages/sklearn/externals/array_api_compat/common/_aliases.py:    # TODO: The standard is not clear about what should happen when x.ndim == 0.\n./.venv/lib/python3.13/site-packages/sklearn/externals/array_api_compat/common/_aliases.py:    # TODO: np.clip has other ufunc kwargs\n./.venv/lib/python3.13/site-packages/sklearn/externals/array_api_compat/common/_helpers.py:    # TODO: import from typing (requires Python >=3.13)\n./.venv/lib/python3.13/site-packages/sklearn/externals/array_api_compat/common/_helpers.py:    # TODO: Should we reject ndarray subclasses?\n./.venv/lib/python3.13/site-packages/sklearn/externals/array_api_compat/common/_helpers.py:    # TODO: Account for other backends.\n./.venv/lib/python3.13/site-packages/sklearn/externals/array_api_compat/common/_helpers.py:        # TODO: drop support for numpy<2 which didn't have __array_namespace__\n./.venv/lib/python3.13/site-packages/sklearn/externals/array_api_compat/common/_helpers.py:        # TODO: drop support for jax<0.4.32 which didn't have __array_namespace__\n./.venv/lib/python3.13/site-packages/sklearn/externals/array_api_compat/common/_helpers.py:            # TODO: Support Python scalars?\n./.venv/lib/python3.13/site-packages/sklearn/externals/array_api_compat/common/_helpers.py:        # TODO: What if our array is on the GPU already?\n./.venv/lib/python3.13/site-packages/sklearn/externals/array_api_extra/_lib/_utils/_typing.pyi:# TODO import from typing (requires Python >=3.12)\n./.venv/lib/python3.13/site-packages/sklearn/externals/array_api_extra/_lib/_utils/_typing.pyi:# TODO: use array-api-typing once it is available\n./.venv/lib/python3.13/site-packages/sklearn/externals/array_api_extra/_lib/_utils/_compat.pyi:# TODO import from typing (requires Python >=3.13)\n./.venv/lib/python3.13/site-packages/sklearn/externals/array_api_extra/_lib/_utils/_helpers.py:    # TODO import from typing (requires Python >=3.13)\n./.venv/lib/python3.13/site-packages/sklearn/externals/array_api_extra/_lib/_at.py:    # TODO import from typing (requires Python >=3.11)\n./.venv/lib/python3.13/site-packages/sklearn/externals/array_api_extra/testing.py:    # TODO import override from typing (requires Python >=3.12)\n./.venv/lib/python3.13/site-packages/sklearn/compose/_column_transformer.py:        TODO: Remove when ``set_config(enable_metadata_routing=False)`` is no\n./.venv/lib/python3.13/site-packages/sklearn/compose/tests/test_column_transformer.py:# TODO(1.9): remove this test\n./.venv/lib/python3.13/site-packages/sklearn/datasets/_svmlight_format_io.py:    # TODO We can do this cheaper; sorted_indices copies the whole matrix.\n./.venv/lib/python3.13/site-packages/sklearn/datasets/_svmlight_format_io.py:        # TODO: simplify interfaces and implementations in _svmlight_format_fast.pyx.\n./.venv/lib/python3.13/site-packages/sklearn/datasets/_arff_parser.py:    # TODO: improve for efficiency\n./.venv/lib/python3.13/site-packages/sklearn/datasets/_openml.py:        # TODO: feature request OpenML.\n./.venv/lib/python3.13/site-packages/sklearn/multioutput.py:    # TODO(1.9): Remove base_estimator\n./.venv/lib/python3.13/site-packages/sklearn/multioutput.py:    # TODO(1.8): This is a temporary getter method to validate input wrt deprecation.\n./.venv/lib/python3.13/site-packages/sklearn/multioutput.py:            # TODO: remove this condition check when the minimum supported scipy version\n./.venv/lib/python3.13/site-packages/sklearn/multioutput.py:    # TODO(1.9): Remove base_estimator from __init__\n./.venv/lib/python3.13/site-packages/sklearn/inspection/_partial_dependence.py:    # TODO: we should handle missing values (i.e. `np.nan`) specifically and store them\n./.venv/lib/python3.13/site-packages/sklearn/inspection/_partial_dependence.py:            # TODO(1.9): raise a ValueError instead.\n./.venv/lib/python3.13/site-packages/sklearn/inspection/tests/test_partial_dependence.py:    # TODO: extend to HistGradientBoosting once sample_weight is supported\n./.venv/lib/python3.13/site-packages/sklearn/inspection/tests/test_partial_dependence.py:    # TODO: remove/fix when PDP supports HGBT with sample weights\n./.venv/lib/python3.13/site-packages/sklearn/impute/__init__.py:    # TODO: remove this check once the estimator is no longer experimental.\n./.venv/lib/python3.13/site-packages/sklearn/impute/__init__.py:# TODO: remove this check once the estimator is no longer experimental.\n./.venv/lib/python3.13/site-packages/sklearn/impute/tests/test_impute.py:# TODO (1.8): check that `keep_empty_features=False` drop the\n./.venv/lib/python3.13/site-packages/sklearn/impute/tests/test_impute.py:# TODO (1.8): check that `keep_empty_features=False` drop the\n./.venv/lib/python3.13/site-packages/sklearn/impute/tests/test_impute.py:# TODO (1.8): check that `keep_empty_features=False` drop the\n./.venv/lib/python3.13/site-packages/sklearn/impute/tests/test_impute.py:# TODO (1.8): check that `keep_empty_features=False` drop the\n./.venv/lib/python3.13/site-packages/sklearn/impute/tests/test_impute.py:# TODO (1.8): check that `keep_empty_features=False` drop the\n./.venv/lib/python3.13/site-packages/sklearn/impute/tests/test_impute.py:        # TODO(1.8): Remove the condition and still call getattr(imputer, method)(X)\n./.venv/lib/python3.13/site-packages/sklearn/impute/_iterative.py:        # TODO (1.8): remove this once the deprecation is removed. In the meantime,\n./.venv/lib/python3.13/site-packages/sklearn/impute/_iterative.py:            # TODO (1.8): remove this once the deprecation is removed to keep only\n./.venv/lib/python3.13/site-packages/sklearn/impute/_iterative.py:            # TODO (1.8): remove this once the deprecation is removed to keep only\n./.venv/lib/python3.13/site-packages/sklearn/impute/_iterative.py:                # TODO (1.8): remove this `if` branch once the following issue is\n./.venv/lib/python3.13/site-packages/sklearn/impute/_base.py:            # TODO(1.8): Remove FutureWarning and add `np.nan` as a statistic\n./.venv/lib/python3.13/site-packages/sklearn/impute/_base.py:            # TODO(1.8): Remove FutureWarning and add `np.nan` as a statistic\n./.venv/lib/python3.13/site-packages/sklearn/tree/_classes.py:                # TODO: tree shouldn't need this in this case\n./.venv/lib/python3.13/site-packages/sklearn/tree/_classes.py:                # TODO: the tree shouldn't need this param\n./.venv/lib/python3.13/site-packages/sklearn/tree/_tree.pyx:    # TODO: Convert n_classes to cython.integral memory view once\n./.venv/lib/python3.13/site-packages/sklearn/preprocessing/_polynomial.py:        # TODO: Remove this condition, once scipy 1.10 is the minimum version.\n./.venv/lib/python3.13/site-packages/sklearn/preprocessing/_polynomial.py:            # TODO: Remove this conditional error when the minimum supported version of\n./.venv/lib/python3.13/site-packages/sklearn/preprocessing/_polynomial.py:            # TODO: Remove ones scipy 1.10 is the minimum version. See comments above.\n./.venv/lib/python3.13/site-packages/sklearn/preprocessing/_csr_polynomial_expansion.pyx:# TODO: use `{int,float}{32,64}_t` when cython#5230 is resolved:\n./.venv/lib/python3.13/site-packages/sklearn/preprocessing/_discretization.py:        # TODO(1.9): remove and switch to quantile_method=\"averaged_inverted_cdf\"\n./.venv/lib/python3.13/site-packages/sklearn/preprocessing/_discretization.py:                    # TODO: make _weighted_percentile and\n./.venv/lib/python3.13/site-packages/sklearn/preprocessing/_target_encoder_fast.pyx:    # TODO: parallelize this with OpenMP prange. When n_features >= n_threads, it's\n./.venv/lib/python3.13/site-packages/sklearn/preprocessing/tests/test_discretization.py:    ## TODO: change to averaged inverted cdf, but that means we only get bin\n./.venv/lib/python3.13/site-packages/sklearn/preprocessing/tests/test_discretization.py:    # TODO this check is redundant with common checks and can be removed\n./.venv/lib/python3.13/site-packages/sklearn/preprocessing/_data.py:        # TODO: This should be refactored because binarize also calls\n./.venv/lib/python3.13/site-packages/sklearn/tests/test_common.py:# TODO(1.8): remove test when generate_only is removed\n./.venv/lib/python3.13/site-packages/sklearn/tests/test_common.py:# TODO(1.8): remove this filter\n./.venv/lib/python3.13/site-packages/sklearn/tests/test_common.py:                # TODO: FIX MLP to not check validation set during MLP\n./.venv/lib/python3.13/site-packages/sklearn/tests/test_common.py:# TODO: As more modules support get_feature_names_out they should be removed\n./.venv/lib/python3.13/site-packages/sklearn/tests/test_metadata_routing.py:    # TODO: these test classes can be moved to sklearn.utils._testing once we\n./.venv/lib/python3.13/site-packages/sklearn/tests/test_multioutput.py:# TODO(1.9):  remove when deprecated `base_estimator` is removed\n./.venv/lib/python3.13/site-packages/sklearn/tests/test_docstrings.py:    # TODO: this detection can be improved. Currently we assume that we have\n./.venv/lib/python3.13/site-packages/sklearn/tests/test_base.py:# TODO(1.8): Remove this test when the deprecation is removed\n./.venv/lib/python3.13/site-packages/sklearn/tests/test_calibration.py:    # TODO(1.8): Remove cv=\"prefit\" options here and the @ignore_warnings of the test\n./.venv/lib/python3.13/site-packages/sklearn/tests/test_calibration.py:    # TODO(1.8): remove me once the deprecation period is over.\n./.venv/lib/python3.13/site-packages/sklearn/tests/test_pipeline.py:# TODO(1.8): change warning to checking for NotFittedError\n./.venv/lib/python3.13/site-packages/sklearn/tests/test_metaestimators.py:# TODO: remove data validation for the following estimators\n./.venv/lib/python3.13/site-packages/sklearn/tests/test_docstring_parameters.py:        # TODO(devtools): use _tested_estimators instead of all_estimators in the\n./.venv/lib/python3.13/site-packages/sklearn/tests/test_docstring_parameters.py:    # TODO(1.9) remove\n./.venv/lib/python3.13/site-packages/sklearn/tests/test_docstring_parameters.py:    # TODO(1.9) remove\n./.venv/lib/python3.13/site-packages/sklearn/neighbors/_binary_tree.pxi.tp:# TODO: use cnp.PyArray_ENABLEFLAGS when Cython>=3.0 is used.\n./.venv/lib/python3.13/site-packages/sklearn/neighbors/_binary_tree.pxi.tp:    # TODO: idx_array and node_bounds must not be const, but this change needs\n./.venv/lib/python3.13/site-packages/sklearn/neighbors/_binary_tree.pxi.tp:                    # TODO: remove the explicit cast to cnp.intp_t* when cython min version >= 3.0\n./.venv/lib/python3.13/site-packages/sklearn/neighbors/_binary_tree.pxi.tp:                    # TODO: remove the explicit cast to cnp.intp_t* when cython min version >= 3.0\n./.venv/lib/python3.13/site-packages/sklearn/neighbors/_binary_tree.pxi.tp:                    # TODO: remove the explicit cast to cnp.intp_t* when cython min version >= 3.0\n./.venv/lib/python3.13/site-packages/sklearn/neighbors/_binary_tree.pxi.tp:        # TODO: implement dual tree approach.\n./.venv/lib/python3.13/site-packages/sklearn/neighbors/_binary_tree.pxi.tp:                # TODO: rank by the spread rather than the distance?\n./.venv/lib/python3.13/site-packages/sklearn/neighbors/tests/test_neighbors.py:        # TODO: also test radius_neighbors, but requires different assertion\n./.venv/lib/python3.13/site-packages/sklearn/neighbors/tests/test_neighbors.py:# TODO: remove when NearestNeighbors methods uses parameter validation mechanism\n./.venv/lib/python3.13/site-packages/sklearn/neighbors/tests/test_neighbors.py:# TODO: Remove ignore_warnings when minimum supported SciPy version is 1.17\n./.venv/lib/python3.13/site-packages/sklearn/neighbors/tests/test_neighbors.py:# TODO: Remove ignore_warnings when minimum supported SciPy version is 1.17\n./.venv/lib/python3.13/site-packages/sklearn/neighbors/tests/test_neighbors.py:    # TODO: if score is refactored to evaluate models for other scoring\n./.venv/lib/python3.13/site-packages/sklearn/neighbors/tests/test_lof.py:    # TODO: compare results on dense and sparse data as proposed in:\n./.venv/lib/python3.13/site-packages/sklearn/neighbors/_kde.py:# TODO: implement a brute force version for testing purposes\n./.venv/lib/python3.13/site-packages/sklearn/neighbors/_kde.py:# TODO: create a density estimation base class?\n./.venv/lib/python3.13/site-packages/sklearn/neighbors/_kde.py:        # TODO: implement sampling for other valid kernel shapes\n./.venv/lib/python3.13/site-packages/sklearn/neighbors/_classification.py:            # TODO: systematize this mapping of metric for\n./.venv/lib/python3.13/site-packages/sklearn/neighbors/_classification.py:                # TODO: Implement efficient multi-output solution\n./.venv/lib/python3.13/site-packages/sklearn/neighbors/_classification.py:                    # TODO: adapt the heuristic for `strategy=\"auto\"` for\n./.venv/lib/python3.13/site-packages/sklearn/neighbors/meson.build:    # TODO in principle this should go in py.exension_module below. This is\n./.venv/lib/python3.13/site-packages/sklearn/_loss/_loss.pyx.tp:        # TODO: If every memoryview is contiguous and raw_prediction is\n./.venv/lib/python3.13/site-packages/sklearn/_loss/tests/test_loss.py:            # TODO: What could we test if loss.approx_hessian?\n./.venv/lib/python3.13/site-packages/sklearn/_loss/tests/test_loss.py:                # TODO: What could we test if loss.approx_hessian?\n./.venv/lib/python3.13/site-packages/sklearn/_loss/meson.build:  # TODO in principle this should go in py.exension_module below. This is\n./.venv/lib/python3.13/site-packages/sklearn/decomposition/_dict_learning.py:        # TODO: Make verbosity argument for Lasso?\n./.venv/lib/python3.13/site-packages/sklearn/decomposition/_dict_learning.py:            # TODO: move this handling (which is currently too broad)\n./.venv/lib/python3.13/site-packages/sklearn/decomposition/_lda.py:        # TODO: make Parallel._effective_n_jobs public instead?\n./.venv/lib/python3.13/site-packages/sklearn/decomposition/tests/test_nmf.py:        # TODO: use the provided W when init=\"custom\".\n./.venv/lib/python3.13/site-packages/sklearn/decomposition/tests/test_pca.py:    # TODO: explain what this is testing\n./.venv/lib/python3.13/site-packages/sklearn/decomposition/tests/test_pca.py:    # TODO: explain what this is testing\n./.venv/lib/python3.13/site-packages/sklearn/decomposition/_pca.py:        # TODO: remove the following two lines when scikit-learn only depends\n./.venv/lib/python3.13/site-packages/sklearn/decomposition/_pca.py:            # TODO: remove the following two lines when scikit-learn only\n./.venv/lib/python3.13/site-packages/sklearn/decomposition/_pca.py:        # TODO: update this code to either:\n./.venv/lib/python3.13/site-packages/sklearn/semi_supervised/tests/test_self_training.py:# TODO(1.8): remove in 1.8\n./.venv/lib/python3.13/site-packages/sklearn/semi_supervised/_self_training.py:        # TODO(1.8) remove None option\n./.venv/lib/python3.13/site-packages/sklearn/semi_supervised/_self_training.py:        # TODO(1.8) remove\n./.venv/lib/python3.13/site-packages/sklearn/semi_supervised/_self_training.py:        # TODO(1.8) remove\n./.venv/lib/python3.13/site-packages/sklearn/semi_supervised/_self_training.py:        # TODO(1.8): remove and only keep clone(self.estimator)\n./.venv/lib/python3.13/site-packages/sklearn/semi_supervised/_self_training.py:        # TODO(1.8) remove\n./.venv/lib/python3.13/site-packages/sklearn/semi_supervised/_self_training.py:        # TODO(1.8): remove the condition check together with base_estimator\n./.venv/lib/python3.13/site-packages/sklearn/pipeline.py:    TODO(1.8): remove this context manager and replace with check_is_fitted.\n./.venv/lib/python3.13/site-packages/sklearn/pipeline.py:    # TODO(1.8): Remove this property\n./.venv/lib/python3.13/site-packages/sklearn/pipeline.py:        # TODO(1.8): Remove the context manager and use check_is_fitted(self)\n./.venv/lib/python3.13/site-packages/sklearn/pipeline.py:        # TODO(1.8): Remove the context manager and use check_is_fitted(self)\n./.venv/lib/python3.13/site-packages/sklearn/pipeline.py:        # TODO(1.8): Remove the context manager and use check_is_fitted(self)\n./.venv/lib/python3.13/site-packages/sklearn/pipeline.py:        # TODO(1.8): Remove the context manager and use check_is_fitted(self)\n./.venv/lib/python3.13/site-packages/sklearn/pipeline.py:        # TODO(1.8): Remove the context manager and use check_is_fitted(self)\n./.venv/lib/python3.13/site-packages/sklearn/pipeline.py:        # TODO(1.8): Remove the context manager and use check_is_fitted(self)\n./.venv/lib/python3.13/site-packages/sklearn/pipeline.py:        # TODO(1.8): Remove the context manager and use check_is_fitted(self)\n./.venv/lib/python3.13/site-packages/sklearn/pipeline.py:        # TODO(1.8): Remove the context manager and use check_is_fitted(self)\n./.venv/lib/python3.13/site-packages/sklearn/pipeline.py:            # TODO(SLEP6): remove when metadata routing cannot be disabled.\n./.venv/lib/python3.13/site-packages/sklearn/pipeline.py:            # TODO(SLEP6): remove when metadata routing cannot be disabled.\n./.venv/lib/python3.13/site-packages/sklearn/pipeline.py:            # TODO(SLEP6): remove when metadata routing cannot be disabled.\n./.venv/lib/python3.13/site-packages/sklearn/calibration.py:            # TODO(1.8): Remove this code branch and cv='prefit'\n./.venv/lib/python3.13/site-packages/sklearn/calibration.py:        # TODO: Remove casting to np.float64 when minimum supported SciPy is 1.11.2\n./.venv/lib/python3.13/site-packages/sklearn/svm/src/libsvm/libsvm_helper.c:     * TODO: does this provoke memory leaks (we just malloc'ed them)?\n./.venv/lib/python3.13/site-packages/sklearn/svm/src/libsvm/libsvm_sparse_helper.c: * TODO: precomputed kernel.\n./.venv/lib/python3.13/site-packages/sklearn/svm/src/libsvm/libsvm_sparse_helper.c: * TODO: merge in the cython layer\n./.venv/lib/python3.13/site-packages/sklearn/svm/_libsvm.pyx:    # TODO: use check_model\n./.venv/lib/python3.13/site-packages/sklearn/svm/_libsvm_sparse.pyx:    # TODO: custom kernel\n./.venv/lib/python3.13/site-packages/sklearn/svm/_libsvm_sparse.pyx:    # TODO: do only in classification\n./.venv/lib/python3.13/site-packages/sklearn/svm/_libsvm_sparse.pyx:    # TODO: use check_model\n./.venv/lib/python3.13/site-packages/sklearn/svm/_libsvm_sparse.pyx:    # TODO: use check_model\n./.venv/lib/python3.13/site-packages/sklearn/svm/tests/test_svm.py:TODO: remove hard coded numerical results when possible\n./.venv/lib/python3.13/site-packages/sklearn/svm/tests/test_bounds.py:# TODO(1.8): remove filterwarnings after the deprecation of liblinear multiclass\n./.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:            # TODO: add keyword copy to copy on demand\n./.venv/lib/python3.13/site-packages/sklearn/model_selection/__init__.py:    # TODO: remove this check once the estimator is no longer experimental.\n./.venv/lib/python3.13/site-packages/sklearn/model_selection/__init__.py:# TODO: remove this check once the estimator is no longer experimental.\n./.venv/lib/python3.13/site-packages/sklearn/model_selection/_search.py:    # TODO(1.8) remove this property\n./.venv/lib/python3.13/site-packages/sklearn/model_selection/_search.py:        # TODO(slep006): remove when metadata routing is the only way\n./.venv/lib/python3.13/site-packages/sklearn/model_selection/_validation.py:    # TODO(SLEP6): To be removed when set_config(enable_metadata_routing=False) is not\n./.venv/lib/python3.13/site-packages/sklearn/model_selection/_validation.py:# TODO(SLEP6): To be removed when set_config(enable_metadata_routing=False) is not\n./.venv/lib/python3.13/site-packages/sklearn/model_selection/_validation.py:                # TODO(SLEP6): also pass metadata to the predict method for\n./.venv/lib/python3.13/site-packages/sklearn/model_selection/_validation.py:                # TODO(SLEP6): also pass metadata for the predict method.\n./.venv/lib/python3.13/site-packages/sklearn/model_selection/_validation.py:                # TODO(SLEP6): also pass metadata to the predict method for\n./.venv/lib/python3.13/site-packages/sklearn/model_selection/_validation.py:                # TODO(SLEP6): also pass metadata to the predict method for\n./.venv/lib/python3.13/site-packages/sklearn/model_selection/tests/test_validation.py:# TODO(1.8): remove `learning_curve`, `validation_curve` and `permutation_test_score`.\n./.venv/lib/python3.13/site-packages/sklearn/model_selection/tests/test_search.py:    # Test the IID parameter  TODO: Clearly this test does something else???\n./.venv/lib/python3.13/site-packages/sklearn/utils/_indexing.py:        # TODO(pyarrow): remove version checking and following if-branch when\n./.venv/lib/python3.13/site-packages/sklearn/utils/_indexing.py:    # TODO(1.9) remove UserList when the force_int_remainder_cols param\n./.venv/lib/python3.13/site-packages/sklearn/utils/_indexing.py:        # TODO: we should probably use _is_pandas_df_or_series(X) instead but:\n./.venv/lib/python3.13/site-packages/sklearn/utils/_indexing.py:            # TODO(1.3): check if the warning is still raised or remove the filter.\n./.venv/lib/python3.13/site-packages/sklearn/utils/_test_common/instance_generator.py:# TODO(devtools): allow third-party developers to pass test specific params to checks\n./.venv/lib/python3.13/site-packages/sklearn/utils/_test_common/instance_generator.py:    # TODO(devtools): check that function names here exist in checks for the estimator\n./.venv/lib/python3.13/site-packages/sklearn/utils/_test_common/instance_generator.py:    # TODO(1.9) simplify when averaged_inverted_cdf is the default\n./.venv/lib/python3.13/site-packages/sklearn/utils/_test_common/instance_generator.py:            # TODO: dual=True is a stochastic solver: we cannot rely on\n./.venv/lib/python3.13/site-packages/sklearn/utils/_test_common/instance_generator.py:    # TODO(devtools): enable this behavior for third party estimators as well\n./.venv/lib/python3.13/site-packages/sklearn/utils/_test_common/instance_generator.py:        # TODO: replace by a statistical test, see meta-issue #16298\n./.venv/lib/python3.13/site-packages/sklearn/utils/_test_common/instance_generator.py:        # TODO: replace by a statistical test, see meta-issue #16298\n./.venv/lib/python3.13/site-packages/sklearn/utils/_test_common/instance_generator.py:        # TODO: replace by a statistical test, see meta-issue #16298\n./.venv/lib/python3.13/site-packages/sklearn/utils/_test_common/instance_generator.py:        # TODO: replace by a statistical test, see meta-issue #16298\n./.venv/lib/python3.13/site-packages/sklearn/utils/_test_common/instance_generator.py:        # TODO: replace by a statistical test, see meta-issue #16298\n./.venv/lib/python3.13/site-packages/sklearn/utils/_test_common/instance_generator.py:        # TODO: investigate failure see meta-issue #16298\n./.venv/lib/python3.13/site-packages/sklearn/utils/_test_common/instance_generator.py:        # TODO: investigate failure see meta-issue #16298\n./.venv/lib/python3.13/site-packages/sklearn/utils/_test_common/instance_generator.py:        # TODO: replace by a statistical test, see meta-issue #16298\n./.venv/lib/python3.13/site-packages/sklearn/utils/_test_common/instance_generator.py:        # TODO: replace by a statistical test, see meta-issue #16298\n./.venv/lib/python3.13/site-packages/sklearn/utils/_test_common/instance_generator.py:        # TODO: replace by a statistical test, see meta-issue #16298\n./.venv/lib/python3.13/site-packages/sklearn/utils/_test_common/instance_generator.py:        # TODO: replace by a statistical test, see meta-issue #16298\n./.venv/lib/python3.13/site-packages/sklearn/utils/_test_common/instance_generator.py:        # TODO: replace by a statistical test when _dual=True, see meta-issue #16298\n./.venv/lib/python3.13/site-packages/sklearn/utils/_test_common/instance_generator.py:        # TODO: replace by a statistical test, see meta-issue #16298\n./.venv/lib/python3.13/site-packages/sklearn/utils/_test_common/instance_generator.py:        # TODO: fix sample_weight handling of this estimator, see meta-issue #16298\n./.venv/lib/python3.13/site-packages/sklearn/utils/_test_common/instance_generator.py:        # TODO: replace by a statistical test, see meta-issue #16298\n./.venv/lib/python3.13/site-packages/sklearn/utils/_test_common/instance_generator.py:        # TODO: fix sample_weight handling of this estimator when probability=False\n./.venv/lib/python3.13/site-packages/sklearn/utils/_test_common/instance_generator.py:        # TODO: replace by a statistical test when probability=True\n./.venv/lib/python3.13/site-packages/sklearn/utils/_test_common/instance_generator.py:        # TODO: fix sample_weight handling of this estimator, see meta-issue #16298\n./.venv/lib/python3.13/site-packages/sklearn/utils/_test_common/instance_generator.py:        # TODO: fix sample_weight handling of this estimator, see meta-issue #16298\n./.venv/lib/python3.13/site-packages/sklearn/utils/_test_common/instance_generator.py:        # TODO: replace by a statistical test, see meta-issue #16298\n./.venv/lib/python3.13/site-packages/sklearn/utils/_test_common/instance_generator.py:        # TODO: replace by a statistical test, see meta-issue #16298\n./.venv/lib/python3.13/site-packages/sklearn/utils/_test_common/instance_generator.py:        # TODO: replace by a statistical test, see meta-issue #16298\n./.venv/lib/python3.13/site-packages/sklearn/utils/_test_common/instance_generator.py:        # TODO: replace by a statistical test, see meta-issue #16298\n./.venv/lib/python3.13/site-packages/sklearn/utils/_test_common/instance_generator.py:        # TODO: replace by a statistical test, see meta-issue #16298\n./.venv/lib/python3.13/site-packages/sklearn/utils/_test_common/instance_generator.py:        # TODO: replace by a statistical test, see meta-issue #16298\n./.venv/lib/python3.13/site-packages/sklearn/utils/_test_common/instance_generator.py:        # TODO: replace by a statistical test, see meta-issue #16298\n./.venv/lib/python3.13/site-packages/sklearn/utils/_test_common/instance_generator.py:        # TODO: replace by a statistical test, see meta-issue #16298\n./.venv/lib/python3.13/site-packages/sklearn/utils/_test_common/instance_generator.py:        # TODO: fix sample_weight handling of this estimator when probability=False\n./.venv/lib/python3.13/site-packages/sklearn/utils/_test_common/instance_generator.py:        # TODO: replace by a statistical test when probability=True\n./.venv/lib/python3.13/site-packages/sklearn/utils/_test_common/instance_generator.py:        # TODO: fix sample_weight handling of this estimator, see meta-issue #16298\n./.venv/lib/python3.13/site-packages/sklearn/utils/_test_common/instance_generator.py:# TODO: remove when scipy min version >= 1.11\n./.venv/lib/python3.13/site-packages/sklearn/utils/_test_common/instance_generator.py:        # TODO: remove when scipy min version >= 1.16\n./.venv/lib/python3.13/site-packages/sklearn/utils/arrayfuncs.pyx:# TODO: put transpose as an option\n./.venv/lib/python3.13/site-packages/sklearn/utils/_random.pyx:        # TODO the pool based method can also be used.\n./.venv/lib/python3.13/site-packages/sklearn/utils/fixes.py:# TODO: We can consider removing the containers and importing\n./.venv/lib/python3.13/site-packages/sklearn/utils/fixes.py:# TODO: Remove when SciPy 1.11 is the minimum supported version\n./.venv/lib/python3.13/site-packages/sklearn/utils/fixes.py:# TODO: Remove when Scipy 1.12 is the minimum supported version\n./.venv/lib/python3.13/site-packages/sklearn/utils/fixes.py:# TODO : remove this when required minimum version of scipy >= 1.9.0\n./.venv/lib/python3.13/site-packages/sklearn/utils/fixes.py:# TODO: Fuse the modern implementations of _sparse_min_max and _sparse_nan_min_max\n./.venv/lib/python3.13/site-packages/sklearn/utils/fixes.py:# TODO: Adapt when Pandas > 2.2 is the minimum supported version\n./.venv/lib/python3.13/site-packages/sklearn/utils/fixes.py:# TODO: remove when SciPy 1.12 is the minimum supported version\n./.venv/lib/python3.13/site-packages/sklearn/utils/fixes.py:# TODO: remove when SciPy 1.12 is the minimum supported version\n./.venv/lib/python3.13/site-packages/sklearn/utils/fixes.py:# TODO: Remove when Scipy 1.12 is the minimum supported version\n./.venv/lib/python3.13/site-packages/sklearn/utils/fixes.py:# TODO: Remove when Python min version >= 3.12.\n./.venv/lib/python3.13/site-packages/sklearn/utils/fixes.py:# TODO: Remove when Scipy 1.15 is the minimum supported version. In scipy 1.15,\n./.venv/lib/python3.13/site-packages/sklearn/utils/fixes.py:# TODO(pyarrow): Remove when minimum pyarrow version is 17.0.0\n./.venv/lib/python3.13/site-packages/sklearn/utils/murmurhash.pyx:    # TODO make it possible to pass preallocated output array\n./.venv/lib/python3.13/site-packages/sklearn/utils/murmurhash.pyx:    # TODO make it possible to pass preallocated output array\n./.venv/lib/python3.13/site-packages/sklearn/utils/estimator_checks.py:    # TODO: test with intercept\n./.venv/lib/python3.13/site-packages/sklearn/utils/estimator_checks.py:    # TODO: test with multiple responses\n./.venv/lib/python3.13/site-packages/sklearn/utils/estimator_checks.py:            TODO(1.8): remove return value\n./.venv/lib/python3.13/site-packages/sklearn/utils/estimator_checks.py:    # TODO(1.8): remove generate_only\n./.venv/lib/python3.13/site-packages/sklearn/utils/estimator_checks.py:        # TODO There are a few errors in SearchCV with array-api-strict because\n./.venv/lib/python3.13/site-packages/sklearn/utils/estimator_checks.py:    # TODO: find out why PLS and CCA fail. RANSAC is random\n./.venv/lib/python3.13/site-packages/sklearn/utils/estimator_checks.py:        # TODO(devtools): this should be a separate check.\n./.venv/lib/python3.13/site-packages/sklearn/utils/estimator_checks.py:                    # TODO(devtools): separately check that the constructor doesn't\n./.venv/lib/python3.13/site-packages/sklearn/utils/_estimator_html_repr.py:# TODO(1.8): Remove the entire module\n./.venv/lib/python3.13/site-packages/sklearn/utils/_array_api.py:# TODO: complete __all__\n./.venv/lib/python3.13/site-packages/sklearn/utils/_array_api.py:    # TODO: Update to use `__array_namespace__info__()` from array-api v2023.12\n./.venv/lib/python3.13/site-packages/sklearn/utils/_array_api.py:    # TODO: Remove this once https://github.com/scipy/scipy/issues/21736 is fixed\n./.venv/lib/python3.13/site-packages/sklearn/utils/_array_api.py:    # TODO: refactor once nan-aware reductions are standardized:\n./.venv/lib/python3.13/site-packages/sklearn/utils/_array_api.py:    # TODO: refactor once nan-aware reductions are standardized:\n./.venv/lib/python3.13/site-packages/sklearn/utils/_array_api.py:    # TODO: refactor once nan-aware reductions are standardized:\n./.venv/lib/python3.13/site-packages/sklearn/utils/_array_api.py:    # TODO: once sufficiently adopted, we might want to instead rely on the\n./.venv/lib/python3.13/site-packages/sklearn/utils/_array_api.py:    # TODO: update if bincount is ever adopted in a future version of the standard:\n./.venv/lib/python3.13/site-packages/sklearn/utils/deprecation.py:# TODO(1.8): remove force_all_finite and change the default value of ensure_all_finite\n./.venv/lib/python3.13/site-packages/sklearn/utils/_plotting.py:        # TODO(1.9): Remove deprecated **kwargs\n./.venv/lib/python3.13/site-packages/sklearn/utils/tests/test_estimator_html_repr.py:# TODO(1.8): Remove the entire file\n./.venv/lib/python3.13/site-packages/sklearn/utils/tests/test_validation.py:# TODO: add this estimator into the _mocking module in a further refactoring\n./.venv/lib/python3.13/site-packages/sklearn/utils/tests/test_validation.py:# TODO(1.8): remove\n./.venv/lib/python3.13/site-packages/sklearn/utils/tests/test_plotting.py:# TODO(1.9) : Remove\n./.venv/lib/python3.13/site-packages/sklearn/utils/tests/test_tags.py:# TODO(1.8): Update when implementing __sklearn_tags__ is required\n./.venv/lib/python3.13/site-packages/sklearn/utils/tests/test_tags.py:# TODO(1.8): Update this test to check for errors\n./.venv/lib/python3.13/site-packages/sklearn/utils/tests/test_extmath.py:    # more accurate but slow (TODO find realistic settings here)\n./.venv/lib/python3.13/site-packages/sklearn/utils/tests/test_array_api.py:# TODO: add cupy to the list of libraries once the following upstream issue\n./.venv/lib/python3.13/site-packages/sklearn/utils/_tags.py:# TODO(1.8): Remove this function\n./.venv/lib/python3.13/site-packages/sklearn/utils/_tags.py:        # TODO(1.8): turn the warning into an error\n./.venv/lib/python3.13/site-packages/sklearn/utils/stats.py:# TODO: refactor to do the symmetrisation inside _weighted_percentile to avoid\n./.venv/lib/python3.13/site-packages/sklearn/utils/_sorting.pyx:    # TODO: In order to support discrete distance metrics, we need to have a\n./.venv/lib/python3.13/site-packages/sklearn/utils/_cython_blas.pyx:    # TODO: Remove the pointer casts below once SciPy uses const-qualification.\n./.venv/lib/python3.13/site-packages/sklearn/utils/_typedefs.pxd:# TODO: Stop defining custom types locally or globally like DTYPE_t and friends and\n./.venv/lib/python3.13/site-packages/sklearn/utils/_testing.py:        # TODO: remove when pyamg > 5.0.1\n./.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:    # TODO: Remove when the minimum version of SciPy supported is 1.12\n./.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:        # TODO: remove the pandas-specific branch once the minimum supported\n./.venv/lib/python3.13/site-packages/sklearn/utils/meson.build:    # TODO in principle this should go in py.exension_module below. This is\n./.venv/lib/python3.13/site-packages/sklearn/utils/optimize.py:        # TODO: It seems that the new check for the sum of absolute gradients above\n./.venv/lib/python3.13/site-packages/markdown_it/rules_core/replacements.py:# TODO:\n./.venv/lib/python3.13/site-packages/markdown_it/parser_inline.py:            # TODO: remove this workaround when CM standard will allow nested links\n./.venv/lib/python3.13/site-packages/yaml/scanner.py:        # TODO: support for BOM within a stream.\n./.venv/lib/python3.13/site-packages/yaml/scanner.py:        # TODO: We need to make tab handling rules more sane. A good rule is\n./.venv/lib/python3.13/site-packages/websocket/_http.py:    # TODO: Use python-socks for http protocol also, to standardize flow\n./.venv/lib/python3.13/site-packages/websocket/_http.py:    # TODO: support digest auth.\n./.venv/lib/python3.13/site-packages/websocket/tests/test_websocket.py:        # TODO: add longer frame data\n./.venv/lib/python3.13/site-packages/websocket/tests/test_websocket.py:        # TODO: add longer frame data\n./.venv/lib/python3.13/site-packages/websocket/tests/test_http.py:        # TODO: Test SOCKS4 and SOCK5 proxies with unit tests\n./.venv/lib/python3.13/site-packages/grpc/beta/_client_adaptations.py:        pass  # TODO(https://github.com/grpc/grpc/issues/4078): design, implement.\n./.venv/lib/python3.13/site-packages/grpc/beta/_server_adaptations.py:        pass  # TODO(https://github.com/grpc/grpc/issues/4078): design, implement.\n./.venv/lib/python3.13/site-packages/grpc/beta/_server_adaptations.py:                return None  # TODO(nathaniel): call the multimethod.\n./.venv/lib/python3.13/site-packages/grpc/aio/_server.py:        # TODO(xuanwn): Implement this for AsyncIO.\n./.venv/lib/python3.13/site-packages/grpc/aio/_channel.py:                # TODO(lidiz) drop this hack after 3.8 deprecation\n./.venv/lib/python3.13/site-packages/grpc/aio/_channel.py:    # TODO(xuanwn): Implement this method after we have\n./.venv/lib/python3.13/site-packages/grpc/aio/_channel.py:    # TODO(xuanwn): Implement _registered_method after we have\n./.venv/lib/python3.13/site-packages/grpc/aio/_channel.py:    # TODO(xuanwn): Implement _registered_method after we have\n./.venv/lib/python3.13/site-packages/grpc/aio/_channel.py:    # TODO(xuanwn): Implement _registered_method after we have\n./.venv/lib/python3.13/site-packages/grpc/aio/_channel.py:    # TODO(xuanwn): Implement _registered_method after we have\n./.venv/lib/python3.13/site-packages/grpc/_auth.py:    # TODO(xuanwn): Give credentials an actual type.\n./.venv/lib/python3.13/site-packages/grpc/_observability.py:    # TODO(xuanwn): use channel args to exclude those metrics.\n./.venv/lib/python3.13/site-packages/grpc/_server.py:        # TODO(https://github.com/grpc/grpc/issues/6597): eliminate these fields.\n./.venv/lib/python3.13/site-packages/grpc/_server.py:# TODO(https://github.com/grpc/grpc/issues/6597): delete this function.\n./.venv/lib/python3.13/site-packages/grpc/_server.py:        # TODO(xuanwn): We should validate method_handlers first.\n./.venv/lib/python3.13/site-packages/grpc/_channel.py:# TODO(xuanwn): Create a base class for IntegratedCall and SegregatedCall.\n./.venv/lib/python3.13/site-packages/grpc/_channel.py:    # TODO(xuanwn): Refactor this: https://github.com/grpc/grpc/issues/31704\n./.venv/lib/python3.13/site-packages/grpc/_channel.py:        # TODO(https://github.com/grpc/grpc/issues/12531): Several releases\n./.venv/lib/python3.13/site-packages/torchgen/packaged/autograd/gen_autograd_functions.py:# TODO: This is probably not exhaustive, but it's a start\n./.venv/lib/python3.13/site-packages/torchgen/packaged/autograd/gen_trace_type.py:    # TODO: byte-for-byte compatible with old codegen behavior - should clean up\n./.venv/lib/python3.13/site-packages/torchgen/packaged/autograd/gen_trace_type.py:    # TODO: clean up old codegen behavior\n./.venv/lib/python3.13/site-packages/torchgen/packaged/autograd/gen_inplace_or_view_type.py:    # TODO: Should handle optional here?\n./.venv/lib/python3.13/site-packages/torchgen/packaged/autograd/gen_inplace_or_view_type.py:    # TODO: Should handle optional here?\n./.venv/lib/python3.13/site-packages/torchgen/packaged/autograd/gen_inplace_or_view_type.py:    return f.func.name.name.base  # TODO: should be str(f.func.name.name)?\n./.venv/lib/python3.13/site-packages/torchgen/packaged/autograd/gen_inplace_or_view_type.py:    # TODO: Clean this logic up if we get rid of reverse view funcs or reify them.\n./.venv/lib/python3.13/site-packages/torchgen/packaged/autograd/gen_python_functions.py:        # TODO: should use some canonical form instead of 'str(arg.type)' - see comments\n./.venv/lib/python3.13/site-packages/torchgen/packaged/autograd/gen_python_functions.py:        # TODO: Checking `ps.method and ('requires_grad' in parser_outputs)` is a hacky\n./.venv/lib/python3.13/site-packages/torchgen/packaged/autograd/gen_variable_type.py:            # TODO: it would be nice to not have these special cases\n./.venv/lib/python3.13/site-packages/torchgen/packaged/autograd/gen_variable_type.py:        # TODO: `cpp_type` is only to keep it byte-for-byte compatible with the old codegen, should remove.\n./.venv/lib/python3.13/site-packages/torchgen/packaged/autograd/gen_variable_type.py:                    # TODO(crcrpar): Make it simpler.\n./.venv/lib/python3.13/site-packages/torchgen/packaged/autograd/gen_variable_type.py:            # TODO: process all derivative formulas!!!\n./.venv/lib/python3.13/site-packages/torchgen/packaged/autograd/gen_variable_type.py:            # TODO: should be `arg.type.is_tensor_like()`?\n./.venv/lib/python3.13/site-packages/torchgen/packaged/autograd/gen_variable_type.py:            base_name = f.func.name.name.base  # TODO: should be str(f.func.name.name)?\n./.venv/lib/python3.13/site-packages/torchgen/packaged/autograd/gen_variable_type.py:        # TODO: flatten allocates a std::vector, which could be expensive\n./.venv/lib/python3.13/site-packages/torchgen/packaged/autograd/gen_variable_type.py:                # TODO update this when inplace namings are unified\n./.venv/lib/python3.13/site-packages/torchgen/packaged/autograd/gen_variable_type.py:                    # TODO(crcrpar): Should this (= the foreach specific logic) be refactored somehow?\n./.venv/lib/python3.13/site-packages/torchgen/packaged/autograd/derivatives.yaml:# TODO: this derivative is not SymInt safe, need sum_to support\n./.venv/lib/python3.13/site-packages/torchgen/packaged/autograd/derivatives.yaml:      # TODO: replace this with grad.expand_as(self) / self.sym_numel() when that is supported\n./.venv/lib/python3.13/site-packages/torchgen/packaged/autograd/derivatives.yaml:# TODO: This note is only referenced by to_dense and to_sparse*. Make\n./.venv/lib/python3.13/site-packages/torchgen/packaged/autograd/derivatives.yaml:      # TODO: replace this with grad.expand_as(self) when that is supported\n./.venv/lib/python3.13/site-packages/torchgen/packaged/autograd/derivatives.yaml:      # TODO: replace this function once semantics for nested tensor expand have been settled on\n./.venv/lib/python3.13/site-packages/torchgen/packaged/autograd/derivatives.yaml:  # TODO: remove to_dense after gh-107381 is fixed\n./.venv/lib/python3.13/site-packages/torchgen/packaged/autograd/derivatives.yaml:  # TODO: glu_backward can benefit from forward result,\n./.venv/lib/python3.13/site-packages/torchgen/packaged/autograd/templates/variable_factories.h:    at::AutoDispatchBelowAutograd guard;  // TODO: remove\n./.venv/lib/python3.13/site-packages/torchgen/packaged/autograd/templates/variable_factories.h:    at::AutoDispatchBelowAutograd guard;  // TODO: remove\n./.venv/lib/python3.13/site-packages/torchgen/packaged/autograd/templates/variable_factories.h:    at::AutoDispatchBelowAutograd guard;  // TODO: remove\n./.venv/lib/python3.13/site-packages/torchgen/packaged/autograd/templates/variable_factories.h:    at::AutoDispatchBelowAutograd guard;  // TODO: remove\n./.venv/lib/python3.13/site-packages/torchgen/packaged/autograd/templates/Functions.h:    // TODO(crcrpar): Use `std::move(saved_for)` to avoid incrementing refcount, which would need refactoring.\n./.venv/lib/python3.13/site-packages/torchgen/packaged/autograd/templates/python_variable_methods.cpp:    // TODO: consider factoring this out\n./.venv/lib/python3.13/site-packages/torchgen/packaged/autograd/templates/python_variable_methods.cpp:  // TODO: change the condition to `self_.dim() != 0` once we expose scalars\n./.venv/lib/python3.13/site-packages/torchgen/packaged/autograd/templates/python_variable_methods.cpp:  // TODO: Make this call the TensorOptions version, maybe?\n./.venv/lib/python3.13/site-packages/torchgen/packaged/autograd/templates/python_variable_methods.cpp:  // TODO: Make this call the TensorOptions version, maybe?\n./.venv/lib/python3.13/site-packages/torchgen/packaged/autograd/gen_variable_factories.py:# TODO: maybe update the cpp argument API to take optional namespace argument?\n./.venv/lib/python3.13/site-packages/torchgen/packaged/autograd/load_derivatives.py:# TODO: Why is this going through CppSignatureGroup, that doesn't make sense...\n./.venv/lib/python3.13/site-packages/torchgen/packaged/autograd/load_derivatives.py:            # TODO we are trolling\n./.venv/lib/python3.13/site-packages/torchgen/packaged/autograd/load_derivatives.py:        # TODO: do we need eagerly calculate and save it here? Can it be derived\n./.venv/lib/python3.13/site-packages/torchgen/packaged/autograd/load_derivatives.py:    # TODO: maybe the logic to handle the legacy schema is no longer necessary?\n./.venv/lib/python3.13/site-packages/torchgen/packaged/ATen/templates/DispatchKeyFunctions.h:// TODO Undo all logic introduced for Note [Avoiding Include Cycles In Static Dispatch]\n./.venv/lib/python3.13/site-packages/torchgen/packaged/ATen/templates/TensorBody.h:  // TODO: temporarily disabled\n./.venv/lib/python3.13/site-packages/torchgen/packaged/ATen/templates/TensorBody.h:  // TODO: Deprecate me\n./.venv/lib/python3.13/site-packages/torchgen/packaged/ATen/templates/TensorBody.h:  // TODO: The Python version also accepts arguments\n./.venv/lib/python3.13/site-packages/torchgen/packaged/ATen/templates/TensorBody.h:  // TODO: remove following two after at::kDouble and its friends are TypeMeta's.\n./.venv/lib/python3.13/site-packages/torchgen/packaged/ATen/templates/FunctionalInverses.h:// TODO: Change codegen to generate these. See the following link:\n./.venv/lib/python3.13/site-packages/torchgen/packaged/ATen/templates/Functions.cpp:   AutoDispatchBelowADInplaceOrView guard{}; // TODO: Remove.\n./.venv/lib/python3.13/site-packages/torchgen/packaged/ATen/templates/CompositeViewCopyKernels.cpp:// TODO: rename this file to something more generic.\n./.venv/lib/python3.13/site-packages/torchgen/packaged/ATen/templates/CompositeViewCopyKernels.cpp:// TODO: this doesn't handle restriding empty tensors correctly; see\n./.venv/lib/python3.13/site-packages/torchgen/packaged/ATen/native/native_functions.yaml:# TODO: need to specify this is side-effectful somehow\n./.venv/lib/python3.13/site-packages/torchgen/packaged/ATen/native/native_functions.yaml:# TODO: Why do I have to call this grad?!\n./.venv/lib/python3.13/site-packages/torchgen/packaged/ATen/native/native_functions.yaml:# TODO: Add this function to MPS dispatch key so that we avoid declaring it in\n./.venv/lib/python3.13/site-packages/torchgen/packaged/ATen/native/native_functions.yaml:# TODO: In 2 weeks, we should make native_batch_norm composite implicit so that this correct schema percolates correctly through our dispatching\n./.venv/lib/python3.13/site-packages/torchgen/packaged/ATen/native/native_functions.yaml:# TODO: add a copy kwarg that guarantees that the tensor is put into fresh\n./.venv/lib/python3.13/site-packages/torchgen/packaged/ATen/native/native_functions.yaml:  # TODO: Align the signature of sum.dim_IntList and _sparse_csr_sum.dim_dtype\n./.venv/lib/python3.13/site-packages/torchgen/packaged/ATen/native/native_functions.yaml:# TODO: this function will be replaced once nested expand semantics have been settled on\n./.venv/lib/python3.13/site-packages/torchgen/packaged/ATen/native/native_functions.yaml:# TODO: namespace threshold in 'nn'\n./.venv/lib/python3.13/site-packages/torchgen/packaged/ATen/native/native_functions.yaml:# TODO: reduce signatures down to one when optional args is available\n./.venv/lib/python3.13/site-packages/torchgen/packaged/ATen/native/native_functions.yaml:# TODO: clone indices in sparse tensor ctor.\n./.venv/lib/python3.13/site-packages/torchgen/packaged/ATen/native/native_functions.yaml:# TODO: Two weeks after this lands, combine these two overloads,\n./.venv/lib/python3.13/site-packages/torchgen/packaged/ATen/native/native_functions.yaml:# TODO: this should have a view annotation\n./.venv/lib/python3.13/site-packages/torchgen/packaged/ATen/native/native_functions.yaml:# TODO: shouldn't be a method\n./.venv/lib/python3.13/site-packages/torchgen/packaged/ATen/native/native_functions.yaml:# TODO: remove dispatch section when porting TH CUDA to ATen\n./.venv/lib/python3.13/site-packages/torchgen/utils.py:# TODO: Use a real parser here; this will get bamboozled\n./.venv/lib/python3.13/site-packages/torchgen/utils.py:        # TODO: this does the wrong thing with KeyError\n./.venv/lib/python3.13/site-packages/torchgen/utils.py:    # TODO: put this somewhere else, maybe\n./.venv/lib/python3.13/site-packages/torchgen/gen_backend_stubs.py:            # TODO: allow structured external backends later.\n./.venv/lib/python3.13/site-packages/torchgen/api/autograd.py:    # TODO: maybe the logic to search for all variants is no longer necessary?\n./.venv/lib/python3.13/site-packages/torchgen/api/autograd.py:    # TODO: only to keep it byte-for-byte compatible with the old codegen, should remove.\n./.venv/lib/python3.13/site-packages/torchgen/api/autograd.py:#   TODO: some cpp naming logic (e.g. resolving name conflict) might be irrelevant?\n./.venv/lib/python3.13/site-packages/torchgen/api/autograd.py:    # TODO: only to keep it byte-for-byte compatible with the old codegen, should remove.\n./.venv/lib/python3.13/site-packages/torchgen/api/autograd.py:# TODO: Update comment below since it is out of date.\n./.venv/lib/python3.13/site-packages/torchgen/api/autograd.py:# TODO(crcrpar): Avoid hard coding \"Default\" ideally.\n./.venv/lib/python3.13/site-packages/torchgen/api/autograd.py:            # TODO(crcrpar): Avoid hard coding \"Default\" ideally.\n./.venv/lib/python3.13/site-packages/torchgen/api/python.py:        # [old codegen] TODO: remove this? doesn't rename in codegen, it's just\n./.venv/lib/python3.13/site-packages/torchgen/api/python.py:        # [old codegen] TODO: remove this? doesn't rename in codegen, it's just\n./.venv/lib/python3.13/site-packages/torchgen/api/python.py:    # TODO: maybe don't need keep scattered out fields for python signature?\n./.venv/lib/python3.13/site-packages/torchgen/api/python.py:                # TODO: shouldn't this be OptionalType[ListType[...]], since it defaults to None?\n./.venv/lib/python3.13/site-packages/torchgen/api/python.py:    # TODO: create a dedicated SelfArgument type for 'self'?\n./.venv/lib/python3.13/site-packages/torchgen/api/python.py:    # TODO: maybe create a PythonTensorOptionsArgument?\n./.venv/lib/python3.13/site-packages/torchgen/api/python.py:        # TODO: directly translate a.default to python default\n./.venv/lib/python3.13/site-packages/torchgen/api/python.py:    # [old codegen] TODO: because these aren't guaranteed to be 100% faithful\n./.venv/lib/python3.13/site-packages/torchgen/api/python.py:            # TODO: this doesn't seem right...\n./.venv/lib/python3.13/site-packages/torchgen/api/python.py:# TODO: This is to keep same byte-for-byte result as the old codegen - maybe unnecessary?\n./.venv/lib/python3.13/site-packages/torchgen/api/python.py:            # TODO: avoid this special handling?\n./.venv/lib/python3.13/site-packages/torchgen/api/python.py:            # TODO: why this needs to be special case?\n./.venv/lib/python3.13/site-packages/torchgen/api/python.py:            # TODO: make this part of something more general, or get rid of it.\n./.venv/lib/python3.13/site-packages/torchgen/api/python.py:    # TODO: maybe move to the generator side as it's not related to binding.\n./.venv/lib/python3.13/site-packages/torchgen/api/structured.py:        # TODO: delete these special cases; see torchgen.api.cpp--these\n./.venv/lib/python3.13/site-packages/torchgen/api/translate.py:        # TODO: My kingdom for a pattern matcher\n./.venv/lib/python3.13/site-packages/torchgen/api/translate.py:        # TODO: This could get us in recomputation trouble if b.expr is nontrivial.\n./.venv/lib/python3.13/site-packages/torchgen/api/translate.py:        # TODO: These are referentially equal, shouldn't have to do this;\n./.venv/lib/python3.13/site-packages/torchgen/api/translate.py:            # TODO: You might also want to solve this from longSymVec_ctype or\n./.venv/lib/python3.13/site-packages/torchgen/api/cpp.py:                )  # TODO: fix this discrepancy\n./.venv/lib/python3.13/site-packages/torchgen/api/cpp.py:        # TODO: remove these special cases, ArrayRef fallthrough works fine\n./.venv/lib/python3.13/site-packages/torchgen/api/cpp.py:        # TODO: Consider incorporating this into the data model\n./.venv/lib/python3.13/site-packages/torchgen/api/cpp.py:                default = \"at::kLong\"  # TODO: this is wrong\n./.venv/lib/python3.13/site-packages/torchgen/api/native.py:    # TODO: delete this!\n./.venv/lib/python3.13/site-packages/torchgen/api/native.py:        # TODO: Not sure why the arguments assigned here are for\n./.venv/lib/python3.13/site-packages/torchgen/api/types/types_base.py:    # TODO: maybe don't represent default here\n./.venv/lib/python3.13/site-packages/torchgen/api/lazy.py:            raise AssertionError(f\"TODO add support for type {repr(typ)}\")\n./.venv/lib/python3.13/site-packages/torchgen/api/lazy.py:            # TODO(whc) is this actually correct? or should it use a Vector like above\n./.venv/lib/python3.13/site-packages/torchgen/api/lazy.py:            # TODO: return a value type.  The problem here is analogous to\n./.venv/lib/python3.13/site-packages/torchgen/api/lazy.py:# TODO: Determining this based off of CType is bad; this should be computed\n./.venv/lib/python3.13/site-packages/torchgen/api/lazy.py:        # TODO: report True for this\n./.venv/lib/python3.13/site-packages/torchgen/api/lazy.py:# TODO: dedupe with Type.is_generator_like\n./.venv/lib/python3.13/site-packages/torchgen/api/lazy.py:    # TODO: this is lies, it is false for symint list\n./.venv/lib/python3.13/site-packages/torchgen/api/lazy.py:            # TODO: lists of symints are not currently treated as value types\n./.venv/lib/python3.13/site-packages/torchgen/api/lazy.py:# TODO: This is not idiomatic with how other torchgen APIs transform on schema.\n./.venv/lib/python3.13/site-packages/torchgen/api/lazy.py:    # TODO: Need to handle collisions with argument names at some point\n./.venv/lib/python3.13/site-packages/torchgen/gen_lazy_tensor.py:    # TODO(whc) add a check for shape inference functions that have meta kernels implement and should be retired.\n./.venv/lib/python3.13/site-packages/torchgen/gen_lazy_tensor.py:        TODO(alanwaketan): Remove this sorting hack once all ops are grouped properly.\n./.venv/lib/python3.13/site-packages/torchgen/operator_versions/gen_mobile_upgraders.py:        # TODO: remove the skip after these two operators schemas are fixed\n./.venv/lib/python3.13/site-packages/torchgen/operator_versions/gen_mobile_upgraders.py:            # TODO: remove the skip after these two operators schemas are fixed\n./.venv/lib/python3.13/site-packages/torchgen/gen_aoti_c_shim.py:    BaseTy.float: \"double\",  # TODO: how about other floating point types?\n./.venv/lib/python3.13/site-packages/torchgen/gen_aoti_c_shim.py:            # TODO: BaseTy.Dimname, etc.\n./.venv/lib/python3.13/site-packages/torchgen/gen_aoti_c_shim.py:            raise NotImplementedError(f\"TODO: add support for arg type {repr(typ)}\")\n./.venv/lib/python3.13/site-packages/torchgen/gen_aoti_c_shim.py:                f\"TODO: add support for return type {repr(ret.type)}\"\n./.venv/lib/python3.13/site-packages/torchgen/gen_functionalization_type.py:        # TODO: The below ops all have \"problematic\" schemas that prevent them from\n./.venv/lib/python3.13/site-packages/torchgen/dest/register_dispatch_key.py:      // TODO: avoid the redispatch here\n./.venv/lib/python3.13/site-packages/torchgen/dest/register_dispatch_key.py:            # TODO: dedupe this with the structured codegen\n./.venv/lib/python3.13/site-packages/torchgen/dest/register_dispatch_key.py:                    # TODO: handle in place on tensor list\n./.venv/lib/python3.13/site-packages/torchgen/dest/register_dispatch_key.py:            # TODO: Make sure out argument is guaranteed to be self\n./.venv/lib/python3.13/site-packages/torchgen/dest/register_dispatch_key.py:            # TODO: Move to OptionalMPSGuard.\n./.venv/lib/python3.13/site-packages/torchgen/dest/register_dispatch_key.py:            f\"      return {output_value};\\n\",  # type: ignore[possibly-undefined]  # TODO: audit\n./.venv/lib/python3.13/site-packages/torchgen/dest/register_dispatch_key.py:            # type: ignore[possibly-undefined]  # TODO: audit\n./.venv/lib/python3.13/site-packages/torchgen/dest/register_dispatch_key.py:            f\"{textwrap.indent(proxy_field, indent)}\",  # type: ignore[possibly-undefined]  # TODO: audit\n./.venv/lib/python3.13/site-packages/torchgen/dest/register_dispatch_key.py:        # TODO: Now, there is something interesting going on here.  In the code below,\n./.venv/lib/python3.13/site-packages/torchgen/dest/register_dispatch_key.py:        # (e.g., at::cpu::add).  We don't generate methods (TODO: do this\n./.venv/lib/python3.13/site-packages/torchgen/dest/register_dispatch_key.py:                # TODO: dedup this branch\n./.venv/lib/python3.13/site-packages/torchgen/dest/register_dispatch_key.py:                        # TODO: Stop hardcoding that the output type is a Tensor.  Note\n./.venv/lib/python3.13/site-packages/torchgen/dest/register_dispatch_key.py:                # TODO: https://github.com/pytorch/pytorch/issues/53023\n./.venv/lib/python3.13/site-packages/torchgen/dest/register_dispatch_key.py:                # TODO: I think this means structured won't work with method\n./.venv/lib/python3.13/site-packages/torchgen/dest/register_dispatch_key.py:            # TODO: Do this in translate instead\n./.venv/lib/python3.13/site-packages/torchgen/dest/register_dispatch_key.py:            sig_body.append(f\"return {ret_expr};\")  # type: ignore[possibly-undefined]  # TODO: audit\n./.venv/lib/python3.13/site-packages/torchgen/dest/ufunc.py:# TODO: use BackendIndex\n./.venv/lib/python3.13/site-packages/torchgen/dest/ufunc.py:        # TODO: don't hardcode; return type will be inferred based on tags on\n./.venv/lib/python3.13/site-packages/torchgen/dest/ufunc.py:        # TODO: don't hardcode ufunc:: namespace here, should be centralized smh\n./.venv/lib/python3.13/site-packages/torchgen/dest/lazy_ir.py:    # TODO: Matching on CType seems wrong; should be matching on Type\n./.venv/lib/python3.13/site-packages/torchgen/dest/lazy_ir.py:                # TODO: I don't understand when you should put lazy_ in the name\n./.venv/lib/python3.13/site-packages/torchgen/dest/lazy_ir.py:                f\"TODO not sure if there are other valid types to handle here ({arg.lazy_type})\"\n./.venv/lib/python3.13/site-packages/torchgen/dest/lazy_ir.py:                # TODO(alanwaketan): Maybe we want to apply GetLtcTensorOrCreateForWrappedNumber here, but hold it\n./.venv/lib/python3.13/site-packages/torchgen/dest/lazy_ir.py:                    f\"TODO not sure if there are other valid types to handle here ({arg.lazy_type})\"\n./.venv/lib/python3.13/site-packages/torchgen/dest/lazy_ir.py:            # TODO: this is trolling\n./.venv/lib/python3.13/site-packages/torchgen/dest/lazy_ir.py:            # TODO(whc) remove this if XLA switches to using static method for creation\n./.venv/lib/python3.13/site-packages/torchgen/static_runtime/generator.py:        # TODO: these ones got added recently and need manual inspection\n./.venv/lib/python3.13/site-packages/torchgen/static_runtime/generator.py:        # TODO: stop doing type tests by converting to C++ and then testing\n./.venv/lib/python3.13/site-packages/torchgen/static_runtime/generator.py:    # TODO: stop type testing by converting to C++\n./.venv/lib/python3.13/site-packages/torchgen/model.py:    # TODO: figure out what this does\n./.venv/lib/python3.13/site-packages/torchgen/model.py:            # TODO: verify that the tag is valid and has an entry in tags.yaml\n./.venv/lib/python3.13/site-packages/torchgen/model.py:                # TODO: maybe it's better to test the return\n./.venv/lib/python3.13/site-packages/torchgen/model.py:        # TODO: probably better to accumulate these errors and report them all\n./.venv/lib/python3.13/site-packages/torchgen/model.py:            # TODO: This discrepancy isn't required; we could also generated\n./.venv/lib/python3.13/site-packages/torchgen/model.py:    # TODO: Need to handle collisions with argument names at some point\n./.venv/lib/python3.13/site-packages/torchgen/model.py:        # TODO: implement a proper parser if this gets more ugly\n./.venv/lib/python3.13/site-packages/torchgen/model.py:        # TODO: deduplicate annotation matching with Return\n./.venv/lib/python3.13/site-packages/torchgen/model.py:        # TODO: Use a real parser here; this will get bamboozled\n./.venv/lib/python3.13/site-packages/torchgen/model.py:        # TODO: These invariants are weirdly asymmetric?\n./.venv/lib/python3.13/site-packages/torchgen/model.py:        # TODO: Fancier types?\n./.venv/lib/python3.13/site-packages/torchgen/gen.py:        # TODO: for ops with structured_delegate it should check the dispatch table of\n./.venv/lib/python3.13/site-packages/torchgen/gen.py:# TODO: This was historically used to help some JIT interop code\n./.venv/lib/python3.13/site-packages/torchgen/gen.py:# TODO: Get rid of dynamic_type, after getting tools/autograd\n./.venv/lib/python3.13/site-packages/torchgen/gen.py:        # TODO: What exactly is the semantics of the 'dispatch' field?\n./.venv/lib/python3.13/site-packages/torchgen/gen.py:    # TODO: how come ValuesView isn't a Sequence lol\n./.venv/lib/python3.13/site-packages/torchgen/gen.py:                    # TODO: this condition is a bit questionable\n./.venv/lib/python3.13/site-packages/torchgen/gen.py:    # TODO: --op-registration-whitelist will be removed when all call-sites\n./.venv/lib/python3.13/site-packages/torchgen/gen.py:    # TODO: stop generating CUDA kernels for non-CUDA builds\n./.venv/lib/python3.13/site-packages/tokenizers/tools/visualizer.py:                # TODO is this the right name for the data attribute ?\n./.venv/lib/python3.13/site-packages/tokenizers/tools/visualizer.py:        # TODO I think there is an edge case here where an annotation's span might not close\n./.venv/lib/python3.13/site-packages/networkx/readwrite/json_graph/node_link.py:    # TODO: Remove between the lines when `link` deprecation expires\n./.venv/lib/python3.13/site-packages/networkx/readwrite/json_graph/node_link.py:    # TODO: Remove between the lines when `link` deprecation expires\n./.venv/lib/python3.13/site-packages/networkx/readwrite/json_graph/tests/test_node_link.py:    # TODO: To be removed when signature change complete\n./.venv/lib/python3.13/site-packages/networkx/generators/geometric.py:    # TODO Is this function just a special case of the geographical\n./.venv/lib/python3.13/site-packages/networkx/generators/tests/test_expanders.py:    # TODO The second largest eigenvalue should be smaller than a constant,\n./.venv/lib/python3.13/site-packages/networkx/generators/community.py:    # TODO The original code incremented the number of iterations each\n./.venv/lib/python3.13/site-packages/networkx/generators/degree_seq.py:    # TODO Does this need to be sorted in reverse order?\n./.venv/lib/python3.13/site-packages/networkx/algorithms/centrality/reaching.py:    # TODO This can be trivially parallelized.\n./.venv/lib/python3.13/site-packages/networkx/algorithms/centrality/reaching.py:    # TODO This can be trivially parallelized.\n./.venv/lib/python3.13/site-packages/networkx/algorithms/similarity.py:    # TODO: support DiGraph\n./.venv/lib/python3.13/site-packages/networkx/algorithms/traversal/beamsearch.py:        # TODO The Python documentation states that for small values, it\n./.venv/lib/python3.13/site-packages/networkx/algorithms/assortativity/tests/test_connectivity.py:        # TODO Is this really the intended behavior for providing a\n./.venv/lib/python3.13/site-packages/networkx/algorithms/connectivity/edge_kcomponents.py:            # TODO: investigate https://arxiv.org/abs/1412.6466 for k=2\n./.venv/lib/python3.13/site-packages/networkx/algorithms/connectivity/edge_kcomponents.py:    # @not_implemented_for('multigraph')  # TODO: fix decor for classmethods\n./.venv/lib/python3.13/site-packages/networkx/algorithms/bipartite/redundancy.py:    # TODO This can be trivially parallelized.\n./.venv/lib/python3.13/site-packages/networkx/algorithms/bipartite/tests/test_matching.py:        # TODO Assert that the vertices are the correct ones.\n./.venv/lib/python3.13/site-packages/networkx/algorithms/bipartite/matching.py:            # TODO - The lines between --- were unused and were thus commented\n./.venv/lib/python3.13/site-packages/networkx/algorithms/bipartite/matching.py:            #     # TODO Why is extra inner loop necessary?\n./.venv/lib/python3.13/site-packages/networkx/algorithms/bipartite/matching.py:            # TODO Originally, this function returned a three-tuple:\n./.venv/lib/python3.13/site-packages/networkx/algorithms/link_analysis/pagerank_alg.py:    # TODO: csr_array\n./.venv/lib/python3.13/site-packages/networkx/algorithms/coloring/equitable_coloring.py:        # TODO: Checking whether a color has been visited can be made faster by\n./.venv/lib/python3.13/site-packages/networkx/algorithms/isomorphism/ismags.py:        # TODO: graph and subgraph setter methods that invalidate the caches.\n./.venv/lib/python3.13/site-packages/networkx/algorithms/isomorphism/ismags.py:        # TODO: allow for precomputed partitions and colors\n./.venv/lib/python3.13/site-packages/networkx/algorithms/isomorphism/isomorphvf2.py:        # TODO:\n./.venv/lib/python3.13/site-packages/networkx/algorithms/approximation/dominating_set.py:# TODO Why doesn't this algorithm work for directed graphs?\n./.venv/lib/python3.13/site-packages/networkx/algorithms/approximation/traveling_salesman.py:            # TODO: this branch does not restore original_edge_weights of G!\n./.venv/lib/python3.13/site-packages/networkx/algorithms/shortest_paths/unweighted.py:    # TODO This can be trivially parallelized.\n./.venv/lib/python3.13/site-packages/networkx/algorithms/shortest_paths/unweighted.py:    # TODO This can be trivially parallelized.\n./.venv/lib/python3.13/site-packages/networkx/algorithms/shortest_paths/weighted.py:    # TODO This can be trivially parallelized.\n./.venv/lib/python3.13/site-packages/networkx/algorithms/node_classification.py:    # TODO: csr_array\n./.venv/lib/python3.13/site-packages/networkx/algorithms/node_classification.py:    # TODO: csr_array\n./.venv/lib/python3.13/site-packages/networkx/algorithms/cuts.py:# TODO STILL NEED TO UPDATE ALL THE DOCUMENTATION!\n./.venv/lib/python3.13/site-packages/networkx/algorithms/cuts.py:# TODO What is the generalization to two arguments, S and T? Does the\n./.venv/lib/python3.13/site-packages/networkx/algorithms/cuts.py:# TODO What is the generalization to two arguments, S and T? Does the\n./.venv/lib/python3.13/site-packages/networkx/algorithms/cycles.py:                if thisnode not in B[nextnode]:  # TODO: use set for speedup?\n./.venv/lib/python3.13/site-packages/networkx/algorithms/tree/branchings.py:# TODO: Implement method from Gabow, Galil, Spence and Tarjan:\n./.venv/lib/python3.13/site-packages/networkx/algorithms/tree/mst.py:        # TODO This can be parallelized, both in the outer loop over\n./.venv/lib/python3.13/site-packages/networkx/algorithms/tree/mst.py:        # TODO This loop can be parallelized, to an extent (the union\n./.venv/lib/python3.13/site-packages/networkx/algorithms/efficiency_measures.py:    # TODO This can be made more efficient by computing all pairs shortest\n./.venv/lib/python3.13/site-packages/networkx/algorithms/tests/test_swap.py:    # TODO: Rewrite function to explicitly check for impossible swaps and raise error\n./.venv/lib/python3.13/site-packages/networkx/algorithms/walks.py:    # TODO: Use matrix_power from scipy.sparse when available\n./.venv/lib/python3.13/site-packages/networkx/algorithms/dag.py:    # TODO In Python 3, this would be better as `yield from ...`.\n./.venv/lib/python3.13/site-packages/networkx/algorithms/clique.py:# TODO Should this also be not implemented for directed graphs?\n./.venv/lib/python3.13/site-packages/networkx/algorithms/distance_regular.py:# TODO There is a definition for directed strongly regular graphs.\n./.venv/lib/python3.13/site-packages/networkx/conftest.py:# TODO: The warnings below need to be dealt with, but for now we silence them.\n./.venv/lib/python3.13/site-packages/networkx/drawing/nx_pylab.py:        # TODO should this be list or array (as in a numpy array)?\n./.venv/lib/python3.13/site-packages/networkx/drawing/nx_latex.py:    # TODO allow pos to be None and use a nice TikZ default\n./.venv/lib/python3.13/site-packages/networkx/drawing/nx_latex.py:        # TODO -- handle bending of multiedges\n./.venv/lib/python3.13/site-packages/networkx/drawing/layout.py:            Ai = A.getrowview(i).toarray()  # TODO: revisit w/ sparse 1D container\n./.venv/lib/python3.13/site-packages/networkx/drawing/layout.py:    # TODO: Rm csr_array wrapper in favor of spdiags array constructor when available\n./.venv/lib/python3.13/site-packages/networkx/linalg/laplacianmatrix.py:    # TODO: rm csr_array wrapper when spdiags can produce arrays\n./.venv/lib/python3.13/site-packages/networkx/linalg/laplacianmatrix.py:    # TODO: rm csr_array wrapper when spdiags can produce arrays\n./.venv/lib/python3.13/site-packages/networkx/linalg/laplacianmatrix.py:    # TODO: rm csr_array wrapper when spdiags can produce arrays\n./.venv/lib/python3.13/site-packages/networkx/linalg/laplacianmatrix.py:        # TODO: rm csr_array wrapper when spdiags creates arrays\n./.venv/lib/python3.13/site-packages/networkx/linalg/laplacianmatrix.py:        # TODO: rm csr_array wrapper when spdiags creates arrays\n./.venv/lib/python3.13/site-packages/networkx/linalg/laplacianmatrix.py:    # TODO: Rm csr_array wrapper when spdiags array creation becomes available\n./.venv/lib/python3.13/site-packages/networkx/linalg/laplacianmatrix.py:        # TODO: Rm csr_array wrapper when spdiags array creation becomes available\n./.venv/lib/python3.13/site-packages/networkx/linalg/laplacianmatrix.py:            # TODO: Rm csr_array wrapper when identity array creation becomes available\n./.venv/lib/python3.13/site-packages/networkx/linalg/algebraicconnectivity.py:        # TODO: rm csr_array wrapper when spdiags array creation becomes available\n./.venv/lib/python3.13/site-packages/networkx/linalg/algebraicconnectivity.py:                # TODO: rm csc_array wrapping when spdiags array becomes available\n./.venv/lib/python3.13/site-packages/networkx/linalg/algebraicconnectivity.py:                # TODO: rm csr_array wrapping when spdiags array becomes available\n./.venv/lib/python3.13/site-packages/networkx/linalg/bethehessianmatrix.py:    # TODO: Rm csr_array wrapper when spdiags array creation becomes available\n./.venv/lib/python3.13/site-packages/networkx/linalg/bethehessianmatrix.py:    # TODO: Rm csr_array wrapper when eye array creation becomes available\n./.venv/lib/python3.13/site-packages/pyasn1/type/univ.py:        # TODO: remove when Py2.5 support is gone\n./.venv/lib/python3.13/site-packages/pyasn1/type/univ.py:                # TODO: we should wrap componentType with UnnamedType to carry\n./.venv/lib/python3.13/site-packages/pyasn1/type/constraint.py:        # TODO: fix possible comparison of set vs scalars here\n./.venv/lib/python3.13/site-packages/pyasn1/type/constraint.py:# TODO:\n./.venv/lib/python3.13/site-packages/pyasn1/codec/der/decoder.py:# TODO: prohibit non-canonical encoding\n./.venv/lib/python3.13/site-packages/pyasn1/codec/der/encoder.py:                # TODO: move out of sorting key function\n./.venv/lib/python3.13/site-packages/pyasn1/codec/der/encoder.py:                # TODO: support nested CHOICE ordering\n./.venv/lib/python3.13/site-packages/pyasn1/codec/cer/decoder.py:# TODO: prohibit non-canonical encoding\n./.venv/lib/python3.13/site-packages/pyasn1/codec/ber/decoder.py:        raise error.PyAsn1Error('SingleItemDecoder not implemented for %s' % (tagSet,))  # TODO: Seems more like an NotImplementedError?\n./.venv/lib/python3.13/site-packages/pyasn1/codec/ber/decoder.py:        raise error.PyAsn1Error('Indefinite length mode decoder not implemented for %s' % (tagSet,)) # TODO: Seems more like an NotImplementedError?\n./.venv/lib/python3.13/site-packages/pyasn1/codec/ber/decoder.py:            # TODO: Seems not to be tested\n./.venv/lib/python3.13/site-packages/pyasn1/codec/ber/decoder.py:            yield chunk  # TODO: Weird\n./.venv/lib/python3.13/site-packages/pyasn1/codec/ber/encoder.py:            # TODO: try to avoid ASN.1 schema instantiation\n./.venv/lib/python3.13/site-packages/pyasn1/codec/ber/encoder.py:    # TODO: handling three flavors of input is too much -- split over codecs\n./.venv/lib/python3.13/site-packages/anyio/_core/_fileio.py:        def info(self) -> Any:  # TODO: add return type annotation when Typeshed gets it\n./.venv/lib/python3.13/site-packages/uvloop/sslproto.pyx:    # TODO:\n./.venv/lib/python3.13/site-packages/uvloop/_testbase.py:        # TODO This warning has to be fixed in asyncio.\n./.venv/lib/python3.13/site-packages/uvloop/loop.pyx:        # TODO: Implement close_fds (might not be very important in\n./.venv/lib/python3.13/site-packages/uvloop/loop.pyx:                # TODO: ideally we should always raise an error here,\n./.venv/lib/python3.13/site-packages/referencing/_core.py:            # TODO: Is this true for non JSON Schema resources? Probably not.\n./.venv/lib/python3.13/site-packages/sentence_transformers/SentenceTransformer.py:# TODO: Fill in the PR number\n./.venv/lib/python3.13/site-packages/sentence_transformers/cross_encoder/evaluation/reranking.py:        always_rerank_positives: bool = True,  # TODO: This is also confusing, perhaps setting=\"\"\n./.venv/lib/python3.13/site-packages/sentence_transformers/sparse_encoder/models/MLMTransformer.py:                # TODO: Consider following these steps automatically so we can load PEFT models with other backends\n./.venv/lib/python3.13/site-packages/sentence_transformers/backend/utils.py:# TODO: Fill in the PR number\n./.venv/lib/python3.13/site-packages/sentence_transformers/backend/utils.py:# TODO: Fill in the PR number\n./.venv/lib/python3.13/site-packages/sentence_transformers/backend/utils.py:# TODO: Fill in the PR number\n./.venv/lib/python3.13/site-packages/sentence_transformers/fit_mixin.py:        # TODO: This is rather inefficient, as we load all data into memory. We might benefit from a more efficient solution\n./.venv/lib/python3.13/site-packages/sentence_transformers/models/Transformer.py:                # TODO: Consider following these steps automatically so we can load PEFT models with other backends\n./.venv/lib/python3.13/site-packages/tqdm/__init__.py:from .cli import main  # TODO: remove in v5.0.0\n./.venv/lib/python3.13/site-packages/tqdm/__init__.py:from .gui import tqdm as tqdm_gui  # TODO: remove in v5.0.0\n./.venv/lib/python3.13/site-packages/tqdm/__init__.py:from .gui import trange as tgrange  # TODO: remove in v5.0.0\n./.venv/lib/python3.13/site-packages/tqdm/utils.py:# TODO consider using wcswidth third-party package for 0-width characters\n./.venv/lib/python3.13/site-packages/tqdm/tk.py:    # TODO: @classmethod: write()?\n./.venv/lib/python3.13/site-packages/tqdm/gui.py:    # TODO: @classmethod: write() on GUI?\n./.venv/lib/python3.13/site-packages/tqdm/rich.py:    # TODO: @classmethod: write()?\n./.venv/lib/python3.13/site-packages/tqdm/cli.py:# TODO: add custom support for some of the following?\n./.venv/lib/python3.13/site-packages/tqdm/cli.py:        TODO: find out why this is needed.\n./.venv/lib/python3.13/site-packages/tqdm/std.py:        # TODO: private method\n./.venv/lib/python3.13/site-packages/pydantic_core/core_schema.py:            TODO: use of a tzinfo where offset changes based on the datetime is not yet supported\n./.venv/lib/python3.13/site-packages/pip/_internal/commands/inspect.py:            # TODO tags? scheme?\n./.venv/lib/python3.13/site-packages/pip/_internal/metadata/base.py:from pip._internal.utils.compat import stdlib_pkgs  # TODO: Move definition here.\n./.venv/lib/python3.13/site-packages/pip/_internal/metadata/base.py:        # TODO: this property is relatively costly to compute, memoize it ?\n./.venv/lib/python3.13/site-packages/pip/_internal/metadata/base.py:                # TODO: get project location from second line of egg_link file\n./.venv/lib/python3.13/site-packages/pip/_internal/operations/prepare.py:        # TODO: separate this part out from RequirementPreparer when the v1\n./.venv/lib/python3.13/site-packages/pip/_internal/network/lazy_wheel.py:        # TODO: Get range requests to be correctly cached\n./.venv/lib/python3.13/site-packages/pip/_internal/cli/base_command.py:        # TODO: Try to get these passing down from the command?\n./.venv/lib/python3.13/site-packages/pip/_internal/req/constructors.py:        # TODO: The is_installable_dir test here might not be necessary\n./.venv/lib/python3.13/site-packages/pip/_internal/req/req_file.py:    # TODO: replace this with slots=True when dropping Python 3.9 support.\n./.venv/lib/python3.13/site-packages/pip/_internal/req/req_file.py:    # TODO: handle space after '\\'.\n./.venv/lib/python3.13/site-packages/pip/_internal/req/req_set.py:        TODO remove this property together with the legacy resolver, since the new\n./.venv/lib/python3.13/site-packages/pip/_internal/cache.py:                # TODO: use DirectUrl.equivalent when\n./.venv/lib/python3.13/site-packages/pip/_internal/index/collector.py:        # TODO: In the future, it would be nice if pip supported PEP 691\n./.venv/lib/python3.13/site-packages/pip/_internal/resolution/resolvelib/factory.py:        # TODO: Check already installed candidate, and use it if the link and\n./.venv/lib/python3.13/site-packages/pip/_internal/resolution/resolvelib/factory.py:        # TODO: Are there more cases this needs to return True? Editable?\n./.venv/lib/python3.13/site-packages/pip/_internal/resolution/resolvelib/candidates.py:        # TODO performance: this means we iterate the dependencies at least twice,\n./.venv/lib/python3.13/site-packages/pip/_internal/resolution/resolvelib/candidates.py:        # TODO: Supply reason based on force_reinstall and upgrade_strategy.\n./.venv/lib/python3.13/site-packages/pip/_internal/models/selection_prefs.py:# TODO: This needs Python 3.10's improved slots support for dataclasses\n./.venv/lib/python3.13/site-packages/pip/_internal/models/installation_report.py:            # TODO: currently, the resolver uses the default environment to evaluate\n./.venv/lib/python3.13/site-packages/pip/_vendor/truststore/_macos.py:            # TODO: Not sure if we need the SecTrustResultType for anything?\n./.venv/lib/python3.13/site-packages/pip/_vendor/pygments/filters/__init__.py:       highlight ``XXX``, ``TODO``, ``FIXME``, ``BUG`` and ``NOTE``.\n./.venv/lib/python3.13/site-packages/pip/_vendor/pygments/filters/__init__.py:                            ['XXX', 'TODO', 'FIXME', 'BUG', 'NOTE'])\n./.venv/lib/python3.13/site-packages/pip/_vendor/pygments/lexers/python.py:        # different tokens.  TODO: DelegatingLexer should support this\n./.venv/lib/python3.13/site-packages/pip/_vendor/pygments/lexer.py:    TODO: clean up the code here.\n./.venv/lib/python3.13/site-packages/pip/_vendor/typing_extensions.py:                # TODO: Use inspect.VALUE here, and make the annotations lazily evaluated\n./.venv/lib/python3.13/site-packages/pip/_vendor/typing_extensions.py:                # TODO: Use inspect.VALUE here, and make the annotations lazily evaluated\n./.venv/lib/python3.13/site-packages/pip/_vendor/packaging/metadata.py:        # TODO: The spec doesn't say anything about if the keys should be\n./.venv/lib/python3.13/site-packages/pip/_vendor/packaging/metadata.py:    description: _Validator[str | None] = _Validator()  # TODO 2.1: can be in body\n./.venv/lib/python3.13/site-packages/pip/_vendor/packaging/tags.py:        # TODO: Need to care about 32-bit PPC for ppc64 through 10.2?\n./.venv/lib/python3.13/site-packages/pip/_vendor/packaging/requirements.py:    # TODO: Can we test whether something is contained within a requirement?\n./.venv/lib/python3.13/site-packages/pip/_vendor/packaging/requirements.py:    # TODO: Can we normalize the name and extra name?\n./.venv/lib/python3.13/site-packages/pip/_vendor/cachecontrol/controller.py:        # TODO: There is an assumption that the result will be a\n./.venv/lib/python3.13/site-packages/pip/_vendor/cachecontrol/filewrapper.py:        # TODO: Add some logging here...\n./.venv/lib/python3.13/site-packages/pip/_vendor/pkg_resources/__init__.py:# TODO: Add Generic type annotations to initialized collections.\n./.venv/lib/python3.13/site-packages/pip/_vendor/pkg_resources/__init__.py:_ResourceStream = Any  # TODO / Incomplete: A readable file-like object\n./.venv/lib/python3.13/site-packages/pip/_vendor/pkg_resources/__init__.py:            # TODO: remove this except clause when python/cpython#103632 is fixed.\n./.venv/lib/python3.13/site-packages/pip/_vendor/pkg_resources/__init__.py:        # TODO: Add a deadline?\n./.venv/lib/python3.13/site-packages/pip/_vendor/requests/adapters.py:                # TODO: Remove this in 3.0.0: see #2811\n./.venv/lib/python3.13/site-packages/pip/_vendor/requests/hooks.py:# TODO: response is the only one\n./.venv/lib/python3.13/site-packages/pip/_vendor/distlib/wheel.py:            # TODO version verification\n./.venv/lib/python3.13/site-packages/pip/_vendor/distlib/util.py:        # TODO check k, v for valid values\n./.venv/lib/python3.13/site-packages/pip/_vendor/distlib/metadata.py:    # TODO document the mapping API and UNKNOWN default key\n./.venv/lib/python3.13/site-packages/pip/_vendor/distlib/metadata.py:    # TODO could add iter* variants\n./.venv/lib/python3.13/site-packages/pip/_vendor/distlib/metadata.py:        # TODO: any other fields wanted\n./.venv/lib/python3.13/site-packages/pip/_vendor/distlib/version.py:        TODO: fill this out\n./.venv/lib/python3.13/site-packages/pip/_vendor/distlib/version.py:    # TODO: unintended side-effect on, e.g., \"2003.05.09\"\n./.venv/lib/python3.13/site-packages/pip/_vendor/distlib/locators.py:        XXX TODO Note: this cache is never actually cleared. It's assumed that\n./.venv/lib/python3.13/site-packages/pip/_vendor/distlib/locators.py:                # TODO SHA256 digest\n./.venv/lib/python3.13/site-packages/pip/_vendor/msgpack/fallback.py:        # TODO should we eliminate the recursion?\n./.venv/lib/python3.13/site-packages/pip/_vendor/msgpack/fallback.py:                    # TODO check whether we need to call `list_hook`\n./.venv/lib/python3.13/site-packages/pip/_vendor/msgpack/fallback.py:            # TODO is the interaction between `list_hook` and `use_list` ok?\n./.venv/lib/python3.13/site-packages/pip/_vendor/msgpack/fallback.py:                    # TODO check whether we need to call hooks\n./.venv/lib/python3.13/site-packages/pip/_vendor/rich/text.py:        # TODO: This is a little inefficient, it is only used by full justify\n./.venv/lib/python3.13/site-packages/pip/_vendor/urllib3/util/retry.py:# TODO: In v2 we can remove this sentinel and metaclass with deprecated options.\n./.venv/lib/python3.13/site-packages/pip/_vendor/urllib3/util/retry.py:        # TODO: Deprecated, remove in v2.0\n./.venv/lib/python3.13/site-packages/pip/_vendor/urllib3/util/retry.py:        # TODO: If already given in **kw we use what's given to us\n./.venv/lib/python3.13/site-packages/pip/_vendor/urllib3/util/retry.py:        # TODO: For now favor if the Retry implementation sets its own method_whitelist\n./.venv/lib/python3.13/site-packages/pip/_vendor/urllib3/util/retry.py:            # TODO: Remove this deprecated alias in v2.0\n./.venv/lib/python3.13/site-packages/pip/_vendor/urllib3/util/url.py:    # TODO: Remove this when we break backwards compatibility.\n./.venv/lib/python3.13/site-packages/pip/_vendor/urllib3/exceptions.py:    # TODO(t-8ch): Stop inheriting from AssertionError in v2.0.\n./.venv/lib/python3.13/site-packages/pip/_vendor/urllib3/connectionpool.py:        # TODO: Add optional support for socket.gethostbyname checking.\n./.venv/lib/python3.13/site-packages/pip/_vendor/urllib3/connection.py:            # TODO: Fix tunnel so it doesn't depend on self.sock state.\n./.venv/lib/python3.13/site-packages/pip/_vendor/urllib3/contrib/securetransport.py:        # TODO: should I do clean shutdown here? Do I have to?\n./.venv/lib/python3.13/site-packages/pip/_vendor/urllib3/contrib/securetransport.py:        # TODO: Well, crap.\n./.venv/lib/python3.13/site-packages/pip/_vendor/urllib3/contrib/securetransport.py:        # TODO: Update in line with above.\n./.venv/lib/python3.13/site-packages/PIL/PdfParser.py:            # TODO: support reuse of deleted objects\n./.venv/lib/python3.13/site-packages/PIL/TiffImagePlugin.py:                    raise RuntimeError(msg)  # XXX TODO\n./.venv/lib/python3.13/site-packages/PIL/IcoImagePlugin.py:            # TODO: invent a more convenient method for proportional scalings\n./.venv/lib/python3.13/site-packages/pyflakes/checker.py:                # TODO: if the global is not used in this scope, it does not\n./.venv/lib/python3.13/site-packages/typer/_completion_shared.py:# TODO: Probably refactor this, copied from Click 7.x\n./.venv/lib/python3.13/site-packages/typer/_completion_shared.py:    # TODO: consider setting the style only for the current program\n./.venv/lib/python3.13/site-packages/typer/models.py:        # TODO: Remove shell_complete in a future version (after 0.16.0)\n./.venv/lib/python3.13/site-packages/typer/models.py:        # TODO: Remove shell_complete in a future version (after 0.16.0)\n./.venv/lib/python3.13/site-packages/typer/models.py:        # TODO: remove is_flag and flag_value in a future release\n./.venv/lib/python3.13/site-packages/typer/models.py:        # TODO: Remove shell_complete in a future version (after 0.16.0)\n./.venv/lib/python3.13/site-packages/typer/core.py:        # TODO: Remove shell_complete in a future version (after 0.16.0)\n./.venv/lib/python3.13/site-packages/typer/core.py:        # TODO: When deprecating Click < 8.2, remove this\n./.venv/lib/python3.13/site-packages/typer/core.py:        # TODO: /When deprecating Click < 8.2, remove this, uncomment the line below\n./.venv/lib/python3.13/site-packages/typer/core.py:        # TODO: Remove shell_complete in a future version (after 0.16.0)\n./.venv/lib/python3.13/site-packages/typer/params.py:    # TODO: Remove shell_complete in a future version (after 0.16.0)\n./.venv/lib/python3.13/site-packages/typer/params.py:    # TODO: remove is_flag and flag_value in a future release\n./.venv/lib/python3.13/site-packages/typer/params.py:    # TODO: Remove shell_complete in a future version (after 0.16.0)\n./.venv/lib/python3.13/site-packages/typer/params.py:    # TODO: remove is_flag and flag_value in a future release\n./.venv/lib/python3.13/site-packages/typer/params.py:    # TODO: Remove shell_complete in a future version (after 0.16.0)\n./.venv/lib/python3.13/site-packages/typer/params.py:    # TODO: remove is_flag and flag_value in a future release\n./.venv/lib/python3.13/site-packages/typer/params.py:    # TODO: Remove shell_complete in a future version (after 0.16.0)\n./.venv/lib/python3.13/site-packages/typer/params.py:    # TODO: Remove shell_complete in a future version (after 0.16.0)\n./.venv/lib/python3.13/site-packages/typer/params.py:    # TODO: Remove shell_complete in a future version (after 0.16.0)\n./.venv/lib/python3.13/site-packages/typer/_completion_classes.py:    # TODO: when removing support for Click < 8.2, remove this import\n./.venv/lib/python3.13/site-packages/typer/_completion_classes.py:        # TODO: Explore replicating the new behavior from Click, with item types and\n./.venv/lib/python3.13/site-packages/typer/_completion_classes.py:        # TODO: Explore replicating the new behavior from Click, pay attention to\n./.venv/lib/python3.13/site-packages/typer/_completion_classes.py:        # TODO: Explore replicating the new behavior from Click, pay attention to\n./.venv/lib/python3.13/site-packages/typer/rich_utils.py:        # TODO: when deprecating Click < 8.2, make ctx required\n./.venv/lib/python3.13/site-packages/uvicorn/protocols/websockets/wsproto_impl.py:            # TODO: Remove `type: ignore` when wsproto fixes the type annotation.\n./.venv/lib/python3.13/site-packages/fsspec/utils.py:    # TODO: allow length to be None and read to the end of the file?\n./.venv/lib/python3.13/site-packages/fsspec/implementations/local.py:    # TODO: if all incoming paths were posix-compliant then separator would\n./.venv/lib/python3.13/site-packages/fsspec/implementations/local.py:                # TODO: check if path is writable?\n./.venv/lib/python3.13/site-packages/fsspec/implementations/reference.py:        # TODO: derive fs from `root`\n./.venv/lib/python3.13/site-packages/fsspec/implementations/reference.py:        # TODO: only save needed columns\n./.venv/lib/python3.13/site-packages/fsspec/implementations/reference.py:        # TODO: only clear those that we wrote to?\n./.venv/lib/python3.13/site-packages/fsspec/implementations/reference.py:            # TODO: warning here, since this can be very expensive?\n./.venv/lib/python3.13/site-packages/fsspec/implementations/reference.py:        # TODO: if references is lazy, pre-fetch all paths in batch before access\n./.venv/lib/python3.13/site-packages/fsspec/implementations/reference.py:        # TODO: we make dircache by iterating over all entries, but for Spec >= 1,\n./.venv/lib/python3.13/site-packages/fsspec/implementations/http.py:        # TODO: Maybe rename `self.kwargs` to `self.request_options` to make\n./.venv/lib/python3.13/site-packages/fsspec/implementations/cached.py:            # TODO: action where partial file exists in read-only cache\n./.venv/lib/python3.13/site-packages/fsspec/implementations/http_sync.py:        # TODO: encoding from headers\n./.venv/lib/python3.13/site-packages/fsspec/implementations/http_sync.py:    # TODO: not allowed in JS\n./.venv/lib/python3.13/site-packages/fsspec/implementations/http_sync.py:    # TODO:\n./.venv/lib/python3.13/site-packages/fsspec/implementations/smb.py:        # TODO: use transaction support in SMB protocol\n./.venv/lib/python3.13/site-packages/fsspec/implementations/tar.py:            # TODO: tarfile already implements compression with modes like \"'r:gz'\",\n./.venv/lib/python3.13/site-packages/fsspec/implementations/tar.py:        # TODO: load and set saved index, if exists\n./.venv/lib/python3.13/site-packages/fsspec/implementations/tar.py:        # TODO: save index to self.index_store here, if set\n./.venv/lib/python3.13/site-packages/fsspec/implementations/cache_metadata.py:                # TODO: consolidate blocks here\n./.venv/lib/python3.13/site-packages/fsspec/asyn.py:        # TODO: implement on_error\n./.venv/lib/python3.13/site-packages/fsspec/asyn.py:        # TODO: on_error\n./.venv/lib/python3.13/site-packages/fsspec/asyn.py:    # TODO: readahead might still be useful here, but needs async version\n./.venv/lib/python3.13/site-packages/fsspec/compression.py:# TODO: files should also be available as contexts\n./.venv/lib/python3.13/site-packages/fsspec/spec.py:        # TODO: allow equivalent of -name parameter\n./.venv/lib/python3.13/site-packages/fsspec/caching.py:        # TODO: use rich for better formatting\n./.venv/lib/python3.13/site-packages/fsspec/caching.py:        # TODO: only set start/end after fetch, in case it fails?\n./.venv/lib/python3.13/site-packages/fsspec/generic.py:        # TODO: special case for one FS being local, which can use get/put\n./.venv/lib/python3.13/site-packages/fsspec/generic.py:        # TODO: special case for one being memFS, which can use cat/pipe\n./.venv/lib/python3.13/site-packages/torch/__init__.py:# TODO(torch_deploy) figure out how to freeze version.py in fbcode build\n./.venv/lib/python3.13/site-packages/torch/__init__.py:    # TODO: Remove this ugly hack when deploy typing extensions are updated to 4.10+\n./.venv/lib/python3.13/site-packages/torch/__init__.py:        # TODO: Remove once CUDA 11.8 binaries are deprecated\n./.venv/lib/python3.13/site-packages/torch/__init__.py:            # TODO: Force specialization\n./.venv/lib/python3.13/site-packages/torch/__init__.py:        # TODO: A more relaxed guard is possible here, where you guard to\n./.venv/lib/python3.13/site-packages/torch/__init__.py:    # TODO: Probably can make bool work too, just lazy\n./.venv/lib/python3.13/site-packages/torch/__init__.py:                # TODO: fix their module from C++ side\n./.venv/lib/python3.13/site-packages/torch/__init__.py:            # TODO: Call like get_device_index() method corresponding to\n./.venv/lib/python3.13/site-packages/torch/__init__.py:        # TODO: Once the undocumented FC window is passed, remove the line below\n./.venv/lib/python3.13/site-packages/torch/_decomp/__init__.py:# TODO: relax key type here; torch registrations should be possible to; but\n./.venv/lib/python3.13/site-packages/torch/_decomp/decompositions_for_jvp.py:# TODO: The mechanism we are using to register decompositions doesn't\n./.venv/lib/python3.13/site-packages/torch/_decomp/decompositions_for_jvp.py:# TODO: do these also belong here?\n./.venv/lib/python3.13/site-packages/torch/_decomp/decompositions_for_rng.py:# TODO - We have to register many more distributions here, and also higher level\n./.venv/lib/python3.13/site-packages/torch/_decomp/decompositions_for_rng.py:        # TODO: Investigate if there is be a better way to wrap the tuple in a\n./.venv/lib/python3.13/site-packages/torch/_decomp/decompositions.py:        # TODO: pretty sure this is not quite right\n./.venv/lib/python3.13/site-packages/torch/_decomp/decompositions.py:# TODO: None of these loss castings are quite correct, see\n./.venv/lib/python3.13/site-packages/torch/_decomp/decompositions.py:# TODO: this doesn't appear to have enough precision in bfloat16\n./.venv/lib/python3.13/site-packages/torch/_decomp/decompositions.py:# TODO: Take a closer look at the type promotion semantics\n./.venv/lib/python3.13/site-packages/torch/_decomp/decompositions.py:# TODO: this decomposition is NOT here to stay. We would much prefer replacing native_batch_norm\n./.venv/lib/python3.13/site-packages/torch/_decomp/decompositions.py:    assert not layout or layout == torch.strided, \"TODO\"\n./.venv/lib/python3.13/site-packages/torch/_decomp/decompositions.py:    assert not pin_memory, \"TODO\"\n./.venv/lib/python3.13/site-packages/torch/_decomp/decompositions.py:            # TODO make minimum accept scalars\n./.venv/lib/python3.13/site-packages/torch/_decomp/decompositions.py:        # TODO: handling of slice\n./.venv/lib/python3.13/site-packages/torch/_guards.py:# TODO: mark as kw_only=True once we drop support for <Python 3.10\n./.venv/lib/python3.13/site-packages/torch/_guards.py:    # TODO: consider also tracking the recompilation count\n./.venv/lib/python3.13/site-packages/torch/_guards.py:# TODO(voz): Consider a toplevel torch/_source.py\n./.venv/lib/python3.13/site-packages/torch/_numpy/_unary_ufuncs_impl.py:# TODO set __name__ and __qualname__\n./.venv/lib/python3.13/site-packages/torch/_numpy/random.py:    # TODO: check a.dtype is integer -- cf np.random.choice(3.4) which raises\n./.venv/lib/python3.13/site-packages/torch/ao/ns/fx/ns_types.py:# TODO(future PR): see if we can use typing_extensions's TypedDict instead\n./.venv/lib/python3.13/site-packages/torch/ao/ns/fx/mappings.py:# TODO(future PR): clean this up\n./.venv/lib/python3.13/site-packages/torch/ao/ns/fx/mappings.py:        # TODO(future PR): implement shadowing for binary ops and\n./.venv/lib/python3.13/site-packages/torch/ao/ns/fx/utils.py:# TODO(future PR): consider deleting this enum and using the torch types\n./.venv/lib/python3.13/site-packages/torch/ao/ns/fx/utils.py:    # TODO(future PR): while these functions can support multiple dtypes,\n./.venv/lib/python3.13/site-packages/torch/ao/ns/fx/utils.py:    # TODO(future PRs): dynamic quant, fake quant, etc\n./.venv/lib/python3.13/site-packages/torch/ao/ns/fx/utils.py:    # TODO(future PR): clean this up\n./.venv/lib/python3.13/site-packages/torch/ao/ns/fx/utils.py:        # TODO(future PR): handle more functionals\n./.venv/lib/python3.13/site-packages/torch/ao/ns/fx/utils.py:        # TODO(future PR): handle functional ops which inherit qparams from input\n./.venv/lib/python3.13/site-packages/torch/ao/ns/fx/utils.py:        # TODO(future PR): use relationship map instead of hardcoding\n./.venv/lib/python3.13/site-packages/torch/ao/ns/fx/graph_passes.py:        # TODO(future PR): determine the actual dtype of node_c,\n./.venv/lib/python3.13/site-packages/torch/ao/ns/fx/graph_passes.py:                # TODO(future PR): add handling for quantize_per_tensor\n./.venv/lib/python3.13/site-packages/torch/ao/ns/fx/graph_passes.py:# TODO(future PR): look into using copy_node API instead\n./.venv/lib/python3.13/site-packages/torch/ao/ns/fx/graph_passes.py:    TODO(before land): real docblock\n./.venv/lib/python3.13/site-packages/torch/ao/ns/fx/graph_passes.py:            # TODO(future PR): enable multiple inputs for nodes which are not at start of subgraph\n./.venv/lib/python3.13/site-packages/torch/ao/ns/fx/graph_passes.py:                    # TODO: explain this\n./.venv/lib/python3.13/site-packages/torch/ao/ns/fx/graph_matcher.py:    # TODO(next): make this code handle matching by what is before the base op\n./.venv/lib/python3.13/site-packages/torch/ao/ns/fx/graph_matcher.py:                # TODO(future PR): check for matches start_op_node and base_op_node\n./.venv/lib/python3.13/site-packages/torch/ao/ns/fx/weight_utils.py:    # TODO(future PR): make more generic, handle everything\n./.venv/lib/python3.13/site-packages/torch/ao/ns/fx/weight_utils.py:    # TODO(future PR): why does packed_weight.unpack() not work?\n./.venv/lib/python3.13/site-packages/torch/ao/ns/fx/n_shadows_utils.py:from torch.ao.ns.fx.utils import (  # TODO(future PR): make this work correctly for methods\n./.venv/lib/python3.13/site-packages/torch/ao/ns/fx/n_shadows_utils.py:# TODO(future PR): reuse existing mapping instead of creating a new one\n./.venv/lib/python3.13/site-packages/torch/ao/ns/fx/n_shadows_utils.py:    # TODO(future PR): try reversed(list(matches.items()))\n./.venv/lib/python3.13/site-packages/torch/ao/ns/fx/n_shadows_utils.py:            # TODO(future PR): make this code less confusing,  see discussion\n./.venv/lib/python3.13/site-packages/torch/ao/ns/fx/n_shadows_utils.py:    # TODO(future PR): reconsider the design to make this more intuitive.\n./.venv/lib/python3.13/site-packages/torch/ao/ns/fx/n_shadows_utils.py:            # TODO(future): some graphs could have placeholders which are unrelated\n./.venv/lib/python3.13/site-packages/torch/ao/ns/fx/n_shadows_utils.py:            # TODO(future PR): handle non-normalized kwargs\n./.venv/lib/python3.13/site-packages/torch/ao/ns/fx/n_shadows_utils.py:            # TODO(future PR): this is not handling complicated graphs correctly, need to\n./.venv/lib/python3.13/site-packages/torch/ao/ns/fx/n_shadows_utils.py:            # TODO(future PR): this is ignoring kwargs, will need to support kwargs\n./.venv/lib/python3.13/site-packages/torch/ao/ns/fx/n_shadows_utils.py:    # TODO(future PR): move logger classes to utils to remove circular dependency\n./.venv/lib/python3.13/site-packages/torch/ao/ns/fx/n_shadows_utils.py:        # TODO(future PR): deduplicate equivalent qconfigs that come from\n./.venv/lib/python3.13/site-packages/torch/ao/ns/fx/n_shadows_utils.py:            # TODO(future PR): handle fusion patterns where non-first nodes\n./.venv/lib/python3.13/site-packages/torch/ao/ns/fx/n_shadows_utils.py:                    # TODO(future PR): clarify why we are adding kwargs to args\n./.venv/lib/python3.13/site-packages/torch/ao/ns/fx/n_shadows_utils.py:    # TODO(future PR): implement this\n./.venv/lib/python3.13/site-packages/torch/ao/ns/fx/n_shadows_utils.py:        # TODO(future PR): add a test case for this once we have an easy\n./.venv/lib/python3.13/site-packages/torch/ao/ns/fx/n_shadows_utils.py:    # TODO(future): consider making this configurable\n./.venv/lib/python3.13/site-packages/torch/ao/ns/fx/n_shadows_utils.py:    # TODO(future PR): move logger classes to utils to remove circular dependency\n./.venv/lib/python3.13/site-packages/torch/ao/ns/fx/n_shadows_utils.py:                # TODO(future PR): make this support all possible args/kwargs\n./.venv/lib/python3.13/site-packages/torch/ao/ns/fx/n_shadows_utils.py:                        # cur_node_orig.name,  # TODO(future PR): set name explicitly\n./.venv/lib/python3.13/site-packages/torch/ao/ns/fx/n_shadows_utils.py:    # TODO(future PR): move this to config\n./.venv/lib/python3.13/site-packages/torch/ao/ns/fx/n_shadows_utils.py:        # TODO(future PR, if needed): support kwargs\n./.venv/lib/python3.13/site-packages/torch/ao/ns/fx/n_shadows_utils.py:        # TODO(future PR, if needed): support multiple shadow users\n./.venv/lib/python3.13/site-packages/torch/ao/ns/fx/n_shadows_utils.py:            # TODO(before land): fix string match\n./.venv/lib/python3.13/site-packages/torch/ao/ns/fx/n_shadows_utils.py:# TODO(future PR): redesign this to make it easier to consume outputs\n./.venv/lib/python3.13/site-packages/torch/ao/ns/fx/n_shadows_utils.py:# TODO(future PR): redesign this to make it easier to consume outputs\n./.venv/lib/python3.13/site-packages/torch/ao/ns/fx/n_shadows_utils.py:# TODO(future PR): redesign this to make it easier to consume outputs\n./.venv/lib/python3.13/site-packages/torch/ao/ns/fx/pattern_utils.py:    # TODO(future PR): allow customizations\n./.venv/lib/python3.13/site-packages/torch/ao/ns/fx/pattern_utils.py:    # TODO(future PR): reuse existing quantization mappings\n./.venv/lib/python3.13/site-packages/torch/ao/ns/fx/pattern_utils.py:    # TODO(future PR): add the rest of modules and ops here\n./.venv/lib/python3.13/site-packages/torch/ao/ns/fx/pattern_utils.py:    # TODO(future PR): allow customizations from default patterns.\n./.venv/lib/python3.13/site-packages/torch/ao/ns/fx/pattern_utils.py:        # TODO: this is a temporary hack to flatten the patterns from quantization so\n./.venv/lib/python3.13/site-packages/torch/ao/ns/fx/pattern_utils.py:        # TODO(future PR): if needed, implement matching for a node\n./.venv/lib/python3.13/site-packages/torch/ao/ns/_numeric_suite_fx.py:        # TODO(future PR): consider designing this better, as the difference\n./.venv/lib/python3.13/site-packages/torch/ao/ns/_numeric_suite_fx.py:        # TODO(future PR): consider refactoring this to better reuse the parent\n./.venv/lib/python3.13/site-packages/torch/ao/ns/_numeric_suite_fx.py:        # TODO(future PR): make the comparison function configurable\n./.venv/lib/python3.13/site-packages/torch/ao/ns/_numeric_suite_fx.py:    # TODO(future PR): expose these\n./.venv/lib/python3.13/site-packages/torch/ao/ns/_numeric_suite_fx.py:    # TODO(future PR): do not observe nodes we do not care\n./.venv/lib/python3.13/site-packages/torch/ao/ns/_numeric_suite_fx.py:    # TODO(future PR): expose these\n./.venv/lib/python3.13/site-packages/torch/ao/ns/_numeric_suite_fx.py:        # TODO(future PR): better check when scripted\n./.venv/lib/python3.13/site-packages/torch/ao/ns/_numeric_suite_fx.py:# TODO(future PR): align on naming\n./.venv/lib/python3.13/site-packages/torch/ao/ns/_numeric_suite_fx.py:    # TODO(future PR): expose these\n./.venv/lib/python3.13/site-packages/torch/ao/ns/_numeric_suite_fx.py:    High level TODOs for future PRs:\n./.venv/lib/python3.13/site-packages/torch/ao/ns/_numeric_suite_fx.py:    # TODO(future PR): deduplicate repeating entries\n./.venv/lib/python3.13/site-packages/torch/ao/ns/_numeric_suite_fx.py:# TODO(future PR): we should rethink the names of all the PNP APIs\n./.venv/lib/python3.13/site-packages/torch/ao/ns/_numeric_suite_fx.py:# TODO(future PR): we should rethink the names of all the PNP APIs\n./.venv/lib/python3.13/site-packages/torch/ao/ns/_numeric_suite_fx.py:# TODO(future PR): consider aligning API signature with other similar quantization\n./.venv/lib/python3.13/site-packages/torch/ao/ns/_numeric_suite_fx.py:# TODO(future PR): consider aligning API signature with other similar quantization\n./.venv/lib/python3.13/site-packages/torch/ao/ns/_numeric_suite_fx.py:        # TODO(future PR): consider matching in a safer way than\n./.venv/lib/python3.13/site-packages/torch/ao/nn/intrinsic/qat/modules/conv_fused.py:        # TODO(jerryzh): extend\n./.venv/lib/python3.13/site-packages/torch/ao/nn/intrinsic/quantized/modules/conv_relu.py:# TODO: factor out the common parts to ConvNd\n./.venv/lib/python3.13/site-packages/torch/ao/nn/intrinsic/quantized/modules/bn_relu.py:        # TODO: Add qat support for BNReLU2d\n./.venv/lib/python3.13/site-packages/torch/ao/nn/intrinsic/quantized/modules/bn_relu.py:        # TODO: Add qat support for BNReLU3d\n./.venv/lib/python3.13/site-packages/torch/ao/nn/intrinsic/quantized/dynamic/modules/linear_relu.py:            # TODO check if we should set reduce_rage = True by default here\n./.venv/lib/python3.13/site-packages/torch/ao/nn/quantizable/modules/rnn.py:        # TODO: make this tanh a member of the module so its qparams can be configured\n./.venv/lib/python3.13/site-packages/torch/ao/nn/quantizable/modules/activation.py:        # TODO: This method has some duplicate lines with the\n./.venv/lib/python3.13/site-packages/torch/ao/nn/quantized/modules/conv.py:    # TODO: maybe change to this when https://github.com/pytorch/pytorch/pull/32958 is landed\n./.venv/lib/python3.13/site-packages/torch/ao/nn/quantized/modules/activation.py:        # TODO: This is a potential source of accuracy drop.\n./.venv/lib/python3.13/site-packages/torch/ao/nn/quantized/dynamic/modules/rnn.py:        # TODO: dedup with __init__ of RNNBase\n./.venv/lib/python3.13/site-packages/torch/ao/nn/quantized/dynamic/modules/rnn.py:        # TODO: these can be simplified to one level? e.g. using weight_ih as key\n./.venv/lib/python3.13/site-packages/torch/ao/nn/quantized/dynamic/modules/rnn.py:        # TODO: these can be simplified to one level? e.g. using weight_ih as key\n./.venv/lib/python3.13/site-packages/torch/ao/nn/quantized/dynamic/modules/rnn.py:            ret = input  # TODO: remove when jit supports exception flow\n./.venv/lib/python3.13/site-packages/torch/ao/nn/quantized/reference/modules/utils.py:    # TODO: add an util function for converting qdtype to dtype\n./.venv/lib/python3.13/site-packages/torch/ao/nn/quantized/reference/modules/utils.py:        # TODO: torch.quint4x2 is not supported\n./.venv/lib/python3.13/site-packages/torch/ao/nn/quantized/reference/modules/utils.py:    # TODO: get the quant_min and quant_max from activation_post_process\n./.venv/lib/python3.13/site-packages/torch/ao/nn/quantized/reference/modules/utils.py:    # TODO: add an util function for converting qdtype to dtype\n./.venv/lib/python3.13/site-packages/torch/ao/nn/quantized/reference/modules/utils.py:        # TODO: torch.quint4x2 is not supported\n./.venv/lib/python3.13/site-packages/torch/ao/nn/quantized/reference/modules/rnn.py:        # TODO(jerryzh168): maybe make this arg a required arg\n./.venv/lib/python3.13/site-packages/torch/ao/nn/quantized/reference/modules/rnn.py:            # TODO: refactor the duplicated code to utils.py\n./.venv/lib/python3.13/site-packages/torch/ao/nn/quantized/reference/modules/rnn.py:    # TODO: refactor nn.RNNCell to have a _forward that takes weight_ih and weight_hh as input\n./.venv/lib/python3.13/site-packages/torch/ao/nn/quantized/reference/modules/rnn.py:            ret = input  # TODO: remove when jit supports exception flow\n./.venv/lib/python3.13/site-packages/torch/ao/nn/quantized/reference/modules/rnn.py:        # TODO(jerryzh168): maybe make this arg a required arg\n./.venv/lib/python3.13/site-packages/torch/ao/nn/quantized/reference/modules/rnn.py:        # TODO: maybe we can try inheriting from that class and define get_flat_weights\n./.venv/lib/python3.13/site-packages/torch/ao/nn/sparse/quantized/dynamic/linear.py:        # TODO: Need to add options to qconfig to avoid the calibration.\n./.venv/lib/python3.13/site-packages/torch/ao/nn/sparse/quantized/dynamic/linear.py:        # TODO: Add calibration for the sparsity\n./.venv/lib/python3.13/site-packages/torch/ao/nn/sparse/quantized/dynamic/linear.py:        # TODO (zaf): Mask might not be part of the qconfig (T83295194)\n./.venv/lib/python3.13/site-packages/torch/ao/nn/sparse/quantized/linear.py:# TODO (zaf): Inherit from `quantized.LinearPackedParams` (T83294430)\n./.venv/lib/python3.13/site-packages/torch/ao/nn/sparse/quantized/linear.py:# TODO (zaf): Inherit from `quantized.Linear` (T83294430)\n./.venv/lib/python3.13/site-packages/torch/ao/nn/sparse/quantized/linear.py:        TODO(zaf): Need to add the sparse params to the qconfig\n./.venv/lib/python3.13/site-packages/torch/ao/nn/sparse/quantized/linear.py:        # TODO: Need to add options to qconfig to avoid the calibration.\n./.venv/lib/python3.13/site-packages/torch/ao/nn/sparse/quantized/linear.py:        # TODO: Add calibration for the sparsity\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/pt2e/export_utils.py:    # TODO(Leslie): This function still fails to support custom momentum and eps value.\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/pt2e/export_utils.py:# TODO: expose these under this namespace?\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/pt2e/representation/rewrite.py:    # TODO: change to mul.Scalar\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/pt2e/representation/rewrite.py:    # TODO: change to mul.Scalar when we make x_scale/weight_scale etc. Scalar values\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/pt2e/representation/rewrite.py:    # TODO: use out_dtype(mul, ...) here when the op is ready\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/pt2e/representation/rewrite.py:    # TODO: change to mul.Scalar when we make x_scale/weight_scale etc. Scalar values\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/pt2e/representation/rewrite.py:    # TODO: change this to mul.Scalar?\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/pt2e/representation/rewrite.py:    # TODO: use out_dtype op\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/pt2e/representation/rewrite.py:    # TODO: use out_dtype(mul, ...) here when the op is ready\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/pt2e/representation/rewrite.py:    # TODO: use out_dtype op\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/pt2e/representation/rewrite.py:    # TODO: debug the implementation later when torchdynamo time out issue is resolved\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/pt2e/utils.py:# TODO: move this to torch/ao/quantization/utils.py\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/pt2e/utils.py:    # TODO: move this information to fx node itself\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/pt2e/utils.py:# TODO: Handle this in export itself and don't wrap the model in another GraphModule\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/pt2e/_affine_quantization.py:TODO: maybe can replace this with call to torch.iinfo\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/pt2e/_affine_quantization.py:# TODO: decide on if we want to allow custom quant_min/quant_max here\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/pt2e/_affine_quantization.py:    # TODO: validations\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/pt2e/_affine_quantization.py:    # TODO: validate scale/zero_point dimensions are compatible with block_size\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/pt2e/_affine_quantization.py:    # TODO: validate scale/zero_point dimensions are compatible with block_size\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/pt2e/_affine_quantization.py:        # TODO: this seems to be a detail for tinygemm (converting from uint to int, probably need to refactor this)\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/pt2e/qat_utils.py:# TODO: merge this with the `no_conv_bias` case\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/pt2e/qat_utils.py:        # TODO: allow setting eps\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/pt2e/qat_utils.py:        # TODO: allow setting eps\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/pt2e/qat_utils.py:    # TODO: allow setting eps\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/pt2e/qat_utils.py:    # TODO: allow setting eps\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/pt2e/qat_utils.py:# TODO: this is error prone, use the replace_literals_with_placeholders hack instead\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/pt2e/qat_utils.py:    # TODO: use the public replace_pattern API once it also returns replacement nodes\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/pt2e/qat_utils.py:    #       TODO: do this for literal args for batchnorm as well\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/pt2e/prepare.py:# TODO: make pt2e folder private?\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/pt2e/prepare.py:    # TODO: add assertions for types of root qspecs\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/pt2e/prepare.py:            # TODO: maybe edge_or_node_to_qspec should be edge_or_node_to_root_qspec, this will simplify\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/pt2e/prepare.py:        # TODO: simplify logic for inserting observers\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/quant_type.py:# TODO: make this private\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/quantizer/x86_inductor_quantizer.py:                        # TODO<leslie> Remove the annotate of output in QAT when qat util support pattern matcher.\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/quantizer/x86_inductor_quantizer.py:                        # TODO<leslie> Remove the annotate of output in QAT when qat util support pattern matcher.\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/quantizer/x86_inductor_quantizer.py:                        # TODO<leslie> Remove the annotate of output in QAT when qat util support pattern matcher.\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/quantizer/x86_inductor_quantizer.py:                        # TODO<leslie> Remove the annotate of output in QAT when qat util support pattern matcher.\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/quantizer/utils.py:            # TODO This is non standard behavior and should be removed when we migrate off capture_pre_autograd_graph.\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/quantizer/xnnpack_quantizer.py:        # TODO: qat + per channel?\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/quantizer/xnnpack_quantizer.py:        # TODO: move this to BoltNNQuantizer?\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/quantizer/xnnpack_quantizer.py:        # TODO: implement the support for None to be canceling out previous annotations\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/quantizer/xnnpack_quantizer.py:        # TODO: implement the support for None to be canceling out previous annotations\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/quantizer/xnnpack_quantizer_utils.py:    # TODO: remove, since we can use observer_or_fake_quant_ctr to express this\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/quantizer/xnnpack_quantizer_utils.py:        # TODO: annotate the uses of input, weight, and bias separately instead\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/quantizer/xnnpack_quantizer_utils.py:# TODO: remove Optional in return type, fix annotated_partitions logic\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/quantizer/xnnpack_quantizer_utils.py:            # TODO: change this to AnnotationException\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/quantizer/xnnpack_quantizer_utils.py:        # TODO: remove?\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/quantizer/xnnpack_quantizer_utils.py:# TODO: make the list of ops customizable\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/quantizer/quantizer.py:        # TODO: add init for quant_min/quant_max\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/quantizer/quantizer.py:    # TODO: change the value to QuantizationSpec in a separate PR\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/quantize_fx.py:        # TODO: add backend_config after we split the backend_config for fbgemm and qnnpack\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/quantize_fx.py:        # TODO: add backend_config after we split the backend_config for fbgemm and qnnpack\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/quantize_fx.py:        # TODO: add backend_config after we split the backend_config for fbgemm and qnnpack\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/quantize_fx.py:        # TODO: add backend_config after we split the backend_config for fbgemm and qnnpack\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/quantize_fx.py:        # TODO: add backend_config after we split the backend_config for fbgemm and qnnpack\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/utils.py:# TODO(future PR): improve this.\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/utils.py:# TODO: not sure if typing supports recursive data types\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/utils.py:# TODO: maybe rename this to MatchInputNode\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/utils.py:# TODO: not used now, remove\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/utils.py:    # TODO: reuse is_fixed_qparam_node after we move this function to _lower_to_native_backend.py\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/utils.py:    # TODO(jerryzh): Figure out why custom quant_min/quant_max are still adjusted.\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/utils.py:# (last update over 1 year ago) and when torchscript is fully deprecated we can refactor. TODO(jakeszwe, jerryzh168)\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/utils.py:        # TODO: switch to scale.item() after adding JIT support\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/utils.py:        # TODO: switch to zero_point.item() after adding JIT support\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fake_quantize.py:        # TODO: keeping self.quant_min/max for BC; remove after a couple releases\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fake_quantize.py:    # TODO: rename observer to observer_ctr\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fake_quantize.py:# TODO: the following 2 variables are kept for backwards compatibility; remove after a few releases\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/backend_config/_qnnpack_pt2e.py:    # TODO: need to fix the way we insert observers for this pattern\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/backend_config/_qnnpack_pt2e.py:    # TODO: remove when functionalization is supported in PT2 mode\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/backend_config/_qnnpack_pt2e.py:        # TODO: this is not used right now since we have extra check in prepare\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/backend_config/_qnnpack_pt2e.py:            # TODO: remove when functionalization is supported in pt2_mode\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/backend_config/_common_operator_config_utils.py:# TODO: rename to be more explicit, e.g. qat_conv_relu\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/backend_config/_common_operator_config_utils.py:        # TODO: this is not used right now since we have extra check in prepare\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/backend_config/_common_operator_config_utils.py:        # TODO: we can add fusion for torch.relu as well\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/backend_config/tensorrt.py:    TODO: add a README when it's more stable\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/backend_config/backend_config.py:# TODO: maybe rename this to something that's not related to observer\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/backend_config/backend_config.py:    # TODO: refer to NativeBackendConfig once that is implemented\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/backend_config/qnnpack.py:# TODO: add additional restriction on qscheme to ensure it\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/backend_config/utils.py:# TODO(future PR): move backend_config_dict to use dataclass and move this logic to\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/backend_config/executorch.py:# TODO: rename executorch to qnnpack_executorch since executorch is a general runtime\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/backend_config/executorch.py:        # TODO: we can add fusion for torch.relu as well\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/backend_config/executorch.py:        # TODO: this is not used right now since we have extra check in prepare\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/backend_config/native.py:    # TODO: express this BackendConfig as a union of the FBGEMM and QNNPACK BackendConfigs\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/backend_config/fbgemm.py:# TODO: For now, these DTypeConfigs are identical to the ones defined in native.py\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/fuse_handler.py:        # TODO: change the signature for fuser_method to take matched module patterns\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/fuse_handler.py:            # TODO: is this logic right?\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/fuse.py:    # TODO: We should make this private in the future\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/fuse.py:    # TODO: change this to inplace changes to graph, since we no longer construct\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/fuse.py:            # TODO: add validation that root_node is a module and has the same type\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/fuse.py:    # TODO: dedup with quantization matching function in match_utils.py\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/_lower_to_native_backend.py:# TODO: correct the namespace for these modules\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/_lower_to_native_backend.py:# TODO: merge with STATIC_LOWER_MODULE_MAP after we merge\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/_lower_to_native_backend.py:    # TODO: LinearLeakyReLU is registered as global but it is only fused and\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/_lower_to_native_backend.py:# TODO: add tests for lowering these ops\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/_lower_to_native_backend.py:        # TODO: maybe define a WeightedDynamicallyQuantizedModule\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/_lower_to_native_backend.py:        # TODO: WeightedQuantizedModule is currently assuming static quant apis\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/_lower_to_native_backend.py:        # TODO: maybe define a WeightedWeightOnlyQuantizedModule\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/_lower_to_native_backend.py:        # TODO: add safety checks that users for the ref_node and dq_node needs to be one\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/_lower_to_native_backend.py:            # TODO: add a warning or error out here? (bc-breaking if error out)\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/_lower_to_native_backend.py:            # TODO: add a warning or error out here? (bc-breaking if error out)\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/_lower_to_native_backend.py:        # TODO: enable we have patterns that needs to swap the modules\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/_decomposed.py:# TODO: remove other variants and keep this one\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/_decomposed.py:        # TODO: investigate why\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/_decomposed.py:# TODO: remove other variants and keep this one\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/_decomposed.py:    # TODO: support fp16\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/_decomposed.py:# TODO: dtype is ignored for now\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/_decomposed.py:    # TODO: check for dtype, currently we can't express torch.int4 so it's omitted\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/utils.py:# TODO: revisit this list. Many helper methods shouldn't be public\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/utils.py:# TODO: delete\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/utils.py:                        # TODO(future PR): remove this entire function  and\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/utils.py:                    # TODO(future PR): remove this entire function  and\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/utils.py:        TODO: traverse upwards from the output and handle the case when tuple is not a\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/utils.py:    # TODO: log warnings only when the user enabled a debug flag\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/utils.py:        # TODO: for now, just use the existing eps value as scale_min. In the future, we should\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/utils.py:            # TODO: handle fp16 qconfigs properly\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/qconfig_mapping_utils.py:            # TODO: currently it only works for modules,\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/qconfig_mapping_utils.py:            # TODO: currently it only works for object_type configurations,\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/lstm_utils.py:# TODO: move all LSTM util functions from fx/utils.py to this file\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/lstm_utils.py:    # TODO: maybe make this work for layer_bw as well\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/convert.py:        # TODO: probably should cleanup this condition check, it's hard\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/convert.py:                # TODO: we can add the information of whether a value needs to\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/convert.py:                    # TODO: maybe need more complex attr name here\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/convert.py:                # TODO: we can add the information of whether a value needs to\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/convert.py:        # TODO: probably should cleanup this condition check, it's hard\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/convert.py:                # TODO: we can add the information of whether a value needs to\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/convert.py:                    # TODO: maybe need more complex attr name here\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/convert.py:        # TODO: get reduce range from observer\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/convert.py:                # TODO: we can add the information of whether a value needs to\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/convert.py:# TODO: DeQuantStubs are currently inserted only after custom module LSTM, while observers are inserted\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/convert.py:    TODO: this logic is hacky, we should think about how to remove it or make it more\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/convert.py:        # TODO: it's not used, so actually we can skip quantization\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/convert.py:    # TODO: remove is_reference flag\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/convert.py:    # TODO: allow convert_custom_config to override backend_config\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/convert.py:    # TODO: rename weight_is_statically_quantized to weight_is_int8_quantized\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/convert.py:    # TODO: move this to the reference quantized module\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/convert.py:        TODO: maybe we want to redesign this part to align with reference model design\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/convert.py:            # TODO: This is the first step in enabling the full fx custom module\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/convert.py:    # TODO refactor this code once we update the prepare logic to have additional information on\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/convert.py:    # TODO: maybe move this to quantize_fx.py\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/convert.py:    # TODO: this looks hacky, we want to check why we need this and see if we can\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/custom_config.py:# TODO: replace all usages with these constants\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/custom_config.py:    # TODO: remove this\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/custom_config.py:    # TODO: remove this\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/custom_config.py:    # TODO: remove this\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/pattern_utils.py:# TODO(future PR): fix the typing on QuantizeHandler (currently a circular dependency)\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/quantize_handler.py:# TODO: remove this class, this is still exposed in torch.ao.quantization\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/quantize_handler.py:# TODO: remove this class\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/quantize_handler.py:# TODO: remove this class\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/quantize_handler.py:# TODO: remove this class\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/quantize_handler.py:# TODO: remove this class\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/quantize_handler.py:# TODO: remove this class\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/quantize_handler.py:# TODO: remove this class\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/quantize_handler.py:# TODO: remove this class\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/quantize_handler.py:# TODO: remove\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/quantize_handler.py:# TODO: remove\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/quantize_handler.py:# TODO: not used, can be removed after torch.ao.quantization namespace is deprecated\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/quantize_handler.py:# TODO: not used, can be removed after torch.ao.quantization namespace is deprecated\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/match_utils.py:# TODO(future PR): the 1st argument is typed as `List[Node]`, but a better type\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/match_utils.py:    # TODO: 1. merge with fuse matcher 2. document the code\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/prepare.py:    # TODO: instead of instantiating the instance, we can use inspect to get the default args\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/prepare.py:    # TODO: support check for standalone module\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/prepare.py:        # TODO(future PR): remove the cast to bool below after figuring\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/prepare.py:        # TODO: move dtype check into `_qconfig_satisfies_dtype_config_constraints` as well\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/prepare.py:        # TODO: move dtype check into `_qconfig_satisfies_dtype_config_constraints` as well\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/prepare.py:    # TODO: move dtype check into `_qconfig_satisfies_dtype_config_constraints` as well\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/prepare.py:    # TODO: we should check is_dynamic here as well, the code from _is_input_arg_dtype_supported_by_backend\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/prepare.py:    # TODO: this is a hack because we can only specify one activation_obs_or_fq for\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/prepare.py:        # TODO: refactor the following code in terms of apply a qconfig to a pattern\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/prepare.py:    TODO(future PR, if needed): explicitly spell out the non-Tensor\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/prepare.py:    # TODO: move this to a separate function\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/prepare.py:            # TODO: we are assuming \"target_dtype_info\" exists here, maybe\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/prepare.py:        # TODO: this is looking into how the value is used in the future\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/prepare.py:            # TODO: this does not handle dynamic quantization yet\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/prepare.py:    # TODO: probably need to remove `is_general_tensor_value_op`\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/prepare.py:    # TODO(future PR): delete the orphaned observer modules\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/prepare.py:    # TODO: we probably don't need this counter since each graph will only have\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/prepare.py:            # TODO(future PR): update the output_quantized_idxs API to match\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/prepare.py:            # TODO(future PR): support more dtypes in model outputs, if necessary\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/prepare.py:        # TODO: we might want to handle these more uniformly with the default path\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/prepare.py:    # TODO: reuse placeholder_node_to_input_index and output_node_to_output_index\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/prepare.py:    # TODO: change this to insert obs/fq by pattern instead of by node\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/prepare.py:            # TODO: take a closer look to see if we can remove this check\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/prepare.py:                            # TODO: This currently diverges from how custom modules are handled today,\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/prepare.py:    # TODO: support regex as well\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/observer.py:        # TODO(jakeszwe, jerryzh168)\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/observer.py:            # TODO: switch to scale.item() after adding JIT support\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/observer.py:            # TODO: switch to zero_point.item() after adding JIT support\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/observer.py:# TODO(after v1.13): delete this\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/observer.py:        # TODO: MinMaxObserver by itself doesn't support dynamic quantization, but\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/observer.py:            # TODO: For some reason, this is required for it to pass torchscript test\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/observer.py:        quant_min: minimum value in quantized domain (TODO: align behavior with other observers)\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/observer.py:# TODO(future PR): remove these defaults and enforce activation functions\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/observer.py:# TODO: the following 2 variables are kept for backwards compatibility; remove after a few releases\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/quantize.py:# TODO remove this once BC is no longer required to avoid a SEV\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/quantize.py:        # TODO remove Dropout special after codebase stable\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/quantize.py:            # TODO: These are the modules that cannot be observed\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/quantize.py:    # TODO: remove allow_list\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/quantize.py:    # TODO: maybe we should change activation_post_process to _activation_post_process\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/quantize.py:# TODO: rename to something more general\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/quantization_mappings.py:# TODO: merge with default static mapping\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/quantization_mappings.py:# TODO: merge with get_static_quant_module_class\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/quantize_pt2e.py:    # TODO: check qconfig_mapping to make sure conv and bn are both configured\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/quantize_pt2e.py:    # TODO: (maybe) rewrite this with subgraph_rewriter\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/quantize_pt2e.py:    # TODO: only fuse if conv and bn are both configured to be quantized\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/qconfig_mapping.py:# TODO: replace all usages with these constants\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/qconfig_mapping.py:# TODO: derive this map from the BackendConfig\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/qconfig_mapping.py:    # TODO Currently it's required that separate ops in a fused op/module have the same qconfig.\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/qconfig_mapping.py:    # TODO: add assert for backend choices\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/qconfig_mapping.py:    # TODO: remove this\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/qconfig_mapping.py:    # TODO: remove this\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/qconfig.py:    # TODO: deprecated, remove\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/qconfig.py:            # TODO: make this compatible with xnnpack constraints\n./.venv/lib/python3.13/site-packages/torch/ao/quantization/qconfig.py:            # TODO: make this compatible with xnnpack constraints\n./.venv/lib/python3.13/site-packages/torch/ao/pruning/sparsifier/base_sparsifier.py:# TODO update desc with new config args\n./.venv/lib/python3.13/site-packages/torch/ao/pruning/sparsifier/base_sparsifier.py:        TODO: Need a clean way of loading the state of the \"prepared\" module\n./.venv/lib/python3.13/site-packages/torch/ao/pruning/sparsifier/base_sparsifier.py:        self.model = model  # TODO: Need to figure out how to load without this.\n./.venv/lib/python3.13/site-packages/torch/ao/pruning/sparsifier/base_sparsifier.py:        # TODO: Remove the configuration by reference ('module')\n./.venv/lib/python3.13/site-packages/torch/ao/pruning/sparsifier/base_sparsifier.py:                # TODO handle multiple tensor being quantized on a single module, where to store sparse_params?\n./.venv/lib/python3.13/site-packages/torch/ao/pruning/sparsifier/utils.py:        # TODO Fix this typing, as Type[Module] has no attribute \"from_dense\"\n./.venv/lib/python3.13/site-packages/torch/ao/pruning/_experimental/pruner/base_structured_sparsifier.py:        # TODO LSTM Structured pruning does not support returned state currently.\n./.venv/lib/python3.13/site-packages/torch/ao/pruning/_experimental/activation_sparsifier/activation_sparsifier.py:        TODO: Might have to treat functions (reduce_fn, mask_fn etc) in a different manner while serializing.\n./.venv/lib/python3.13/site-packages/torch/cpu/amp/autocast_mode.py:    # TODO: discuss a unified TorchScript-friendly API for autocast\n./.venv/lib/python3.13/site-packages/torch/package/importer.py:            # TODO: I guess we should do copyreg too?\n./.venv/lib/python3.13/site-packages/torch/package/package_importer.py:                # TODO: Once we decide to break serialization FC, we can\n./.venv/lib/python3.13/site-packages/torch/package/package_importer.py:        # TODO from zdevito:\n./.venv/lib/python3.13/site-packages/torch/package/package_exporter.py:                # TODO: Once we decide to break serialization FC, we can\n./.venv/lib/python3.13/site-packages/torch/_strobelight/compile_time_profiler.py:    # TODO use threadlevel meta data to tags to record phases.\n./.venv/lib/python3.13/site-packages/torch/_torch_docs.py:# TODO: Fix via https://github.com/pytorch/pytorch/issues/75798\n./.venv/lib/python3.13/site-packages/torch/autograd/__init__.py:            # TODO: We can remove this conditional once we uniformly use\n./.venv/lib/python3.13/site-packages/torch/autograd/profiler_legacy.py:    category=None,  # TODO: change to `FutureWarning`\n./.venv/lib/python3.13/site-packages/torch/autograd/_functions/tensor.py:# TODO: deprecate this\n./.venv/lib/python3.13/site-packages/torch/autograd/profiler.py:        # TODO Consider changing _function_events into data structure with size cap\n./.venv/lib/python3.13/site-packages/torch/autograd/profiler.py:        # TODO: TorchScript ignores standard type annotation here\n./.venv/lib/python3.13/site-packages/torch/autograd/profiler.py:        # TODO: Too slow with __torch_function__ handling enabled\n./.venv/lib/python3.13/site-packages/torch/autograd/profiler.py:        # TODO: Too slow with __torch_function__ handling enabled\n./.venv/lib/python3.13/site-packages/torch/autograd/profiler.py:        )  # TODO: find in sqlite database\n./.venv/lib/python3.13/site-packages/torch/autograd/gradcheck.py:                # TODO: handle the other Ju\n./.venv/lib/python3.13/site-packages/torch/autograd/gradcheck.py:            # TODO: To cover more problematic cases, replace stride = 0 check with\n./.venv/lib/python3.13/site-packages/torch/autograd/gradcheck.py:    # TODO: properly handle case when u is tuple instead of only taking first element\n./.venv/lib/python3.13/site-packages/torch/autograd/gradcheck.py:    # TODO: replicate https://github.com/pytorch/pytorch/pull/77743 for fast gradcheck as well\n./.venv/lib/python3.13/site-packages/torch/autograd/gradcheck.py:    # TODO: do we want to test this too?\n./.venv/lib/python3.13/site-packages/torch/_refs/__init__.py:    \"round\",  # TODO: model kwargs\n./.venv/lib/python3.13/site-packages/torch/_refs/__init__.py:    \"copy_to\",  # TODO: add OpInfo (or implement .to)\n./.venv/lib/python3.13/site-packages/torch/_refs/__init__.py:    # TODO: make common validations available as utils\n./.venv/lib/python3.13/site-packages/torch/_refs/__init__.py:# TODO: add type promotion support\n./.venv/lib/python3.13/site-packages/torch/_refs/__init__.py:# TODO: if this is special maybe it should be defined there and imported here?\n./.venv/lib/python3.13/site-packages/torch/_refs/__init__.py:# TODO: register this as a real ref/decomposition once TorchInductor supports complex!\n./.venv/lib/python3.13/site-packages/torch/_refs/__init__.py:# TODO: skip unnecessary conversion of long to float\n./.venv/lib/python3.13/site-packages/torch/_refs/__init__.py:# TODO: consider refactoring this with add impl\n./.venv/lib/python3.13/site-packages/torch/_refs/__init__.py:# TODO: implement alternate where\n./.venv/lib/python3.13/site-packages/torch/_refs/__init__.py:    # TODO: is_pinned is not currently supported in refs or fake_tensor\n./.venv/lib/python3.13/site-packages/torch/_refs/__init__.py:    # TODO: non_blocking should be handled by `copy_to`\n./.venv/lib/python3.13/site-packages/torch/_refs/__init__.py:            # TODO - this is true for eager mode currently, but it's wrong behavior for complex norms\n./.venv/lib/python3.13/site-packages/torch/_refs/__init__.py:        # TODO: fix this to work with meta tensors\n./.venv/lib/python3.13/site-packages/torch/_refs/__init__.py:    # TODO: make logic consistent with aten contiguous\n./.venv/lib/python3.13/site-packages/torch/_refs/__init__.py:    # TODO: we could look at directing collapse_view to skip its meta function here (unsafe_collapse_view)\n./.venv/lib/python3.13/site-packages/torch/_refs/__init__.py:# TODO: Adding this as a meta function causes functorch tests to fail when compiled with debug mode.\n./.venv/lib/python3.13/site-packages/torch/_refs/__init__.py:    # TODO: Add sparse support\n./.venv/lib/python3.13/site-packages/torch/_refs/__init__.py:# TODO: Turn this into a decomposition (currently fails on reshape meta tests)\n./.venv/lib/python3.13/site-packages/torch/_refs/__init__.py:    requires_grad: bool = False,  # TODO: unused\n./.venv/lib/python3.13/site-packages/torch/_refs/__init__.py:    # TODO: Use requires_grad.  All refs taking the requires_grad kwarg must\n./.venv/lib/python3.13/site-packages/torch/_refs/__init__.py:    # TODO: fix inductor rand_like for integer, bool dtypes\n./.venv/lib/python3.13/site-packages/torch/_refs/__init__.py:# TODO: add support for functionalization aten.normal_functional\n./.venv/lib/python3.13/site-packages/torch/_refs/__init__.py:# TODO: This must return a sparse tensor if the input is sparse, but refs have\n./.venv/lib/python3.13/site-packages/torch/_refs/__init__.py:    # TODO: this is inaccurate, we actually test PySequence_Check\n./.venv/lib/python3.13/site-packages/torch/_refs/__init__.py:    # TODO: this is inaccurate, we actually test PySequence_Check\n./.venv/lib/python3.13/site-packages/torch/_refs/__init__.py:            # TODO: test this\n./.venv/lib/python3.13/site-packages/torch/_refs/__init__.py:    # TODO\n./.venv/lib/python3.13/site-packages/torch/_refs/__init__.py:    # TODO: test for numpy input with PyArray_Check\n./.venv/lib/python3.13/site-packages/torch/_refs/__init__.py:    # TODO (or not): support names kwarg\n./.venv/lib/python3.13/site-packages/torch/_refs/__init__.py:        {\"device\": \"cpu\"},  # TODO: use torch.get_default_tensor_type\n./.venv/lib/python3.13/site-packages/torch/_refs/nn/functional/__init__.py:        # TODO: Raise exception instead of converting value.  This is only for\n./.venv/lib/python3.13/site-packages/torch/_refs/nn/functional/__init__.py:        # TODO: Raise exception instead of converting value.  This is only for\n./.venv/lib/python3.13/site-packages/torch/_refs/nn/functional/__init__.py:        # TODO: Raise exception instead of converting value.  This is only for\n./.venv/lib/python3.13/site-packages/torch/_refs/nn/functional/__init__.py:    # TODO: Enable data-dependent checks with debug mode\n./.venv/lib/python3.13/site-packages/torch/_refs/nn/functional/__init__.py:    # TODO: This check does not work with FakeTensor inputs; See Issue #85834\n./.venv/lib/python3.13/site-packages/torch/_refs/nn/functional/__init__.py:    # TODO: raise exception instead of converting value\n./.venv/lib/python3.13/site-packages/torch/_refs/nn/functional/__init__.py:# TODO: This ref supports int reduction and out kwarg to be compatible with ATen:\n./.venv/lib/python3.13/site-packages/torch/_refs/nn/functional/__init__.py:# TODO: Could be rewritten to support complex:\n./.venv/lib/python3.13/site-packages/torch/_refs/nn/functional/__init__.py:        # TODO: Raise exception instead of converting value.  This is only for\n./.venv/lib/python3.13/site-packages/torch/_refs/nn/functional/__init__.py:        # TODO: Raise exception instead of converting value.  This is only for\n./.venv/lib/python3.13/site-packages/torch/_refs/special/__init__.py:# TODO: add docstring\n./.venv/lib/python3.13/site-packages/torch/_export/passes/constant_folding.py:        # TODO - fix errors with this\n./.venv/lib/python3.13/site-packages/torch/_export/passes/constant_folding.py:        # TODO - constant folding triton kernel returns the inputs -- fix this\n./.venv/lib/python3.13/site-packages/torch/_export/passes/constant_folding.py:        # TODO - more complicated strategy\n./.venv/lib/python3.13/site-packages/torch/_export/passes/replace_quantized_ops_with_standard_ops_pass.py:        TODO: SetAttr + quantized ops will result incorrect program. This flag is used to temporarily\n./.venv/lib/python3.13/site-packages/torch/_export/passes/replace_autocast_with_hop_pass.py:            # TODO: check if current auto-cast type is the same as the args of\n./.venv/lib/python3.13/site-packages/torch/_export/passes/replace_with_hop_pass_util.py:        # TODO (tmanlaibaatar) Figure out if this is right behaviour\n./.venv/lib/python3.13/site-packages/torch/_export/passes/replace_with_hop_pass_util.py:            # TODO (shangdiy): remove this line, since the export graph can be non-functional\n./.venv/lib/python3.13/site-packages/torch/_export/utils.py:    # TODO some annoying circular dependency issue\n./.venv/lib/python3.13/site-packages/torch/_export/utils.py:    # TODO: Directly provide inspect.signature compatible TS-d module.\n./.venv/lib/python3.13/site-packages/torch/_export/non_strict_utils.py:    # TODO(avik): refactor Dynamo to avoid duplication of the following code\n./.venv/lib/python3.13/site-packages/torch/_export/non_strict_utils.py:        # TODO(avik): Maybe record the constraint violation error instead and replay later?\n./.venv/lib/python3.13/site-packages/torch/_export/pass_base.py:                        # TODO (tmanlaibaatar) properly support Quantized FakeTensor\n./.venv/lib/python3.13/site-packages/torch/_export/pass_base.py:                        # TODO we should allocate static shapes\n./.venv/lib/python3.13/site-packages/torch/_export/pass_base.py:                        # TODO: This is just a workaround to get over the\n./.venv/lib/python3.13/site-packages/torch/_export/pass_base.py:                        # TODO (tmanlaibaatar) properly support Quantized FakeTensor\n./.venv/lib/python3.13/site-packages/torch/_export/pass_base.py:                        # TODO: This is just a workaround to get over the\n./.venv/lib/python3.13/site-packages/torch/_export/pass_base.py:        # TODO(angelayi): Update this with what we decide to do for metadata in\n./.venv/lib/python3.13/site-packages/torch/_export/verifier.py:    # TODO(angelayi): remove this in favor of _check_val\n./.venv/lib/python3.13/site-packages/torch/_export/verifier.py:        ):  # TODO(zhxchen17) Remove Tensor.\n./.venv/lib/python3.13/site-packages/torch/_export/verifier.py:            # TODO Remove this allowlist.\n./.venv/lib/python3.13/site-packages/torch/_export/verifier.py:                # TODO (tmanlaibaatar)\n./.venv/lib/python3.13/site-packages/torch/_export/verifier.py:                # TODO (tmanlaibaatar) more proper way is needed here\n./.venv/lib/python3.13/site-packages/torch/_export/verifier.py:                # TODO(T140410192): should have fake tensor for all dialects\n./.venv/lib/python3.13/site-packages/torch/_export/verifier.py:                # TODO(zhxchen17)\n./.venv/lib/python3.13/site-packages/torch/_export/serde/serialize.py:        storage_offset=serialize_sym_int(0),  # TODO needs to be fixed.\n./.venv/lib/python3.13/site-packages/torch/_export/serde/serialize.py:        # TODO: this should be fixed by deserialization instead.\n./.venv/lib/python3.13/site-packages/torch/_export/serde/serialize.py:    # TODO: Remove this adjustment when Ed gets rid of fractional ranges\n./.venv/lib/python3.13/site-packages/torch/_export/serde/serialize.py:            # TODO(zhxchen17) Maybe provide a function name helper in FX.\n./.venv/lib/python3.13/site-packages/torch/_export/serde/serialize.py:        else:  # TODO(zhxchen17) Don't catch all here.\n./.venv/lib/python3.13/site-packages/torch/_export/serde/serialize.py:                # TODO: create a new tensor_values here, meta might have faketensor info\n./.venv/lib/python3.13/site-packages/torch/_export/serde/serialize.py:            raise AssertionError(\"TODO\")\n./.venv/lib/python3.13/site-packages/torch/_export/serde/serialize.py:        # TODO: Directly serialize exported_program.constants once\n./.venv/lib/python3.13/site-packages/torch/_export/serde/serialize.py:        ):  # TODO(zhxchen17) Follow up on this.\n./.venv/lib/python3.13/site-packages/torch/_export/serde/serialize.py:        else:  # TODO(zhxchen17) Don't catch all here.\n./.venv/lib/python3.13/site-packages/torch/_export/serde/serialize.py:                # TODO(avik): find a better way to keep this collection in sync;\n./.venv/lib/python3.13/site-packages/torch/_export/serde/serialize.py:            # TODO(pianpwk): if we can clean up unused symbols in range_constraints,\n./.venv/lib/python3.13/site-packages/torch/_export/serde/serialize.py:        # TODO(zhxchen17) blocked on thrift schema refactor\n./.venv/lib/python3.13/site-packages/torch/_export/converter.py:        # TODO: covnert sourceRange() into stack_trace\n./.venv/lib/python3.13/site-packages/torch/_export/converter.py:        # TODO: covnert sourceRange() into stack_trace\n./.venv/lib/python3.13/site-packages/torch/_export/converter.py:                    # TODO: covnert sourceRange() into stack_trace\n./.venv/lib/python3.13/site-packages/torch/_export/converter.py:        # TODO: (1/N) stage.\n./.venv/lib/python3.13/site-packages/torch/_export/converter.py:        # TODO: support aten::enable_grad in both TorchScript and Converter.\n./.venv/lib/python3.13/site-packages/torch/_export/converter.py:                # TODO: Revisit this later after HigherOrderOp design changes.\n./.venv/lib/python3.13/site-packages/torch/_export/converter.py:        # TODO: adjust input orders to match GraphSignature convention\n./.venv/lib/python3.13/site-packages/torch/_functorch/make_functional.py:        # TODO: We don't need to copy the model to create a stateless copy\n./.venv/lib/python3.13/site-packages/torch/_functorch/make_functional.py:        # TODO: We don't need to copy the model to create a stateless copy\n./.venv/lib/python3.13/site-packages/torch/_functorch/_aot_autograd/collect_metadata_analysis.py:    # TODO: refactor to kill this flag\n./.venv/lib/python3.13/site-packages/torch/_functorch/_aot_autograd/logging_utils.py:# TODO: It would be nice to reset the numbering every time aot_id goes\n./.venv/lib/python3.13/site-packages/torch/_functorch/_aot_autograd/logging_utils.py:    # TODO: Don't shove the aot_id in here; set it in the context\n./.venv/lib/python3.13/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py:    # TODO (tmanlaibaatar) revisit this if we ever need to turn on non-strict joint graph export\n./.venv/lib/python3.13/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py:    # TODO: add subclass guards (later PR).\n./.venv/lib/python3.13/site-packages/torch/_functorch/_aot_autograd/utils.py:            # TODO: Please remove soon\n./.venv/lib/python3.13/site-packages/torch/_functorch/_aot_autograd/utils.py:        # TODO(future): there is likely a less brittle way to do this by walking\n./.venv/lib/python3.13/site-packages/torch/_functorch/_aot_autograd/utils.py:        # TODO(future): there is likely a less brittle way to do this, same\n./.venv/lib/python3.13/site-packages/torch/_functorch/_aot_autograd/schemas.py:    # TODO: we should kill this\n./.venv/lib/python3.13/site-packages/torch/_functorch/_aot_autograd/schemas.py:        # TODO: This function is only a best effort: there are other fields that may not be cache safe\n./.venv/lib/python3.13/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:        # TODO: once we use pre_compile this will be flat_fn at the top of this function\n./.venv/lib/python3.13/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:        # TODO: invoke_subgraph should track which of its inputs static indices\n./.venv/lib/python3.13/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:            # TODO(anijain2305) - Add tensorify_python_scalars to the HOP graph passes.\n./.venv/lib/python3.13/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:        # TODO: we should apply the below \"detach inputs if their gradients are statically known to be None\"\n./.venv/lib/python3.13/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:                # TODO: technically, AOTAutograd does a *little* bit of post processing work\n./.venv/lib/python3.13/site-packages/torch/_functorch/_aot_autograd/functional_utils.py:            # TODO:add sparse tensors support to functionalization\n./.venv/lib/python3.13/site-packages/torch/_functorch/_aot_autograd/functional_utils.py:    # isn't actually true.  (TODO: Could this cause problems for Inductor?)\n./.venv/lib/python3.13/site-packages/torch/_functorch/_aot_autograd/dispatch_and_compile_graph.py:# TODO: Refactor the following code so detach() persists item_memo\n./.venv/lib/python3.13/site-packages/torch/_functorch/_aot_autograd/dispatch_and_compile_graph.py:    # TODO: replace with AOTDispatchSubclassWrapper once we refactor\n./.venv/lib/python3.13/site-packages/torch/_functorch/_aot_autograd/dispatch_and_compile_graph.py:    # TODO: should factor this into a separate function for export that always only returns just the graph.\n./.venv/lib/python3.13/site-packages/torch/_functorch/_aot_autograd/dispatch_and_compile_graph.py:    # TODO: replace with AOTDispatchSubclassWrapper once we refactor\n./.venv/lib/python3.13/site-packages/torch/_functorch/_aot_autograd/dispatch_and_compile_graph.py:    # TODO: in AOTAutograd, we create metadata like _indices_of_inps_to_detach to detect\n./.venv/lib/python3.13/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:                    #     TODO: discuss on the PR and decide if we want to tr to\n./.venv/lib/python3.13/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:    # TODO: I would love to get rid of this argument, but it's\n./.venv/lib/python3.13/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:                    # TODO: this won't be right for the backward when we convert the call_compiled_backward to use the wrapper\n./.venv/lib/python3.13/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:    # TODO: Can avoid the zip here too, probably\n./.venv/lib/python3.13/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:            # TODO(voz): This structure is 1:1, we could consider an alternate structure like\n./.venv/lib/python3.13/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:            # TODO: work out how to setup this assert correctly\n./.venv/lib/python3.13/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:    trace_joint: bool  # TODO: refactor trace_joint\n./.venv/lib/python3.13/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:    # TODO: replace this with FunctionalizedRngRuntimeWrapper\n./.venv/lib/python3.13/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:    # TODO: figure out how to refactor the backward properly\n./.venv/lib/python3.13/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:    # TODO: replace this with FunctionalizedRngRuntimeWrapper.post_compile\n./.venv/lib/python3.13/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:    # TODO: figure out how to refactor the backward properly so I can use aot_dispatch_subclass_wrapper() here.\n./.venv/lib/python3.13/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:            # TODO: Check aliasing relationships\n./.venv/lib/python3.13/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:            # TODO: Check strides for metadata mutation\n./.venv/lib/python3.13/site-packages/torch/_functorch/_aot_autograd/autograd_cache.py:    # TODO: add args and parameters\n./.venv/lib/python3.13/site-packages/torch/_functorch/_aot_autograd/autograd_cache.py:            # TODO: maybe also log to aot_graphs_log\n./.venv/lib/python3.13/site-packages/torch/_functorch/_aot_autograd/autograd_cache.py:        # TODO: this isn't exactly right, because cudagraphs needs to be a shared config\n./.venv/lib/python3.13/site-packages/torch/_functorch/_aot_autograd/autograd_cache.py:        # TODO: this ignores flat_params, which can exist\n./.venv/lib/python3.13/site-packages/torch/_functorch/_aot_autograd/autograd_cache.py:                    # TODO: should we use the same field for remote cache time saved for both\n./.venv/lib/python3.13/site-packages/torch/_functorch/_aot_autograd/autograd_cache.py:                # TODO: this gets logged implicitly by cache_bypass_reason,\n./.venv/lib/python3.13/site-packages/torch/_functorch/compilers.py:            # TODO: There is some sort of problem where we record that an\n./.venv/lib/python3.13/site-packages/torch/_functorch/pytree_hacks.py:# TODO: remove this file when the migration of the pytree utility is done\n./.venv/lib/python3.13/site-packages/torch/_functorch/autograd_function.py:        # TODO: update following link from master to stable once that's out\n./.venv/lib/python3.13/site-packages/torch/_functorch/autograd_function.py:            # TODO: Update link to stable once that's out\n./.venv/lib/python3.13/site-packages/torch/_functorch/autograd_function.py:        # TODO: Update link to stable once that's out\n./.venv/lib/python3.13/site-packages/torch/_functorch/eager_transforms.py:        # TODO: Remove the following hack for namedtuples\n./.venv/lib/python3.13/site-packages/torch/_functorch/config.py:# TODO: turn on by default\n./.venv/lib/python3.13/site-packages/torch/_functorch/config.py:# TODO(ivankobzarev): Remove this config, being able to deduce it compile time.\n./.venv/lib/python3.13/site-packages/torch/_functorch/config.py:# TODO(ivankobzarev): Remove this config once extra memory usage is investigated.\n./.venv/lib/python3.13/site-packages/torch/_functorch/pyfunctorch.py:            # TODO: would be nice to assert that the layers are the same, but\n./.venv/lib/python3.13/site-packages/torch/_functorch/_activation_checkpointing/knapsack.py:    # TODO(chilli): I think if needed, this memory can be optimized with sliding\n./.venv/lib/python3.13/site-packages/torch/_functorch/partitioners.py:            # TODO: maybe use a different process group?\n./.venv/lib/python3.13/site-packages/torch/_functorch/partitioners.py:            # TODO: maybe use a different process group for this\n./.venv/lib/python3.13/site-packages/torch/_functorch/aot_autograd.py:# TODO: bikeshed on this name\n./.venv/lib/python3.13/site-packages/torch/_functorch/aot_autograd.py:                # TODO: Ensure that this codepath is never exercised from\n./.venv/lib/python3.13/site-packages/torch/_functorch/aot_autograd.py:    # TODO: Chillee argues that dynamo itself should pass in fake tensors to\n./.venv/lib/python3.13/site-packages/torch/_functorch/aot_autograd.py:                    # TODO: refactor the subclass path of run_functionalized_fw_and_collect_metadata\n./.venv/lib/python3.13/site-packages/torch/_functorch/aot_autograd.py:    # TODO(mlazos): Revisit if this is still needed. With Dynamo install ID\n./.venv/lib/python3.13/site-packages/torch/_functorch/aot_autograd.py:    # TODO: There is something deeply wrong here; compiled_fn running with\n./.venv/lib/python3.13/site-packages/torch/_functorch/aot_autograd.py:    # TODO: we might have to temporarily patch config.functionalize_rng\n./.venv/lib/python3.13/site-packages/torch/_functorch/aot_autograd.py:                # TODO(avik): Assigning all other types are allowed right now.\n./.venv/lib/python3.13/site-packages/torch/optim/radam.py:            # TODO(mlazos): we should try and get a foreach_where op https://github.com/pytorch/pytorch/issues/117884\n./.venv/lib/python3.13/site-packages/torch/optim/adam.py:            # TODO(crcrpar): [low prec params & their higher prec copy]\n./.venv/lib/python3.13/site-packages/torch/optim/adam.py:        # TODO: Support nonzero-dim Tensor betas, see #147921\n./.venv/lib/python3.13/site-packages/torch/optim/adam.py:    # TODO: Support nonzero-dim Tensor betas, see #147921\n./.venv/lib/python3.13/site-packages/torch/optim/_functional.py:# TODO: use foreach API in optim._functional to do all the computation\n./.venv/lib/python3.13/site-packages/torch/mtia/__init__.py:    # TODO: Update _accelerator_hooks_device_count to abstract a MTIA device count API\n./.venv/lib/python3.13/site-packages/torch/_inductor/__init__.py:        TODO: make it return a list by default\n./.venv/lib/python3.13/site-packages/torch/_inductor/compile_fx_ext.py:        # TODO: For memory purposes should we log to a file and then respond with that?\n./.venv/lib/python3.13/site-packages/torch/_inductor/compile_fx_ext.py:        # TODO: Do we need to figure out what changed in TracingContext in the\n./.venv/lib/python3.13/site-packages/torch/_inductor/compile_fx_ext.py:            # TODO: scuba record about not being able to do this?\n./.venv/lib/python3.13/site-packages/torch/_inductor/compile_fx_ext.py:            # TODO: Should we split the input into multiple sections where each\n./.venv/lib/python3.13/site-packages/torch/_inductor/compile_fx_ext.py:# TODO: make this a FxCompileMode value?\n./.venv/lib/python3.13/site-packages/torch/_inductor/output_code.py:    # TODO: Remove underscores here\n./.venv/lib/python3.13/site-packages/torch/_inductor/output_code.py:    # TODO: Get rid of this\n./.venv/lib/python3.13/site-packages/torch/_inductor/output_code.py:            # TODO: migrate all disable reasons to stack trace, refactor\n./.venv/lib/python3.13/site-packages/torch/_inductor/output_code.py:        # TODO - ordered set\n./.venv/lib/python3.13/site-packages/torch/_inductor/output_code.py:        # TODO: This could be better if we're ever able to serialize compiled\n./.venv/lib/python3.13/site-packages/torch/_inductor/constant_folding.py:        # TODO - fix errors with this\n./.venv/lib/python3.13/site-packages/torch/_inductor/constant_folding.py:        # TODO - constant folding triton kernel returns the inputs -- fix this\n./.venv/lib/python3.13/site-packages/torch/_inductor/constant_folding.py:        # TODO - more complicated strategy\n./.venv/lib/python3.13/site-packages/torch/_inductor/optimize_indexing.py:    # TODO - there are dominated uses whose dtype does not depend on whether\n./.venv/lib/python3.13/site-packages/torch/_inductor/optimize_indexing.py:                    # TODO - not sure if we should be doing int/float casts while tracing,\n./.venv/lib/python3.13/site-packages/torch/_inductor/optimize_indexing.py:    # TODO - if dominated node of one to_dtype is not expressible in int32,\n./.venv/lib/python3.13/site-packages/torch/_inductor/bounds.py:    # TODO: this is slightly inaccurate because truncdiv operates at integer\n./.venv/lib/python3.13/site-packages/torch/_inductor/package/package.py:            # TODO(angelayi): We shouldn't need to do this -- miniz should\n./.venv/lib/python3.13/site-packages/torch/_inductor/comms.py:    # TODO - experiment with whether this limit is useful, setting `len(snodes)` disables it\n./.venv/lib/python3.13/site-packages/torch/_inductor/comms.py:                # TODO - if the wait is for a collective that started before this collective or on another stream,\n./.venv/lib/python3.13/site-packages/torch/_inductor/comms.py:    TODO: Come up with a better approach\n./.venv/lib/python3.13/site-packages/torch/_inductor/comms.py:            # TODO: node_summary was written without FusedSchedulerNode in mind, generally needs to be hardened\n./.venv/lib/python3.13/site-packages/torch/_inductor/comms.py:    # TODO - this function probably doesn't do a very good job estimating the runtime because it doesn't carefully model\n./.venv/lib/python3.13/site-packages/torch/_inductor/comms.py:                ):  # TODO(yf225): implement replacement in kwargs\n./.venv/lib/python3.13/site-packages/torch/_inductor/mkldnn_lowerings.py:            ) and binary_attr == \"add\":  # <TODO> Support inplace sum fusion\n./.venv/lib/python3.13/site-packages/torch/_inductor/graph.py:            # TODO: this should not be needed once #93059 lands\n./.venv/lib/python3.13/site-packages/torch/_inductor/graph.py:            # TODO: make a dedicated UnknownSource for this?\n./.venv/lib/python3.13/site-packages/torch/_inductor/graph.py:            # TODO - get different values per hardware\n./.venv/lib/python3.13/site-packages/torch/_inductor/graph.py:            # TODO fix partitioning issue and re-enable for backward\n./.venv/lib/python3.13/site-packages/torch/_inductor/graph.py:            # TODO(jansel): handle input aliasing\n./.venv/lib/python3.13/site-packages/torch/_inductor/graph.py:                    # TODO: should really switch to \"needs_fixed_stride\" constraint on these\n./.venv/lib/python3.13/site-packages/torch/_inductor/graph.py:                # TODO: this is sus, it probably should be handled in the\n./.venv/lib/python3.13/site-packages/torch/_inductor/graph.py:                # TODO(jansel): introduce a store vs inline choice\n./.venv/lib/python3.13/site-packages/torch/_inductor/graph.py:        # TODO(Eikan): Only support mixing cpu and other device now.\n./.venv/lib/python3.13/site-packages/torch/_inductor/graph.py:        # TODO. Revisit this once the logging API is more mature\n./.venv/lib/python3.13/site-packages/torch/_inductor/template_heuristics.py:        # TODO: Unify with other gemm patterns, mm_plus_mm currently follows\n./.venv/lib/python3.13/site-packages/torch/_inductor/template_heuristics.py:        # TODO: _filter_configs can be removed once backend specific configs are added\n./.venv/lib/python3.13/site-packages/torch/_inductor/utils.py:    # TODO: There is a bug in a call to this function, to repro:\n./.venv/lib/python3.13/site-packages/torch/_inductor/utils.py:        # TODO: remove when support is added in triton\n./.venv/lib/python3.13/site-packages/torch/_inductor/utils.py:            # TODO(future): maybe refactor torch/fx/graph.py to make it easy to\n./.venv/lib/python3.13/site-packages/torch/_inductor/utils.py:    # TODO: Investigate why uint64 tensor creation causes overflow error:\n./.venv/lib/python3.13/site-packages/torch/_inductor/utils.py:        # TODO(rec): or should this be self.__class__(initial_indent=self._indent)?\n./.venv/lib/python3.13/site-packages/torch/_inductor/utils.py:    # TODO we need to properly guard on this global\n./.venv/lib/python3.13/site-packages/torch/_inductor/utils.py:        # TODO(dberard) remove this when we get AOTI support for new TMA APIs (#155047)\n./.venv/lib/python3.13/site-packages/torch/_inductor/utils.py:        and not V.graph.aot_mode  # TODO: Support AOTI for decomposeK\n./.venv/lib/python3.13/site-packages/torch/_inductor/utils.py:    # TODO(jgong5): support dynamic shapes for n or k\n./.venv/lib/python3.13/site-packages/torch/_inductor/utils.py:        and is_last_dim_stride1(mat1)  # TODO(jgong5): support transposed input\n./.venv/lib/python3.13/site-packages/torch/_inductor/utils.py:        # TODO: this is a temporary solution to ensure that we can identify torchrec's\n./.venv/lib/python3.13/site-packages/torch/_inductor/utils.py:    return device != \"mps\" and is_gpu(device)  # TODO: MPS does not expose streams now\n./.venv/lib/python3.13/site-packages/torch/_inductor/utils.py:    # TODO(voz): It would be nice to enable this assert, but there are lots of tests that\n./.venv/lib/python3.13/site-packages/torch/_inductor/utils.py:    # TODO(voz): Should we always have one anyway?\n./.venv/lib/python3.13/site-packages/torch/_inductor/utils.py:    # TODO: remove when support is added in triton\n./.venv/lib/python3.13/site-packages/torch/_inductor/utils.py:        \"or_\",  # TODO should remove this op\n./.venv/lib/python3.13/site-packages/torch/_inductor/utils.py:        # TODO: implement V3_BACKENDS_TUPLE\n./.venv/lib/python3.13/site-packages/torch/_inductor/memory.py:    # TODO: would be nice to remove the try/except block for both places\n./.venv/lib/python3.13/site-packages/torch/_inductor/lowering.py:# TODO(jansel): we should implement decomps or lowerings for these\n./.venv/lib/python3.13/site-packages/torch/_inductor/lowering.py:# TODO(rec): torch._higher_order_ops._foreach_map is not an OpOverload\n./.venv/lib/python3.13/site-packages/torch/_inductor/lowering.py:# TODO(jansel): ezyang says we won't need this in the future, try removing it\n./.venv/lib/python3.13/site-packages/torch/_inductor/lowering.py:    # TODO(jansel): add quantized types?\n./.venv/lib/python3.13/site-packages/torch/_inductor/lowering.py:        # TODO maybe we need to use pytrees here\n./.venv/lib/python3.13/site-packages/torch/_inductor/lowering.py:        # TODO: It would be better to realize the input if any of its sizes\n./.venv/lib/python3.13/site-packages/torch/_inductor/lowering.py:        # TODO <leslie> Remove this fallback when we support vectorization\n./.venv/lib/python3.13/site-packages/torch/_inductor/lowering.py:    # TODO: We observed negative performance impact of pointwise_cat optimization on CPU so disabled it.\n./.venv/lib/python3.13/site-packages/torch/_inductor/lowering.py:    # TODO: don't guard on static shape here\n./.venv/lib/python3.13/site-packages/torch/_inductor/lowering.py:        # TODO: delete once triton adds native support\n./.venv/lib/python3.13/site-packages/torch/_inductor/lowering.py:    # TODO: combine this with require_contiguous after\n./.venv/lib/python3.13/site-packages/torch/_inductor/lowering.py:    # TODO(jansel): memory format\n./.venv/lib/python3.13/site-packages/torch/_inductor/lowering.py:    # deferred_runtime_asserts, TODO: try this assert out\n./.venv/lib/python3.13/site-packages/torch/_inductor/lowering.py:    # TODO: use a masked store for this. currently only triton\n./.venv/lib/python3.13/site-packages/torch/_inductor/lowering.py:            # TODO: Need to support more reduction type\n./.venv/lib/python3.13/site-packages/torch/_inductor/lowering.py:    # TODO: Generalize to other max pooling flavors\n./.venv/lib/python3.13/site-packages/torch/_inductor/lowering.py:    # TODO: should we force these to be realized?\n./.venv/lib/python3.13/site-packages/torch/_inductor/lowering.py:    # TODO(jansel): should we force these to be realized?\n./.venv/lib/python3.13/site-packages/torch/_inductor/lowering.py:        # TODO: does inference need split online_softmax_reduce?\n./.venv/lib/python3.13/site-packages/torch/_inductor/cpu_vec_isa.py:    )  # TODO: use cflags\n./.venv/lib/python3.13/site-packages/torch/_inductor/cpu_vec_isa.py:    )  # TODO: use cflags\n./.venv/lib/python3.13/site-packages/torch/_inductor/cpu_vec_isa.py:    # TODO add sve256 support\n./.venv/lib/python3.13/site-packages/torch/_inductor/decomposition.py:    # TODO: check if XE4 still need this fallback\n./.venv/lib/python3.13/site-packages/torch/_inductor/decomposition.py:# TODO: for now, inductor doesn't handle asserts\n./.venv/lib/python3.13/site-packages/torch/_inductor/decomposition.py:    # TODO: Re-enable for mps once our reductions are performant enough\n./.venv/lib/python3.13/site-packages/torch/_inductor/decomposition.py:    # TODO: Re-enable for mps once our reductions are performant enough\n./.venv/lib/python3.13/site-packages/torch/_inductor/decomposition.py:    # TODO: _to_copy tensor to stride permutation\n./.venv/lib/python3.13/site-packages/torch/_inductor/decomposition.py:# TODO(aakhundov): replace this (and the above) Any by more\n./.venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:            # TODO (from reviewers as well)\n./.venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:                        # TODO: we should have intermediary var shapes\n./.venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:                # TODO(nmacchioni): fix sympy division by zero\n./.venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:        # TODO(AlnisM): Does tile_shape always exist?\n./.venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:        # TODO(jgong5): support multi-template on CPU\n./.venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:        # TODO - assert that we have not mutating kernels here\n./.venv/lib/python3.13/site-packages/torch/_inductor/mkldnn_ir.py:        # TODO <Leslie> cleaned up the fake_tensor trace as Linear implementation\n./.venv/lib/python3.13/site-packages/torch/_inductor/mkldnn_ir.py:    # TODO support channels_last for such zero stride input.\n./.venv/lib/python3.13/site-packages/torch/_inductor/async_compile.py:    TODO: remove after rollout.\n./.venv/lib/python3.13/site-packages/torch/_inductor/async_compile.py:        TODO: We store a LambdaFuture as that's the callable returned by async_compile.triton,\n./.venv/lib/python3.13/site-packages/torch/_inductor/async_compile.py:        TODO: Source code here is not just the kernel's source code, but also includes the inductor preamble, etc.\n./.venv/lib/python3.13/site-packages/torch/_inductor/tiling_utils.py:            # TODO - only need one of these to be solvable to zero\n./.venv/lib/python3.13/site-packages/torch/_inductor/tiling_utils.py:            # TODO: an earlier version for this code tried to iteratively try the maximum number\n./.venv/lib/python3.13/site-packages/torch/_inductor/tiling_utils.py:    # TODO - a few dynamic shapes issues to resolve\n./.venv/lib/python3.13/site-packages/torch/_inductor/tiling_utils.py:        # TODO - not handled well. indirect loads will not be coalesced,\n./.venv/lib/python3.13/site-packages/torch/_inductor/tiling_utils.py:        # TODO - will the names for all the inputs/outputs accurately\n./.venv/lib/python3.13/site-packages/torch/_inductor/tiling_utils.py:    # TODO - deduplicate with candidate_tilings\n./.venv/lib/python3.13/site-packages/torch/_inductor/tiling_utils.py:        # TODO - reason about indirect vars\n./.venv/lib/python3.13/site-packages/torch/_inductor/tiling_utils.py:    # TODO: separate into dataclass that olds mem, dtype, is_write\n./.venv/lib/python3.13/site-packages/torch/_inductor/tiling_utils.py:            # TODO - if a var is in the middle, such as [n0, n1, n2]\n./.venv/lib/python3.13/site-packages/torch/_inductor/tiling_utils.py:    # TODO - for strictly pointwise fusions,\n./.venv/lib/python3.13/site-packages/torch/_inductor/tiling_utils.py:    # TODO - could also prefer index var splits to reduction, better tested\n./.venv/lib/python3.13/site-packages/torch/_inductor/freezing.py:    # TODO (tmanlaibaatar) figure out why this is different\n./.venv/lib/python3.13/site-packages/torch/_inductor/freezing.py:    # TODO - further restrict cse ? right now needed to dedup aliasing ops\n./.venv/lib/python3.13/site-packages/torch/_inductor/config.py:    TODO: Remove when parallel compiled is fully enabled internally. For rollout, use a\n./.venv/lib/python3.13/site-packages/torch/_inductor/config.py:# TODO: Set directly after internal rollout.\n./.venv/lib/python3.13/site-packages/torch/_inductor/config.py:# TODO: remove later\n./.venv/lib/python3.13/site-packages/torch/_inductor/config.py:    # TODO - need to debug why this prevents cleanup\n./.venv/lib/python3.13/site-packages/torch/_inductor/config.py:    # TODO - enable by default\n./.venv/lib/python3.13/site-packages/torch/_inductor/config.py:    # TODO: Move this into metadata\n./.venv/lib/python3.13/site-packages/torch/_inductor/config.py:    # TODO: Move this into metadata\n./.venv/lib/python3.13/site-packages/torch/_inductor/config.py:    # TODO: Move this somewhere else, since it's no longer really a config\n./.venv/lib/python3.13/site-packages/torch/_inductor/cudagraph_trees.py:    # TODO - remove, prevents cleanup\n./.venv/lib/python3.13/site-packages/torch/_inductor/cudagraph_trees.py:    TODO: in the future, we would like to do the following once storage weak refs land\n./.venv/lib/python3.13/site-packages/torch/_inductor/cudagraph_trees.py:        # TODO - when issue #91395 is landed, we can set a weakref on\n./.venv/lib/python3.13/site-packages/torch/_inductor/cudagraph_trees.py:        # TODO: register_generator_state should potentially take explicit device\n./.venv/lib/python3.13/site-packages/torch/_inductor/cudagraph_trees.py:        # TODO: - should we make the storage resizable ?\n./.venv/lib/python3.13/site-packages/torch/_inductor/cudagraph_trees.py:            lambda: \"TODO: graph recording observed an input tensor deallocate during graph \"\n./.venv/lib/python3.13/site-packages/torch/_inductor/cudagraph_trees.py:    # TODO: make generation increment configurable, warn on overwrite.\n./.venv/lib/python3.13/site-packages/torch/_inductor/cudagraph_trees.py:        # TODO: we could also allow the these weak refs to continue to be allocated,\n./.venv/lib/python3.13/site-packages/torch/_inductor/fuzzer.py:# TODO this needs to be indexed to the module, like inductor or dynamo, for name collisions\n./.venv/lib/python3.13/site-packages/torch/_inductor/fuzzer.py:    # TODO support more dimensions\n./.venv/lib/python3.13/site-packages/torch/_inductor/autoheuristic/autoheuristic.py:                # TODO(AlnisM): We might want to allow this in the future\n./.venv/lib/python3.13/site-packages/torch/_inductor/autoheuristic/autoheuristic.py:        # TODO(AlnisM): just using the device name for now, but the same GPU model can have different names\n./.venv/lib/python3.13/site-packages/torch/_inductor/autoheuristic/autoheuristic.py:                # TODO: Find a nicer way to handle this\n./.venv/lib/python3.13/site-packages/torch/_inductor/autoheuristic/autoheuristic_utils.py:        # TODO(AlnisM): there might be a better way to do this\n./.venv/lib/python3.13/site-packages/torch/_inductor/cpp_builder.py:        # TODO(T203137008) Can we unify these flags with triton_cc_command?\n./.venv/lib/python3.13/site-packages/torch/_inductor/cpp_builder.py:                # TODO: fix issue, can't find omp.h\n./.venv/lib/python3.13/site-packages/torch/_inductor/cpp_builder.py:        # TODO: make this work beyond CUDA\n./.venv/lib/python3.13/site-packages/torch/_inductor/choices.py:        # TODO(jansel): should this default on for dynamic shapes?\n./.venv/lib/python3.13/site-packages/torch/_inductor/choices.py:            # TODO the best heuristic currently has XBLOCK (corresponding to numel_hint) 128\n./.venv/lib/python3.13/site-packages/torch/_inductor/index_propagation.py:            # TODO Perhaps move this logic to the simplify indexing pass\n./.venv/lib/python3.13/site-packages/torch/_inductor/scheduler.py:            # TODO - would be nice if we could just cache accesses on ReadWrites,\n./.venv/lib/python3.13/site-packages/torch/_inductor/scheduler.py:                        # but TODO this might be a convenient place to signal to the Collective kernels to inplace\n./.venv/lib/python3.13/site-packages/torch/_inductor/scheduler.py:            # TODO(voz): Should the pragma be constant somewhere?\n./.venv/lib/python3.13/site-packages/torch/_inductor/scheduler.py:        # TODO(voz): Ostensibly, we should not need this. But there are cases where C++ codegen does\n./.venv/lib/python3.13/site-packages/torch/_inductor/scheduler.py:                            # TODO: Figure out what's going on\n./.venv/lib/python3.13/site-packages/torch/_inductor/scheduler.py:        # TODO(xmfan): find a better heuristic to model FLOPS/latency relationship\n./.venv/lib/python3.13/site-packages/torch/_inductor/scheduler.py:    # TODO when we drop support for Python < 3.10, we can use\n./.venv/lib/python3.13/site-packages/torch/_inductor/scheduler.py:            # TODO(shunting) if this cause compilation time increase when\n./.venv/lib/python3.13/site-packages/torch/_inductor/scheduler.py:            # TODO support benchmarking epilogue fusion\n./.venv/lib/python3.13/site-packages/torch/_inductor/scheduler.py:                # TODO: Remove this check after all Triton templates support prologue fusion.\n./.venv/lib/python3.13/site-packages/torch/_inductor/scheduler.py:        # TODO Don't do loop reordering for CPU for now.\n./.venv/lib/python3.13/site-packages/torch/_inductor/scheduler.py:        # TODO - make configurable per input, for instance, bias can fuse fp32 -> fp16 profitably\n./.venv/lib/python3.13/site-packages/torch/_inductor/scheduler.py:        # TODO - would be nice to generalize this, however, we would need more explicit\n./.venv/lib/python3.13/site-packages/torch/_inductor/autotune_process.py:        # TODO(jgong5): use CppPythonBindingsCodeCache for better binding perf\n./.venv/lib/python3.13/site-packages/torch/_inductor/compile_fx.py:        # TODO: This is a hack purely to get some info to extract_tensor_metadata_for_cache_key,\n./.venv/lib/python3.13/site-packages/torch/_inductor/compile_fx.py:        # TODO: this time will be slightly inconsistent with the one computed\n./.venv/lib/python3.13/site-packages/torch/_inductor/compile_fx.py:        # TODO: add remote cache get/put timings here too\n./.venv/lib/python3.13/site-packages/torch/_inductor/compile_fx.py:    # TODO: We should probably eventually add some kind of async version of this\n./.venv/lib/python3.13/site-packages/torch/_inductor/compile_fx.py:        # TODO: _CompileFxKwargs actually has stronger types than in the\n./.venv/lib/python3.13/site-packages/torch/_inductor/compile_fx.py:            # TODO: Should we actually dump this?  It should be redundant with the aot\n./.venv/lib/python3.13/site-packages/torch/_inductor/compile_fx.py:                    # TODO: Remove this when 3.9 is no longer supported\n./.venv/lib/python3.13/site-packages/torch/_inductor/compile_fx.py:                        # TODO(T216453900): need to work around for now to support vllm\n./.venv/lib/python3.13/site-packages/torch/_inductor/compile_fx.py:                    # TODO: The switching between AOT mode and not here is a bit\n./.venv/lib/python3.13/site-packages/torch/_inductor/compile_fx.py:                    # TODO: Hoist this above V.aot_compilation\n./.venv/lib/python3.13/site-packages/torch/_inductor/compile_fx.py:                    # TODO - could make one single op of multiple slices\n./.venv/lib/python3.13/site-packages/torch/_inductor/compile_fx.py:        \"triton.cudagraphs\": False,  # TODO: to be removed\n./.venv/lib/python3.13/site-packages/torch/_inductor/compile_fx.py:    # TODO: This probably shouldn't be a recursive call\n./.venv/lib/python3.13/site-packages/torch/_inductor/compile_fx.py:        # TODO: Get rid of this?\n./.venv/lib/python3.13/site-packages/torch/_inductor/compile_fx.py:        # TODO: Move this before recursive pre-grad passes\n./.venv/lib/python3.13/site-packages/torch/_inductor/compile_fx.py:        # TODO: The modern style is to use CompileId from TracingContext to\n./.venv/lib/python3.13/site-packages/torch/_inductor/pattern_matcher.py:    # TODO: Revisit the functionalize_rng_ops for lowmem dropout\n./.venv/lib/python3.13/site-packages/torch/_inductor/pattern_matcher.py:    # TODO - fix schema\n./.venv/lib/python3.13/site-packages/torch/_inductor/pattern_matcher.py:    # TODO - look into using aot autograd, asserting no mutating ops here\n./.venv/lib/python3.13/site-packages/torch/_inductor/pattern_matcher.py:# TODO: remove in follow up diff, used internally\n./.venv/lib/python3.13/site-packages/torch/_inductor/runtime/benchmarking.py:        # TODO(nmacchioni): For non-CPU functions we default to using the GPU-specific benchmarking\n./.venv/lib/python3.13/site-packages/torch/_inductor/runtime/halide_helpers.py:    # TODO:\n./.venv/lib/python3.13/site-packages/torch/_inductor/runtime/coordinate_descent_tuner.py:    TODO will it be necessary to tune multiple fields simultaneously.\n./.venv/lib/python3.13/site-packages/torch/_inductor/runtime/coordinate_descent_tuner.py:    TODO: what if both increasing and decreasing a field can improve perf.\n./.venv/lib/python3.13/site-packages/torch/_inductor/runtime/hints.py:                # TODO: Fetch the actual value from ioreg\n./.venv/lib/python3.13/site-packages/torch/_inductor/runtime/static_cuda_launcher.py:            # TODO handle nvTmaDesc/CUtensormap\n./.venv/lib/python3.13/site-packages/torch/_inductor/runtime/static_cuda_launcher.py:        # TODO: actually, if the args *don't* match, we probably should\n./.venv/lib/python3.13/site-packages/torch/_inductor/runtime/static_cuda_launcher.py:        # TODO: can handle grid functions here or in C++, so\n./.venv/lib/python3.13/site-packages/torch/_inductor/runtime/triton_helpers.py:    # TODO(isuruf): use inline_asm_elementwise here\n./.venv/lib/python3.13/site-packages/torch/_inductor/runtime/triton_helpers.py:    # TODO(jansel): is this needed?\n./.venv/lib/python3.13/site-packages/torch/_inductor/runtime/triton_heuristics.py:        # TODO(jansel): we should find a way to move this extra compile into the worker process\n./.venv/lib/python3.13/site-packages/torch/_inductor/runtime/triton_heuristics.py:        # TODO: should we just load the kernels ahead of time if we know we're going to call this?\n./.venv/lib/python3.13/site-packages/torch/_inductor/runtime/triton_heuristics.py:        # TODO(jansel): need to fixup src.fn which is now None\n./.venv/lib/python3.13/site-packages/torch/_inductor/runtime/triton_heuristics.py:            # TODO(jansel): delete this branch in mid-2025\n./.venv/lib/python3.13/site-packages/torch/_inductor/runtime/triton_heuristics.py:    )  # TODO: query warp size once #129663 is merged\n./.venv/lib/python3.13/site-packages/torch/_inductor/runtime/triton_heuristics.py:        # TODO: this may only be beneficial when each iteration of the reduction\n./.venv/lib/python3.13/site-packages/torch/_inductor/runtime/triton_heuristics.py:    # TODO(jansel): we should base target on the SM count of the local GPU\n./.venv/lib/python3.13/site-packages/torch/_inductor/runtime/triton_heuristics.py:    # TODO(jansel): add more configs in max_autotune\n./.venv/lib/python3.13/site-packages/torch/_inductor/runtime/triton_heuristics.py:    # TODO(jansel): we should be able to improve these heuristics\n./.venv/lib/python3.13/site-packages/torch/_inductor/runtime/autotune_cache.py:        # TODO: Do we need to compute time_taken_ms and encode that somehow?\n./.venv/lib/python3.13/site-packages/torch/_inductor/runtime/autotune_cache.py:        # TODO: The autotune cache includes configs_hash in the key. The problem\n./.venv/lib/python3.13/site-packages/torch/_inductor/runtime/autotune_cache.py:            # TODO: check cache_dir() vs filename, then strip dirname\n./.venv/lib/python3.13/site-packages/torch/_inductor/codecache.py:        # TODO: pickler.fast is technically deprecated. Will this work on new python versions?\n./.venv/lib/python3.13/site-packages/torch/_inductor/codecache.py:            # TODO: These tensors don't currently pickle, so we can't cache a compiled\n./.venv/lib/python3.13/site-packages/torch/_inductor/codecache.py:    # TODO(masnesral): Investigate whether it's beneficial to store compiled graphs\n./.venv/lib/python3.13/site-packages/torch/_inductor/codecache.py:        # TODO (benjaminglass1): the CMake packaging path doesn't support linking files\n./.venv/lib/python3.13/site-packages/torch/_inductor/codecache.py:            # TODO: Fix mmap weights with cuda\n./.venv/lib/python3.13/site-packages/torch/_inductor/codecache.py:                    \"TODO: add emit_multi_arch_kernel support for cutlass kernels\"\n./.venv/lib/python3.13/site-packages/torch/_inductor/codecache.py:                    # TODO: unify to always use mmap_weights\n./.venv/lib/python3.13/site-packages/torch/_inductor/codecache.py:        # TODO: Make the typing hint strong here\n./.venv/lib/python3.13/site-packages/torch/_inductor/ir.py:        TODO(ezyang): I think, in principle, every IRNode should have an\n./.venv/lib/python3.13/site-packages/torch/_inductor/ir.py:            # TODO this will fail for something like ((1, N) * (N, 1)).sum()\n./.venv/lib/python3.13/site-packages/torch/_inductor/ir.py:            # TODO determine splits when all inputs are broadcast\n./.venv/lib/python3.13/site-packages/torch/_inductor/ir.py:        # TODO(jansel): realize the reduction so we can do dynamic indexing\n./.venv/lib/python3.13/site-packages/torch/_inductor/ir.py:        # TODO: Unrolled reduction\n./.venv/lib/python3.13/site-packages/torch/_inductor/ir.py:        # TODO: Can combine_fn/reindex close over unbacked symbols? If so, we\n./.venv/lib/python3.13/site-packages/torch/_inductor/ir.py:        # TODO: custom splitting heuristic for scan\n./.venv/lib/python3.13/site-packages/torch/_inductor/ir.py:        # TODO: a new class for FixedTransferLayout that output layout is constrained by input layout\n./.venv/lib/python3.13/site-packages/torch/_inductor/ir.py:                # TODO: unbacked should not diverge from backed in determining striding\n./.venv/lib/python3.13/site-packages/torch/_inductor/ir.py:        # TODO: These symbols may not escape, if they don't assert so and\n./.venv/lib/python3.13/site-packages/torch/_inductor/ir.py:        TODO(jansel): A better algorithm here would look at downstream consumers of this\n./.venv/lib/python3.13/site-packages/torch/_inductor/ir.py:            # TODO(jansel): impose layout preference on realized buffer\n./.venv/lib/python3.13/site-packages/torch/_inductor/ir.py:        # TODO - Storage to InputBuffer\n./.venv/lib/python3.13/site-packages/torch/_inductor/ir.py:        # TODO: could also be good to have a codegen fix to recognize overlapping elements\n./.venv/lib/python3.13/site-packages/torch/_inductor/ir.py:        # TODO move this to the more proper places\n./.venv/lib/python3.13/site-packages/torch/_inductor/ir.py:        # TODO: combine this with require_contiguous after\n./.venv/lib/python3.13/site-packages/torch/_inductor/ir.py:        # TODO: I can't tell if the symbols here are temporary\n./.venv/lib/python3.13/site-packages/torch/_inductor/ir.py:        assert isinstance(new_size, int), \"TODO: dynamic shapes\"\n./.venv/lib/python3.13/site-packages/torch/_inductor/ir.py:            # TODO: when we start compiling in C++20, annotate with [[unlikely]].\n./.venv/lib/python3.13/site-packages/torch/_inductor/ir.py:        # TODO(anijain2305) - Support sym expr as operands in future.\n./.venv/lib/python3.13/site-packages/torch/_inductor/ir.py:    # TODO(yifu): add a pre-grad pass to validate the correctness of collective\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/mps.py:        # TODO: This is only accurate up to 2**23\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/mps.py:        # TODO: Type annotation for other is wrong, it's often float or int\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/mps.py:        # TODO: Does it rely on undefined behavior?\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/mps.py:            # TODO(NS): Figure out the right balance between optype casts\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/mps.py:            device=torch.device(\"cpu\"),  # TODO: Fix me, MPS does not expose streams now\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/mps.py:        # TODO(malfet): support asserts\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/mps.py:        # TODO(malfet): Is upper bound inclusive or exclusive?\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/mps.py:            # TODO: Merge multiple kernels into a single library\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/debug_utils.py:        # TODO: Find a more reliable way to detect kernel args types to print for extern kernel calls\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/simd.py:        # TODO instead of trying to blindly find complicated exprs, we should hoist the\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/simd.py:            # TODO(jansel): do we need a reshape here?\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/simd.py:                    # TODO - use split ranges ?\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/simd.py:                    # TODO - this doesn't work with libdevice calls, potentially other bugs\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/simd.py:            # TODO: Maybe unify CUDATemplateKernel to also use PartialRender for flexible epilogue fusion.\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/simd.py:            # TODO: incorporate exact bitwidth, and read/write\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/simd.py:        # TODO, add tests, reduction splits if config.triton.tile_reductions\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/simd.py:        # TODO: we should ignore tiny increases in score for extra splits\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/simd.py:            # TODO - look into, occurs with dynamic shapes often\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/simd.py:        # # TODO: enable by default\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/cpp_utils.py:        # TODO: why are people passing strings to the printer here :think:\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/cpp_utils.py:            # TODO: Add support of fusion when the read of template buffer and the write of epilogue output\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/triton_combo_kernel.py:            # TODO benchmark the performance when large pointwise nodes combining with others\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/triton_combo_kernel.py:        # TODO support combination of kernels with different block dimensions\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/triton_combo_kernel.py:            # TODO: we assume all sub_kernels have the same block size\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/triton_combo_kernel.py:        # TODO: is it correct to use the first sub kernel's heuristics?\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/triton.py:    # TODO: This is wrong, when lhs, rhs > 2**53, Python does a higher\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/triton.py:    # TODO - register these ops as having divergent dtype\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/triton.py:            # TODO: we are not always consistent in enforcing that the output of the index expr printing\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/triton.py:        assert not self.cooperative_reduction, \"TODO\"\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/triton.py:        assert not self.cooperative_reduction, \"TODO\"\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/triton.py:        # TODO(jansel): if there are constants, we shouldn't bother passing them as args\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/triton.py:            # TODO(voz): Ostensibly, we should not need this. But there are cases where C++ codegen does\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/triton.py:                # TODO - would be better as a hook in triton do_bench that reset\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/triton.py:            # TODO(jansel): scan does not yet work with cooperative reductions\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/common.py:        # TODO: why are people passing strings to the printer here :think:\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/common.py:        # TODO: this is wrong\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/common.py:        # TODO: an easy bandaid is to generate runtime asserts that it's\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/cpp_wrapper_gpu.py:        # TODO - support subgraph codegen by lifting functions. Check the\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/cpp_wrapper_gpu.py:            # TODO: This is added because FC. Remove this once the newly added shim symbols,\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/memory_planning.py:            # TODO(jansel): we could try harder here by merging overlapping in space\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/memory_planning.py:                # TODO(jansel): we should support reusing buffers created via ExternKernelAlloc\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/cpp_gemm_template.py:        TODO(jgong5): allow tuning various blocking options\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/cpp_gemm_template.py:            # TODO: tune the factor here\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/cpp_gemm_template.py:                # TODO: Decouple the choice of micro-kernel from cache blocking\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/cpp_gemm_template.py:            min_Mc_ratio = 2  # TODO(jgong5): something to tune?\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/cpp_gemm_template.py:            # TODO(jgong5): perhaps use size hint to decide?\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/cpp_gemm_template.py:        # TODO(jgong5): decide proper number of threads per problem size\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/cpp_gemm_template.py:                # TODO: Move VNNI weight packing for non-constant tensors into the template,\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/cpp_gemm_template.py:        # TODO(jgong5): for int8 gemm, bias-add is handled outside of gemm template,\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/cpp_wrapper_cpu_array_ref.py:        # TODO - support subgraph codegen by lifting functions. Check the\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/cpp_wrapper_cpu_array_ref.py:                        # TODO: input shape checking for regular tensor interface as well?\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/cpp_wrapper_cpu_array_ref.py:        # TODO: integrate memory planning & stack allocation?\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/cpp_wrapper_cpu_array_ref.py:            # TODO: this seems legit, NullLine has no node\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/cpp_wrapper_cpu_array_ref.py:        # TODO: consider remove \"_out\" and add missing inplace variants to fallback_ops.py\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/cpp_wrapper_cpu_array_ref.py:        # TODO: update aoti_torch_index_put_out in ir.py to use autogen out version\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/wrapper.py:# TODO: Move to a well known place\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/wrapper.py:            # TODO(aakhundov): the sorting below is generally not sufficient, so\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/wrapper.py:            # TODO: this seems legit, NullLine has no node\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/wrapper.py:                    # TODO: this fallback and those below actually will generate possibly\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/wrapper.py:            # TODO(aakhundov): add None args to constants, too. currently, this\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/wrapper.py:        # This is handled in `generate_args_decl` which has a correct comment of: TODO: only works for\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/wrapper.py:                    # TODO: need to assert divisibility\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/wrapper.py:                    # TODO: this is invalid C++ codegen\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/wrapper.py:        # TODO (desertfire) - This function is the old way of supporting\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/wrapper.py:        # TODO: Uncomment in future. This will be needed to support subgraph\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/wrapper.py:        # TODO: Uncomment in future. This will be needed to support subgraph\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/cpp_wrapper_cpu.py:        # TODO - support subgraph codegen by lifting functions. Check the\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/cpp_wrapper_cpu.py:            # TODO: this could be auto-generated from a passed-in custom op schema\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/cpp_wrapper_cpu.py:                # TODO: handle integer output (e.g., as in attention)\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/cpp_wrapper_cpu.py:        # TODO: consider remove \"_out\" and add missing inplace variants to fallback_ops.py\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/cpp_wrapper_cpu.py:        # TODO: update aoti_torch_index_put_out in ir.py to use autogen out version\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/cpp_wrapper_cpu.py:            # TODO: assert divisibility here\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/cpp_wrapper_cpu.py:            # TODO: Add buf name directly into check_inf_and_nan.\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/cpp_wrapper_cpu.py:        # TODO (desertfire) - This function is the old way of supporting\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/cpp_wrapper_cpu.py:        # TODO: Only support None and tensor(s) returns for now, SymInt is not implemented yet\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/cpp_wrapper_cpu.py:        # TODO: need to support control flow\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/cpp_wrapper_cpu.py:        # TODO: not using type_ as the first step of refactoring. Will update this later.\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/halide.py:        # TODO(jansel): find a better way to do this, builtin % has wrong sign\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/halide.py:        # TODO(jansel): find a better ways to do this, the select-based trick from triton.py didn't work\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/halide.py:        # TODO(jansel): Halide only supports 32-bit indexing, we should error on overflow\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/halide.py:        # TODO(jansel): look into removing the where in the same places triton does\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/halide.py:                        # TODO(jansel): we should just prevent fusion in cases that hit this\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/halide.py:            ):  # TODO(jansel): negative offsets\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/halide.py:            # TODO(jansel): implement welford_reduce without fallback\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/halide.py:                # TODO(jansel): explore other flags, see:\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/halide.py:            # TODO(jansel): it is unclear if this does anything, since input sizes are still int32\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/halide.py:        return False  # TODO(jansel): support asserts\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/halide.py:        pass  # TODO(jansel): support asserts\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/cpp.py:    # TODO: this seems to be dead\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/cpp.py:                # TODO: avoid hard-code torch.float\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/cpp.py:            # TODO: should we consider load mask here?\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/cpp.py:            self._load_mask is None  # TODO: support transposition with mask\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/cpp.py:        # TODO(jgong5): support alternative tiling factors and data types\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/cpp.py:            # <TODO> This should be removed after full support for vectorization is implemented.\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/cpp.py:        # TODO(leslie-fang-intel): only enable parallel within all outer loop levels.\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/cpp.py:        # TODO(jansel): allow fusion pointwise (vars1, ()) suffix?\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/cpp.py:        # TODO: we can extend fusion support with compatible ranges for FusedSchedulerNode\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/cpp.py:        # TODO: we can fix if it allows us to CSE at least one of the variables\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/cpp.py:            # TODO(jgong5): support pre-op fusion with template\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/cpp.py:        # TODO(voz): Ostensibly, we should not need this. But there are cases where C++ codegen does\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/cpp.py:        # TODO: support kernel profile on other platforms\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/cpp.py:            # TODO(jansel): look into chunk size and other schedules\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/cuda/cutlass_python_evt.py:        # TODO mlazos: relax this, cutlass supports reductions and other ops\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/cuda/gemm_template.py:        # TODO: update epilogue functor according to epilogues.\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/cuda/cuda_kernel.py:        TODO: Will add needed args to pass it in if it is dynamic.\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/cuda/cuda_kernel.py:        TODO: Will add needed args to pass it in if it is dynamic.\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/cuda/cuda_kernel.py:        TODO: Will add needed args to pass it in if it is dynamic.\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/cuda/cutlass_utils.py:    # TODO(ipiszy): remove this hack when CUTLASS solves Python scripts packaging structure issues.\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/cuda/cutlass_utils.py:    # TODO(mlazos): epilogue visitor tree currently lives in python/cutlass,\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/cuda/cutlass_utils.py:    # TODO: these three look dead?\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/cpp_template.py:        # TODO: add c10::ForcedUnroll test to test_aoti_abi_check\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/triton_utils.py:        # TODO: Remove fp8 special handling when Triton supports PyTorch fp8 dtypes.\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/triton_utils.py:            # TODO(voz): These are kinda redundant, if we can solve out statically_known_multiple_of with\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/cpp_flex_attention_template.py:# TODO: reuse cpp codegen to generate below pointwise/reduction kernels\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/cpp_flex_attention_template.py:        // TODO: reduce the number of calls of q_idx and kv_idx initialization\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/cpp_flex_attention_template.py:        # TODO: use inductor IR to rewrite those fusions\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/cpp_flex_attention_template.py:        # TODO: make them general for common bmm templates\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/cpp_micro_gemm.py:    # TODO(jgong5): support constant shapes and lds as template args.\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/cpp_micro_gemm.py:    // TODO(jgong5): loop unroll for M and N\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/cpp_micro_gemm.py:    # TODO add trans_b support for other micro gemms\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/cpp_micro_gemm.py:            # TODO supports tuning of sub_block_m/sub_block_n\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/cpp_micro_gemm.py:    // TODO(jgong5): loop unroll for M and N\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/cpp_micro_gemm.py:    // TODO(jgong5): add prefetch hint for A, B, C\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/cpp_micro_gemm.py:    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/cpp_micro_gemm.py:    # TODO: support float/half input\n./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/cpp_micro_gemm.py:    # TODO(jgong5): allow autotuning on choices of configs\n./.venv/lib/python3.13/site-packages/torch/_inductor/sizevars.py:                # TODO(jansel): should we use sympy.diff here?\n./.venv/lib/python3.13/site-packages/torch/_inductor/dependencies.py:            # TODO(jansel): explore this further normalization\n./.venv/lib/python3.13/site-packages/torch/_inductor/dependencies.py:# TODO: check call sites\n./.venv/lib/python3.13/site-packages/torch/_inductor/virtualized.py:   TODO: Define a parent class / protocol that defines all of the operations\n./.venv/lib/python3.13/site-packages/torch/_inductor/virtualized.py:            # TODO: To be honest, I feel we probably should just error in this\n./.venv/lib/python3.13/site-packages/torch/_inductor/virtualized.py:)  # TODO: improve type\n./.venv/lib/python3.13/site-packages/torch/_inductor/dtype_propagation.py:        # TODO - we avoid calling this in codegen, needs work for non codegen use cases\n./.venv/lib/python3.13/site-packages/torch/_inductor/dtype_propagation.py:        # TODO - TODO - rationalize index_expr. The dtype is not always used and we are inconsistent about int32 or int64\n./.venv/lib/python3.13/site-packages/torch/_inductor/dtype_propagation.py:        # TODO - need to handle multiple outputs\n./.venv/lib/python3.13/site-packages/torch/_inductor/dtype_propagation.py:        # TODO - way of registering dtype for op in backend\n./.venv/lib/python3.13/site-packages/torch/_inductor/kernel/mm.py:        # TODO (paulzhan): There is no template that exists for bias and TMA\n./.venv/lib/python3.13/site-packages/torch/_inductor/kernel/mm.py:        # TODO: is there a cleaner way to ensure aten.mm is always included?\n./.venv/lib/python3.13/site-packages/torch/_inductor/kernel/flex_attention.py:    qk = tl.dot(q, k, input_precision=FLOAT32_PRECISION) # TODO: use cuda matmul when q_len <= 2.\n./.venv/lib/python3.13/site-packages/torch/_inductor/kernel/flex_attention.py:        # TODO: This does not work if DQ is not the same layout as Q (for example,\n./.venv/lib/python3.13/site-packages/torch/_inductor/kernel/flex_attention.py:            # TODO: This does not work if DQ is not the same layout as Q (for example,\n./.venv/lib/python3.13/site-packages/torch/_inductor/kernel/flex_attention.py:# TODO: We probably also need a layout constraint?\n./.venv/lib/python3.13/site-packages/torch/_inductor/kernel/bmm.py:        # TODO: add out_dtype support for Triton Template\n./.venv/lib/python3.13/site-packages/torch/_inductor/kernel/mm_plus_mm.py:        # TODO(jansel): support different K values when this is fixed:\n./.venv/lib/python3.13/site-packages/torch/_inductor/kernel/conv.py:    # TODO: check if it's beneficial to convert Conv1d to Conv2d and then\n./.venv/lib/python3.13/site-packages/torch/_inductor/kernel/conv.py:        # TODO maybe we can convert weights to channels last just once before\n./.venv/lib/python3.13/site-packages/torch/_inductor/kernel/conv.py:                    # TODO(jansel): try unroll for bigger kernels once fixed:\n./.venv/lib/python3.13/site-packages/torch/_inductor/kernel/conv.py:                    # TODO(jansel): try unroll for bigger kernels once fixed:\n./.venv/lib/python3.13/site-packages/torch/_inductor/kernel/flex_decoding.py:    # TODO: workload evening at runtime for splits fully masked out.\n./.venv/lib/python3.13/site-packages/torch/_inductor/kernel/flex_decoding.py:    # TODO: Fix flex decoding non-divisible case!\n./.venv/lib/python3.13/site-packages/torch/_inductor/kernel/flex_decoding.py:    # TODO: fix autotuning.\n./.venv/lib/python3.13/site-packages/torch/_inductor/kernel/flex_decoding.py:    # TODO: This feels sketchy\n./.venv/lib/python3.13/site-packages/torch/_inductor/ops_handler.py:    # TODO: Better explain how the \"collective\" semantics of these ops;\n./.venv/lib/python3.13/site-packages/torch/_inductor/ops_handler.py:    # TODO: in practice, this seems to actually return None, but not returning\n./.venv/lib/python3.13/site-packages/torch/_inductor/ops_handler.py:        # TODO: Improve the description with some pseudocode\n./.venv/lib/python3.13/site-packages/torch/_inductor/compile_fx_async.py:        # TODO: If the future ended in an exception do we want to continue\n./.venv/lib/python3.13/site-packages/torch/_inductor/compile_fx_subproc.py:        # TODO: Do we need to copy across some kind of logging IDs? (ChromiumEventLogger)\n./.venv/lib/python3.13/site-packages/torch/_inductor/compile_fx_subproc.py:        # TODO: This is probably the wrong thing to do long-term - but for now\n./.venv/lib/python3.13/site-packages/torch/_inductor/compile_fx_subproc.py:            # TODO: Consider raising this limit if we start using async w/\n./.venv/lib/python3.13/site-packages/torch/_inductor/compile_fx_subproc.py:        # TODO: In subprocess mode we need to clear the inductor caches.\n./.venv/lib/python3.13/site-packages/torch/_inductor/compile_fx_subproc.py:        # TODO: We probably should be using a separate tmpdir in the worker\n./.venv/lib/python3.13/site-packages/torch/_inductor/compile_fx_subproc.py:        # TODO: We could be less aggressive by keeping a clock which gets\n./.venv/lib/python3.13/site-packages/torch/_inductor/compile_fx_subproc.py:        # TODO: turn off config.fx_graph_async_compile\n./.venv/lib/python3.13/site-packages/torch/_inductor/fx_passes/pre_grad.py:            # TODO: move efficient_conv_bn_eval_pass to the fusions dict too.\n./.venv/lib/python3.13/site-packages/torch/_inductor/fx_passes/pre_grad.py:                # TODO: support kwargs.\n./.venv/lib/python3.13/site-packages/torch/_inductor/fx_passes/pad_mm.py:    # TODO - finetune coefficient here. As a reference point, Triton mm model assumes\n./.venv/lib/python3.13/site-packages/torch/_inductor/fx_passes/pad_mm.py:    # can be planned layout transform is not free. TODO - way to pad and preserve layout ?\n./.venv/lib/python3.13/site-packages/torch/_inductor/fx_passes/pad_mm.py:    # TODO - see issue https://github.com/pytorch/pytorch/issues/128889\n./.venv/lib/python3.13/site-packages/torch/_inductor/fx_passes/pad_mm.py:    # TODO: Build a learned model which would be better than this heuristic\n./.venv/lib/python3.13/site-packages/torch/_inductor/fx_passes/post_grad.py:        # TODO: the pattern can be updated to support the case that index tensor\n./.venv/lib/python3.13/site-packages/torch/_inductor/fx_passes/post_grad.py:    TODO: Right now the scatter value must be a scalar. But we could support it\n./.venv/lib/python3.13/site-packages/torch/_inductor/fx_passes/post_grad.py:    # TODO: to support other reductions like sum, would need to skip\n./.venv/lib/python3.13/site-packages/torch/_inductor/fx_passes/decompose_mem_bound_mm.py:# TODO: need a better strategy for decomposing mm\n./.venv/lib/python3.13/site-packages/torch/_inductor/fx_passes/micro_pipeline_tp.py:            # TODO: explore unifying the _Matmul and _ScaledMatmul approaches to handling reshapes.\n./.venv/lib/python3.13/site-packages/torch/_inductor/fx_passes/joint_graph.py:            # TODO - decompose/type promote to avoid this\n./.venv/lib/python3.13/site-packages/torch/_inductor/fx_passes/joint_graph.py:            # TODO handle Tensor-Scalar adds, it's a different schema\n./.venv/lib/python3.13/site-packages/torch/_inductor/fx_passes/joint_graph.py:        # TODO: cat, more indexing\n./.venv/lib/python3.13/site-packages/torch/_inductor/fx_passes/joint_graph.py:        # TODO - do on cpu to avoid syncs\n./.venv/lib/python3.13/site-packages/torch/_inductor/fx_passes/joint_graph.py:            # TODO - not sure about lossy uint->python value->uint conversions\n./.venv/lib/python3.13/site-packages/torch/_inductor/fx_passes/reinplace.py:            # TODO Using _overlap here causes a several issues.\n./.venv/lib/python3.13/site-packages/torch/_inductor/fx_passes/reinplace.py:        # TODO this logic can be made more precise using _overlap\n./.venv/lib/python3.13/site-packages/torch/_inductor/fx_passes/reinplace.py:                # TODO(yifu): this doesn't properly remove copy epilogues for\n./.venv/lib/python3.13/site-packages/torch/_inductor/fx_passes/quantization.py:            # TODO: add cuda kernel support instead of calling mul+sum\n./.venv/lib/python3.13/site-packages/torch/_inductor/fx_passes/quantization.py:            # TODO: add cuda kernel support instead of calling mul+sum\n./.venv/lib/python3.13/site-packages/torch/_inductor/fx_passes/quantization.py:        # <TODO> Leslie: Here we verify that the quant node has exactly\n./.venv/lib/python3.13/site-packages/torch/_inductor/fx_passes/fuse_attention.py:    # we could also generate all these patterns in 3d.. TODO\n./.venv/lib/python3.13/site-packages/torch/_inductor/fx_passes/fuse_attention.py:            # TODO: Enable CUDA after solving Bert accuracy issue of calling efficient attention\n./.venv/lib/python3.13/site-packages/torch/_inductor/fx_passes/mkldnn_fusion.py:        TODO: Use MultiOutputPattern, current limitation is the pattern requires\n./.venv/lib/python3.13/site-packages/torch/_inductor/fx_passes/mkldnn_fusion.py:            # TODO: Support dynamic shape case for MKLDNN conv transpose.\n./.venv/lib/python3.13/site-packages/torch/_inductor/fx_passes/mkldnn_fusion.py:        # TODO: aarch64: enable op fusion for acl once it supports fused operators. Disabling it for now.\n./.venv/lib/python3.13/site-packages/torch/_inductor/fx_passes/freezing_patterns.py:        # TODO: remove the need to run fake_tensor_prop on the whole model.\n./.venv/lib/python3.13/site-packages/torch/_inductor/fx_passes/split_cat.py:        # TODO dynamic_shapes with assume_static_by_default=False fails while AOT Autograd tracing.\n./.venv/lib/python3.13/site-packages/torch/_inductor/fx_passes/split_cat.py:        # TODO dynamic_shapes with assume_static_by_default=False fails while AOT Autograd tracing.\n./.venv/lib/python3.13/site-packages/torch/_inductor/fx_passes/split_cat.py:        # TODO dynamic_shapes with assume_static_by_default=False fails while AOT Autograd tracing.\n./.venv/lib/python3.13/site-packages/torch/_inductor/fx_passes/split_cat.py:        # TODO dynamic_shapes with assume_static_by_default=False fails while AOT Autograd tracing.\n./.venv/lib/python3.13/site-packages/torch/_inductor/compiler_bisector.py:    ],  # TODO - add cse ?\n./.venv/lib/python3.13/site-packages/torch/_inductor/compiler_bisector.py:    ],  # TODO - add more - fusions ?\n./.venv/lib/python3.13/site-packages/torch/_inductor/comm_analysis.py:    - 8 gpus per node  # TODO: Need to find a way to get accurate \"gpus per node\" and \"# nodes\" info.\n./.venv/lib/python3.13/site-packages/torch/_inductor/comm_analysis.py:    # TODO: Need to find a way to get accurate \"gpus per node\" and \"# nodes\" info.\n./.venv/lib/python3.13/site-packages/torch/_lobpcg.py:        # TODO use torch.linalg.cholesky_solve once it is implemented\n./.venv/lib/python3.13/site-packages/torch/hub.py:    # TODO: Remove `None` option in 2.0 and change the default to \"check\"\n./.venv/lib/python3.13/site-packages/torch/backends/_nnapi/serializer.py:# TODO: Add type annotations\n./.venv/lib/python3.13/site-packages/torch/backends/_nnapi/serializer.py:# TODO: Check tensor types for ops\n./.venv/lib/python3.13/site-packages/torch/backends/_nnapi/serializer.py:# TODO: Expose these directly to Python to avoid maintaining this list.\n./.venv/lib/python3.13/site-packages/torch/backends/_nnapi/serializer.py:    # TODO: Make this an enum.\n./.venv/lib/python3.13/site-packages/torch/backends/_nnapi/serializer.py:    # TODO: Support non-equal-rank broadcast where semantics match.\n./.venv/lib/python3.13/site-packages/torch/backends/_nnapi/serializer.py:    # TODO: Handle dilation\n./.venv/lib/python3.13/site-packages/torch/backends/_nnapi/serializer.py:                # TODO: Improve this error message, possibly after converting\n./.venv/lib/python3.13/site-packages/torch/backends/_nnapi/serializer.py:            # TODO: Possibly check scale and zero point.\n./.venv/lib/python3.13/site-packages/torch/backends/_nnapi/serializer.py:            # TODO: Possibly support variable-sized inputs.\n./.venv/lib/python3.13/site-packages/torch/backends/_nnapi/serializer.py:                # TODO: Support this by adding trailing 1 dims.\n./.venv/lib/python3.13/site-packages/torch/backends/_nnapi/serializer.py:        # TODO: Validate ceil_mode semantics.\n./.venv/lib/python3.13/site-packages/torch/backends/_nnapi/serializer.py:        # TODO: Transform at load time to share weights with CPU model.\n./.venv/lib/python3.13/site-packages/torch/backends/_nnapi/serializer.py:        # TODO: Support automatic reshape\n./.venv/lib/python3.13/site-packages/torch/backends/_nnapi/serializer.py:        # TODO: Transform at load time to share weights with CPU model.\n./.venv/lib/python3.13/site-packages/torch/backends/_nnapi/serializer.py:        # TODO: Transform at load time to share weights with CPU model.\n./.venv/lib/python3.13/site-packages/torch/backends/_nnapi/prepare.py:            # TODO: See if it's possible to use those directly.\n./.venv/lib/python3.13/site-packages/torch/backends/_nnapi/prepare.py:            # TODO: See if it's possible to use those directly.\n./.venv/lib/python3.13/site-packages/torch/backends/_nnapi/prepare.py:    # TODO: Maybe make these names match the original.\n./.venv/lib/python3.13/site-packages/torch/serialization.py:    # TODO: This feature could be added in the future\n./.venv/lib/python3.13/site-packages/torch/serialization.py:                # TODO: Once we decide to break serialization FC, this case\n./.venv/lib/python3.13/site-packages/torch/serialization.py:            # TODO: There's an issue here with FC. It might be impossible to\n./.venv/lib/python3.13/site-packages/torch/serialization.py:    # TODO: This feature could be added in the future\n./.venv/lib/python3.13/site-packages/torch/serialization.py:                # TODO: Once we decide to break serialization FC, this case\n./.venv/lib/python3.13/site-packages/torch/serialization.py:                    # TODO: Once we decide to break serialization FC, we can\n./.venv/lib/python3.13/site-packages/torch/serialization.py:                    # TODO: Once we decide to break serialization FC, we can\n./.venv/lib/python3.13/site-packages/torch/serialization.py:                # TODO: Once we decide to break serialization FC, we can\n./.venv/lib/python3.13/site-packages/torch/serialization.py:                    # TODO: Once we decide to break serialization FC, we can\n./.venv/lib/python3.13/site-packages/torch/serialization.py:        # TODO: Once we decide to break serialization FC, we can\n./.venv/lib/python3.13/site-packages/torch/_prims/__init__.py:        # TODO: This looks wrong, a number that is wrapped into a tensor\n./.venv/lib/python3.13/site-packages/torch/_prims/__init__.py:# TODO: implement dtype validation here, too, or on the corresponding refs\n./.venv/lib/python3.13/site-packages/torch/_prims/__init__.py:    # TODO: fix number type promotion (bool, complex->float)\n./.venv/lib/python3.13/site-packages/torch/_prims/__init__.py:# TODO: complex needs a special meta to account for its float -> complex behavior\n./.venv/lib/python3.13/site-packages/torch/_prims/__init__.py:        # TODO: this is only here to support the unsqueeze ref\n./.venv/lib/python3.13/site-packages/torch/_prims/__init__.py:# TODO: consider renaming split_dim_view\n./.venv/lib/python3.13/site-packages/torch/_prims/__init__.py:# TODO: review stride logic\n./.venv/lib/python3.13/site-packages/torch/_prims/__init__.py:        # TODO: update meta objects so this can be acquired directly\n./.venv/lib/python3.13/site-packages/torch/_prims/__init__.py:# TODO: create a new return type for scalars?\n./.venv/lib/python3.13/site-packages/torch/_prims/__init__.py:# TODO: create a new return type for scalars?\n./.venv/lib/python3.13/site-packages/torch/_prims/__init__.py:# TODO: create a new return type for scalars?\n./.venv/lib/python3.13/site-packages/torch/_prims/__init__.py:    # TODO: move this as an option on the reference\n./.venv/lib/python3.13/site-packages/torch/_prims/__init__.py:# TODO: Remove safe casting and implement on reference instead\n./.venv/lib/python3.13/site-packages/torch/_prims/__init__.py:# TODO: review support arbitrary resizes\n./.venv/lib/python3.13/site-packages/torch/_prims/__init__.py:# TODO: layout, pin_memory, memory_format\n./.venv/lib/python3.13/site-packages/torch/_prims/__init__.py:# TODO: model requires_grad on TensorMeta\n./.venv/lib/python3.13/site-packages/torch/_prims/__init__.py:# TODO: layout, pin_memory, memory_format\n./.venv/lib/python3.13/site-packages/torch/_prims/__init__.py:# TODO: model requires_grad on TensorMeta\n./.venv/lib/python3.13/site-packages/torch/_prims/__init__.py:# TODO: add layout, pin_memory\n./.venv/lib/python3.13/site-packages/torch/_prims/__init__.py:# TODO: add layout, pin_memory\n./.venv/lib/python3.13/site-packages/torch/_prims/__init__.py:# TODO: add layout\n./.venv/lib/python3.13/site-packages/torch/_prims/__init__.py:# TODO: add layout and pin_memory support\n./.venv/lib/python3.13/site-packages/torch/_prims/__init__.py:    # TODO The MAGMA backend returns V, so this is wrong if used with the MAGMA backend\n./.venv/lib/python3.13/site-packages/torch/_prims/__init__.py:# TODO: we should more seriously review randomness modeling and prims\n./.venv/lib/python3.13/site-packages/torch/_prims/context.py:        # TODO: Should these methods be mapped some other way?\n./.venv/lib/python3.13/site-packages/torch/_prims/executor.py:        # TODO: caching\n./.venv/lib/python3.13/site-packages/torch/_prims/rng_prims.py:        # TODO: you don't need to do this, the dispatch here already disabled\n./.venv/lib/python3.13/site-packages/torch/_dispatch/python.py:    # TODO: test the specs match; empirically  sometimes we have a tuple\n./.venv/lib/python3.13/site-packages/torch/_dispatch/python.py:                # TODO: suppress guards\n./.venv/lib/python3.13/site-packages/torch/_dispatch/python.py:        # TODO: This probably does the wrong thing if you're running other\n./.venv/lib/python3.13/site-packages/torch/_jit_internal.py:    # TODO: __name__ not set for submodules in recursive script\n./.venv/lib/python3.13/site-packages/torch/_jit_internal.py:# TODO support future\n./.venv/lib/python3.13/site-packages/torch/export/__init__.py:    # TODO: For backward compatibility, we support loading a zip file from 2.7. Delete this path in 2.9(?)\n./.venv/lib/python3.13/site-packages/torch/export/pt2_archive/_package.py:        # TODO:Consider dedup this with the weights saved in package_aoti_files\n./.venv/lib/python3.13/site-packages/torch/export/pt2_archive/_package.py:        # TODO: turn this into an error\n./.venv/lib/python3.13/site-packages/torch/export/pt2_archive/_package.py:        # TODO: turn this into an error in 2.9\n./.venv/lib/python3.13/site-packages/torch/export/exported_program.py:                # TODO (tmanlaibaatar)https://github.com/pytorch/pytorch/issues/129430\n./.venv/lib/python3.13/site-packages/torch/export/exported_program.py:        # TODO we are silently allowing non-safe(non-functional) ops through a crack\n./.venv/lib/python3.13/site-packages/torch/export/exported_program.py:        # TODO T204030333\n./.venv/lib/python3.13/site-packages/torch/export/exported_program.py:        # TODO (tmanlaibaatar) Ideally run_decomp should just call _non_strict_export\n./.venv/lib/python3.13/site-packages/torch/export/exported_program.py:    # TODO(zhxhchen17) Return the new graph_signature directly.\n./.venv/lib/python3.13/site-packages/torch/export/exported_program.py:    # TODO unfortunately preserving graph-level metadata is not\n./.venv/lib/python3.13/site-packages/torch/export/exported_program.py:        # TODO(zhxchen17) Remove this.\n./.venv/lib/python3.13/site-packages/torch/export/exported_program.py:    # TODO: remove this\n./.venv/lib/python3.13/site-packages/torch/export/exported_program.py:    # TODO(zhxchen17) Formalize this.\n./.venv/lib/python3.13/site-packages/torch/export/_trace.py:                    # TODO Figure out why sometimes we have root sometimes we don't.\n./.venv/lib/python3.13/site-packages/torch/export/_trace.py:                    except Exception:  # TODO(zhxchen17) Remove this.\n./.venv/lib/python3.13/site-packages/torch/export/_trace.py:    transform=lambda x: x,  # TODO(zhxchen17) Revisit if this is needed later.\n./.venv/lib/python3.13/site-packages/torch/export/_trace.py:    # TODO unfortunately preserving graph-level metadata and output node's meta\n./.venv/lib/python3.13/site-packages/torch/export/_trace.py:    TODO(pianpwk): make this a consistent node-level check once nn_module_stack is populated for cond submodules.\n./.venv/lib/python3.13/site-packages/torch/export/_trace.py:    # TODO: Fix recompile() in  _LazyGraphModule. T207713214\n./.venv/lib/python3.13/site-packages/torch/export/dynamic_shapes.py:_Dim = Dim  # TODO(pianpwk): remove after it's no longer internally breaking\n./.venv/lib/python3.13/site-packages/torch/export/dynamic_shapes.py:        # TODO(avik): use sympy value range analysis instead?\n./.venv/lib/python3.13/site-packages/torch/export/dynamic_shapes.py:        # TODO(avik): use sympy value range analysis instead?\n./.venv/lib/python3.13/site-packages/torch/export/dynamic_shapes.py:        # TODO: A better way is needed. Currently we use 't_id' to map the constraint,\n./.venv/lib/python3.13/site-packages/torch/export/dynamic_shapes.py:        # TODO(avik): check that shape is indeed a Shape\n./.venv/lib/python3.13/site-packages/torch/export/dynamic_shapes.py:    # TODO(avik): raise an error in the future\n./.venv/lib/python3.13/site-packages/torch/export/_swap.py:        2. TODO: Remove module's in_spec + initial unflatten call\n./.venv/lib/python3.13/site-packages/torch/export/_swap.py:        3. TODO: Remove module's out_spec + final flatten call\n./.venv/lib/python3.13/site-packages/torch/export/_swap.py:    # TODO: Handle the duplicate module case\n./.venv/lib/python3.13/site-packages/torch/export/unflatten.py:        # TODO(zhxchen17) We can register modules ahead of time instead of reorder later.\n./.venv/lib/python3.13/site-packages/torch/export/unflatten.py:            # TODO(suo): untangle this.\n./.venv/lib/python3.13/site-packages/torch/export/unflatten.py:                # TODO(suo): The FlatArgsAdapter returns a list of flat args,\n./.venv/lib/python3.13/site-packages/torch/export/unflatten.py:    # TODO: support skip connection by inlining the child module.\n./.venv/lib/python3.13/site-packages/torch/export/unflatten.py:    # TODO Can be optimized by adding submodules ahead of time.\n./.venv/lib/python3.13/site-packages/torch/export/_unlift.py:    # TODO T206340015\n./.venv/lib/python3.13/site-packages/torch/_dynamo/__init__.py:    # TODO: https://github.com/pytorch/pytorch/issues/139200\n./.venv/lib/python3.13/site-packages/torch/_dynamo/__init__.py:    # TODO: https://github.com/pytorch/pytorch/issues/139200\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/dicts.py:            # TODO Temporarily remove to figure out what keys are we breaking on\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/dicts.py:            # TODO: Put this in utils and share it between variables/builtin.py and here\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/dicts.py:# TODO: Implementing this via inheritance rather than composition is a\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/functions.py:        # TODO putting this here to avoid duplication, because we could hit this\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/functions.py:        # TODO(anijain2305) - Replace directly calling UserFunctionVariable with\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/functions.py:            # TODO refactor these 3 branches.\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/functions.py:                # TODO figure out why source isn't available here, and whether\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/functions.py:        # TODO(anijain2305) - Add support for more builtin methods\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/functions.py:                # TODO improve trace_rules reasoning to provide better hints.\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/misc.py:        # TODO: support an expression form as well\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/misc.py:        # TODO(dynamo-team) - We can perhaps expand the scope to more names and\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/misc.py:            # TODO Add all the functions that go from constants to constants to can_constant_fold_through\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/user_defined.py:        # TODO(anijain2305) - Extend this to support objects with default tp_new\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/user_defined.py:            )  # TODO(voz): These can invoke user code!\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/user_defined.py:            )  # TODO(voz): These can invoke user code!\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/user_defined.py:    # TODO: arguably, this should route to wrap_symint/wrap_symfloat\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/user_defined.py:        # TODO consider emulating `obj.__dict__` as a `ConstDictVariable` to get\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/user_defined.py:            # TODO else try reconstructing the object by, e.g., leveraging side\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/user_defined.py:                # TODO(anijain2305) - Identity checking should already be a part\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/user_defined.py:                # TODO(jansel): add a guard to check for monkey patching?\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/user_defined.py:                # TODO(anijain2305) - This is a mapping proxy object. Ideally we\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/user_defined.py:        # TODO(anijain2305) - Investigate if we need specialization for more\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/user_defined.py:            # TODO loosen this restriction and fix `as_proxy`.\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/user_defined.py:        # TODO this isn't really safe, because\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/user_defined.py:            # TODO this duplicates the logic in `BuiltinVariable(tuple)`\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/nn_module.py:                # TODO: Use named_children when it supports remove_duplicate=False.\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/nn_module.py:                    # TODO: do we want to support __call__ for GM's?\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/nn_module.py:                # TODO(anijain2305,export-team) - Remove this if condition when inlining of inbuilt nn modules is\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/nn_module.py:                # TODO(anijain2305) - This might not be needed if we let Dynamo\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/distributed.py:    TODO: make it possible to use ProcessGroupVariable as input to simple functions\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/distributed.py:    TODO: should we make this inherit VT instead of UDOV? Do we want any of the default behaviors\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/distributed.py:        # TODO should this just raise unimplemented?\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/lists.py:            # TODO this type check logic mirrors the following\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/tensor.py:        # TODO: strip off fake tensor from repr here\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/tensor.py:        # TODO - This is not a good solution but solves an accuracy issue.\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/tensor.py:                # TODO(voz):\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/tensor.py:                # TODO(jansel): returning None here is wrong, it should be\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/tensor.py:        # TODO: Should we allow non SymTypes here?  Today it is allowed\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/tensor.py:                # tensor data with a new type. TODO polyfill?\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/tensor.py:        # TODO builder should be able to handle `torch.Tensor.__init__`,\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/builtin.py:                # TODO: If we expand this to handle tensor args, we need to manually\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/builtin.py:                # TODO - supporting all comparison operators could also work but\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/builtin.py:        # TODO handle more cases and merge this with this with `generic_jump`.\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/builtin.py:            # TODO(mlazos) - Do we need this?\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/builtin.py:                    # TODO(voz): Make it work properly\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/torch_function.py:            # TODO move this logic into `TensorVariable`, or try to merge it\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/builder.py:    # TODO: storing a SymInt here but not a FakeTensor is a pretty strange\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/builder.py:        # TODO(jansel): something like a REPR_MATCH might be more robust here\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/builder.py:            # TODO support source for sets and remove the special logics here.\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/builder.py:            # TODO: this doing it manually is bad\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/builder.py:            # TODO: see if we need to add custom guard instead of a simple ID_MATCH\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/builder.py:            # TODO: see if we need to add custom guard instead of a simple ID_MATCH\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/builder.py:            # TODO: see if we need to add custom guard instead of a simple ID_MATCH\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/builder.py:                    # TODO (yidi): we need to figure out a way to propagate the guards\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/builder.py:            # TODO(whc): Why do we limit this to methods on NNModules?\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/builder.py:                # TODO(jansel): combine this case with the one above\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/builder.py:        # TODO(pearu,sparse-team) - Add the corresponding SPARSE_TENSOR_MATCH guards\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/builder.py:        # TODO - Why do we need to set the source of the np ndarray vt back to\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/builder.py:            # TODO: This should be dynamic, as we in general do not\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/builder.py:                # TODO: dynamic_dim = DimDynamic.STATIC should work but\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/builder.py:            # TODO: Do I actually need guard for constant source?\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/builder.py:        # TODO: Switch RandomValueSource over to use this, this is more\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/builder.py:        # TODO: Maybe the tensor-ification should be built into the source,\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/builder.py:                # TODO: when can this happen?\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/builder.py:            # TODO: this is a little sus, because we didn't check what the self is\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/builder.py:    # TODO: not sure about this fake mode test\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/builder.py:    # TODO: index export_constraints ahead of time so we don't have to\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/builder.py:            # TODO: This can be batched\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/builder.py:            # TODO: Doing this here is kind of sus, maybe better to set this\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/builder.py:            # TODO: When does this show up?\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/builder.py:            # TODO: for TensorGuards, this eventually may need more\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/builder.py:                # TODO: revise this, but for now this stride instead of ()\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/torch.py:                # TODO: this probably should be folded somewhere else but I'm not sure where\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/torch.py:                # TODO: some of the other symbolic_shapes special tools can also get this treatment too\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/torch.py:                # TODO: this probably should be folded somewhere else but I'm not sure where\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/torch.py:                # TODO: some of the other symbolic_shapes special tools can also get this treatment too\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/torch.py:                # TODO: this probably should be folded somewhere else but I'm not sure where\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/torch.py:                # TODO: some of the other symbolic_shapes special tools can also get this treatment too\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/torch.py:                # TODO: there maybe other recursive structures you need to\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/torch.py:            # TODO support more output types\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/torch.py:        # TODO(voz): Replace w/ dynamic shape rewrite table.\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/torch.py:        # TODO for each of the following check on `out=` or `requires_grad=`\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/torch.py:        # TODO(jansel/bdhirsh) - There is some issue with\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/torch.py:        # TODO(jansel): if the new param falls out of scope, currently it won't get freed until\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/lazy.py:            # TODO: Add support for more types\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/higher_order_ops.py:                # TODO - write an example with tensor as a graph attribute in\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/higher_order_ops.py:            # TODO - call_module is not supported because Dynamo Fx graph does\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/higher_order_ops.py:    # TODO - supports input_mutation and aliasing should be False by default for strictness\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/higher_order_ops.py:            # TODO: support pytree output\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/higher_order_ops.py:        # TODO(voz): Support fake tensor dispatch for recursive\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/higher_order_ops.py:            # TODO: Support kwargs\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/higher_order_ops.py:        # TODO: Support kwargs\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/higher_order_ops.py:        # TODO (tmanlaibaatar) support pytree here\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/higher_order_ops.py:        # TODO: assert that bwd_graph didn't capture values that were\n./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/higher_order_ops.py:        # TODO(oulgen): Ideally, we would not do a linear search for output\n./.venv/lib/python3.13/site-packages/torch/_dynamo/decorators.py:    TODO(voz): We now have allow_in_graph, disallow_in_graph, forbid_in_graph - some more robust\n./.venv/lib/python3.13/site-packages/torch/_dynamo/decorators.py:        # TODO: Make this configurable via a supported public API\n./.venv/lib/python3.13/site-packages/torch/_dynamo/decorators.py:        # TODO(voz): Should we bounds check?\n./.venv/lib/python3.13/site-packages/torch/_dynamo/decorators.py:        # TODO: Make this configurable via a supported public API\n./.venv/lib/python3.13/site-packages/torch/_dynamo/decorators.py:        # TODO(voz): Should we bounds check?\n./.venv/lib/python3.13/site-packages/torch/_dynamo/decorators.py:        # TODO: Make this configurable via a supported public API\n./.venv/lib/python3.13/site-packages/torch/_dynamo/decorators.py:        # TODO(voz): Should we bounds check?\n./.venv/lib/python3.13/site-packages/torch/_dynamo/decorators.py:# TODO: also implement nonrecursive patch_dynamo_config/dont_skip_tracing.\n./.venv/lib/python3.13/site-packages/torch/_dynamo/symbolic_convert.py:            # TODO maybe should respect DtoH sync intention of users later??\n./.venv/lib/python3.13/site-packages/torch/_dynamo/symbolic_convert.py:                    # TODO(anijain2305) - This is not tested .. unable to create a testcase\n./.venv/lib/python3.13/site-packages/torch/_dynamo/symbolic_convert.py:        # TODO: mlazos, add support for enabling multiple artifact logs\n./.venv/lib/python3.13/site-packages/torch/_dynamo/symbolic_convert.py:        # module. TODO generalize the check for other non-importable cases.\n./.venv/lib/python3.13/site-packages/torch/_dynamo/compiled_autograd.py:        # TODO(jansel): are all these modes needed?\n./.venv/lib/python3.13/site-packages/torch/_dynamo/compiled_autograd.py:        # TODO(yf225): work around: remove dead codes like `sym_size` and `sym_numel` which are not used downstream. e.g.\n./.venv/lib/python3.13/site-packages/torch/_dynamo/comptime.py:            # TODO: Maybe complain if this isn't a int/bool/float variable\n./.venv/lib/python3.13/site-packages/torch/_dynamo/comptime.py:    # TODO: API for adding a custom guard\n./.venv/lib/python3.13/site-packages/torch/_dynamo/comptime.py:        # TODO: improve print format, current guard format is extremely\n./.venv/lib/python3.13/site-packages/torch/_dynamo/cache_size.py:    TODO(janimesh) - Consider adding a map from tuple_of_match_ids to count -\n./.venv/lib/python3.13/site-packages/torch/_dynamo/bytecode_analysis.py:    # TODO(jansel): double check exception handling\n./.venv/lib/python3.13/site-packages/torch/_dynamo/debug_utils.py:        # TODO - Assuming that all modules can be safely repr'd. Check if that assumption is correct.\n./.venv/lib/python3.13/site-packages/torch/_dynamo/debug_utils.py:        # TODO - Keep this code for now. But, I don't think we will need this.\n./.venv/lib/python3.13/site-packages/torch/_dynamo/debug_utils.py:# TODO: Support bundling the entire repro into a zip file for ease of\n./.venv/lib/python3.13/site-packages/torch/_dynamo/debug_utils.py:                    # TODO: transfer it to the right device?  But failing this\n./.venv/lib/python3.13/site-packages/torch/_dynamo/debug_utils.py:        # TODO: consider ensuring tensor and storage counters line up?\n./.venv/lib/python3.13/site-packages/torch/_dynamo/debug_utils.py:        # TODO: being optional on device is kind of pointless as the default\n./.venv/lib/python3.13/site-packages/torch/_dynamo/debug_utils.py:    # TODO: this doesn't actually symint atm\n./.venv/lib/python3.13/site-packages/torch/_dynamo/backends/cudagraphs.py:                    # TODO: not correct for args that contain tensors in a struct\n./.venv/lib/python3.13/site-packages/torch/_dynamo/backends/cudagraphs.py:        # TODO: error on unrecognized nodes\n./.venv/lib/python3.13/site-packages/torch/_dynamo/backends/distributed.py:            # TODO: add split id to CompileId: https://github.com/pytorch/tlparse/pull/83/files#r1880649384\n./.venv/lib/python3.13/site-packages/torch/_dynamo/backends/distributed.py:                # TODO(whc)\n./.venv/lib/python3.13/site-packages/torch/_dynamo/backends/distributed.py:            # TODO - better way of doing this?\n./.venv/lib/python3.13/site-packages/torch/_dynamo/backends/tvm.py:            # TODO(shingjan): This could be replaced by tvm.contrib.torch.optimize_torch\n./.venv/lib/python3.13/site-packages/torch/_dynamo/output_graph.py:            # TODO: replace `same` function with the one in testing\n./.venv/lib/python3.13/site-packages/torch/_dynamo/output_graph.py:        # TODO: maybe should just pass the entire f_code in here?  Not\n./.venv/lib/python3.13/site-packages/torch/_dynamo/output_graph.py:                # TODO (tmanlaibaatar) Remove this once we always lift params and buffers\n./.venv/lib/python3.13/site-packages/torch/_dynamo/output_graph.py:    # TODO(rzou): can delete after we refactor speculate_subgraph to use nested GraphTracer.\n./.venv/lib/python3.13/site-packages/torch/_dynamo/output_graph.py:        # TODO `nn_modules` has been historically overloaded to store a lot more\n./.venv/lib/python3.13/site-packages/torch/_dynamo/output_graph.py:        # TODO this local restoration should be removed when fully implementing nested graph breaks\n./.venv/lib/python3.13/site-packages/torch/_dynamo/output_graph.py:                    # TODO(voz): The way export uses gm, and fake tensors, is not supported with us resetting\n./.venv/lib/python3.13/site-packages/torch/_dynamo/output_graph.py:                # TODO(voz): Ostensibily, this should be scoped and\n./.venv/lib/python3.13/site-packages/torch/_dynamo/output_graph.py:                # TODO: Why isn't this stored in meta :think:\n./.venv/lib/python3.13/site-packages/torch/_dynamo/output_graph.py:            # TODO: We can also technically remove all cases when the input\n./.venv/lib/python3.13/site-packages/torch/_dynamo/output_graph.py:            # TODO: I don't think it's possible to have a bare int/float here?\n./.venv/lib/python3.13/site-packages/torch/_dynamo/output_graph.py:            # TODO: This will bail here if you ever end up with a more complicated\n./.venv/lib/python3.13/site-packages/torch/_dynamo/output_graph.py:                # TODO can remove once inline_inbuilt_nn_modules is always True\n./.venv/lib/python3.13/site-packages/torch/_dynamo/output_graph.py:                        # TODO can remove once inline_inbuilt_nn_modules is always True\n./.venv/lib/python3.13/site-packages/torch/_dynamo/precompile_context.py:        # TODO: although this covers completely same artifacts, it's possible\n./.venv/lib/python3.13/site-packages/torch/_dynamo/precompile_context.py:        raise NotImplementedError(\"TODO\")\n./.venv/lib/python3.13/site-packages/torch/_dynamo/exc.py:# TODO: I'm a little uncertain about what error classification we should have\n./.venv/lib/python3.13/site-packages/torch/_dynamo/exc.py:# TODO replace old unimplemented later\n./.venv/lib/python3.13/site-packages/torch/_dynamo/resume_execution.py:            # TODO(jansel): add dead code elimination here\n./.venv/lib/python3.13/site-packages/torch/_dynamo/utils.py:            # TODO: should we assert that the keys of metadata are in CompilationMetrics?\n./.venv/lib/python3.13/site-packages/torch/_dynamo/utils.py:    # TODO(masneral): Deprecate this param.\n./.venv/lib/python3.13/site-packages/torch/_dynamo/utils.py:            # TODO: the events that we capture in calculate_time_spent() seem a little\n./.venv/lib/python3.13/site-packages/torch/_dynamo/utils.py:    # TODO(anijain2305) - Investigate if we can get rid of this function\n./.venv/lib/python3.13/site-packages/torch/_dynamo/utils.py:        # TODO: The following are legacy fields, populated from the fields that replace\n./.venv/lib/python3.13/site-packages/torch/_dynamo/utils.py:    TODO: Get rid of this function and replace it with CompileEventLogger directly instead.\n./.venv/lib/python3.13/site-packages/torch/_dynamo/utils.py:        # TODO: log to init/id tlparse after I add support for it\n./.venv/lib/python3.13/site-packages/torch/_dynamo/utils.py:    # TODO: this is questionable\n./.venv/lib/python3.13/site-packages/torch/_dynamo/utils.py:        # TODO: Delete this condition when rollout is done.  NB: this\n./.venv/lib/python3.13/site-packages/torch/_dynamo/utils.py:            from tabulate import tabulate  # TODO: Check that this is installed\n./.venv/lib/python3.13/site-packages/torch/_dynamo/utils.py:    # TODO - This is a temporary situation where we have two versions of\n./.venv/lib/python3.13/site-packages/torch/_dynamo/convert_frame.py:class TODO_UNKNOWN:\n./.venv/lib/python3.13/site-packages/torch/_dynamo/convert_frame.py:        # TODO - Running exec generated frame seems propagates f_globals to the\n./.venv/lib/python3.13/site-packages/torch/_dynamo/convert_frame.py:            # TODO: replace with CompileEventLogger.compilation_metrics\n./.venv/lib/python3.13/site-packages/torch/_dynamo/convert_frame.py:# TODO mlazos: add support for same args, or record them\n./.venv/lib/python3.13/site-packages/torch/_dynamo/convert_frame.py:            # TODO: the first condition is not covered by any test\n./.venv/lib/python3.13/site-packages/torch/_dynamo/_trace_wrapped_higher_order_op.py:# TODO(jansel): need to ensure this does not get DCEed\n./.venv/lib/python3.13/site-packages/torch/_dynamo/create_parameter_op.py:    # TODO(jansel): alloc followed by free is inefficient, need a way to allocate an unbacked tensor.\n./.venv/lib/python3.13/site-packages/torch/_dynamo/eval_frame.py:            # TODO: a bit awkward to time, this isn't inside of the dynamo compile region\n./.venv/lib/python3.13/site-packages/torch/_dynamo/eval_frame.py:# TODO(voz): Consider making \"explain\" output alongside a run / part of a run\n./.venv/lib/python3.13/site-packages/torch/_dynamo/eval_frame.py:        # TODO(voz): Do we want a decorator for this?\n./.venv/lib/python3.13/site-packages/torch/_dynamo/eval_frame.py:        # TODO(voz): We may have instances of `f` that mutate inputs, we should track sideeffects and reject.\n./.venv/lib/python3.13/site-packages/torch/_dynamo/eval_frame.py:        # TODO(voz): Do we want a decorator for this?\n./.venv/lib/python3.13/site-packages/torch/_dynamo/eval_frame.py:                    # TODO(zhxchen17) Also preserve all the user constraints here.\n./.venv/lib/python3.13/site-packages/torch/_dynamo/eval_frame.py:        # TODO: option to print ALL of the stack traces at once\n./.venv/lib/python3.13/site-packages/torch/_dynamo/eval_frame.py:            # TODO(voz): We may have instances of `f` that mutate inputs, we should track sideeffects and reject.\n./.venv/lib/python3.13/site-packages/torch/_dynamo/source.py:# TODO: can probably write a generic \"test this on everything in the chain\"\n./.venv/lib/python3.13/site-packages/torch/_dynamo/side_effects.py:            # TODO plumb HOP information here\n./.venv/lib/python3.13/site-packages/torch/_dynamo/side_effects.py:            # TODO(anijain2305) - Is it possible to remove this specialization?\n./.venv/lib/python3.13/site-packages/torch/_dynamo/side_effects.py:            # TODO(anijain2305) - Consider adding get_example_value method to\n./.venv/lib/python3.13/site-packages/torch/_dynamo/side_effects.py:            # TODO track from all possible sources.\n./.venv/lib/python3.13/site-packages/torch/_dynamo/side_effects.py:                # TODO generalize this so we never need to call `make_cell`.\n./.venv/lib/python3.13/site-packages/torch/_dynamo/side_effects.py:                    # cleared it. TODO move this call into `add_cache`\n./.venv/lib/python3.13/site-packages/torch/_dynamo/side_effects.py:                # TODO generalize this for cells created during inlining.\n./.venv/lib/python3.13/site-packages/torch/_dynamo/pgo.py:        # TODO: I'm not sure if we should just bong the entire pgo\n./.venv/lib/python3.13/site-packages/torch/_dynamo/pgo.py:    # TODO: info versions of these logs that log only once\n./.venv/lib/python3.13/site-packages/torch/_dynamo/pgo.py:            # TODO: I don't really understand why there's a JSON container format\n./.venv/lib/python3.13/site-packages/torch/_dynamo/pgo.py:    # TODO: use a safe tempfile create to eliminate lock\n./.venv/lib/python3.13/site-packages/torch/_dynamo/pgo.py:        # TODO: don't log this multiple times\n./.venv/lib/python3.13/site-packages/torch/_dynamo/trace_rules.py:# TODO(yanboliang, anijain2305) - There are a few concerns that we should\n./.venv/lib/python3.13/site-packages/torch/_dynamo/test_minifier_common.py:            # TODO: return a more appropriate data structure here\n./.venv/lib/python3.13/site-packages/torch/_dynamo/types.py:        # TODO(whc) how do I annotate a _RecordFunction here?\n./.venv/lib/python3.13/site-packages/torch/_dynamo/config.py:# TODO(janimesh, voz): Remove both of these flags (or at least guard_nn_modules)\n./.venv/lib/python3.13/site-packages/torch/_dynamo/config.py:# TODO: Detect this situation automatically so the user doesn't need\n./.venv/lib/python3.13/site-packages/torch/_dynamo/guards.py:        # TODO - source debug string is probably wrong here.\n./.venv/lib/python3.13/site-packages/torch/_dynamo/guards.py:        # TODO(anijain2305) - Delete this when DictGuardManager uses tags\n./.venv/lib/python3.13/site-packages/torch/_dynamo/guards.py:            # TODO(anijain2305) - This is currently restricted to nn.Module objects\n./.venv/lib/python3.13/site-packages/torch/_dynamo/guards.py:        # TODO(anijain2305) - Consider this moving this guard to C++\n./.venv/lib/python3.13/site-packages/torch/_dynamo/guards.py:        # TODO(anijain2305) - Consider this moving this guard to C++\n./.venv/lib/python3.13/site-packages/torch/_dynamo/guards.py:    # TODO(voz): Deduplicate w/ AOTAutograd dupe input guards\n./.venv/lib/python3.13/site-packages/torch/_dynamo/guards.py:        # TODO(anijain2305,williamwen42) - Consider moving this to C++.\n./.venv/lib/python3.13/site-packages/torch/_dynamo/guards.py:            # TODO(voz): Either populate a dispatch_key check into the guards, or error on users passing in an unsupported\n./.venv/lib/python3.13/site-packages/torch/_dynamo/guards.py:            # TODO(voz): We are missing storage offset in all our tensor guards?\n./.venv/lib/python3.13/site-packages/torch/_dynamo/guards.py:        # TODO(anijain2305) - Currently this information is stored as an attr on\n./.venv/lib/python3.13/site-packages/torch/_dynamo/guards.py:        # TODO(anijain2305, ydwu4) - Skipping export because of following test\n./.venv/lib/python3.13/site-packages/torch/_dynamo/guards.py:        # TODO: don't do the string rep, do something more structured here\n./.venv/lib/python3.13/site-packages/torch/_dynamo/guards.py:                # TODO: we could make use of 'DefaultsSource' and offer a .guard.is_defaults() API\n./.venv/lib/python3.13/site-packages/torch/_dynamo/guards.py:        # TODO(anijain2305) - There is a duplicate logic in Dynamo to find\n./.venv/lib/python3.13/site-packages/torch/_dynamo/guards.py:        # TODO: the \"guard\" here is actually just the top level SHAPE_ENV\n./.venv/lib/python3.13/site-packages/torch/_dynamo/guards.py:        # TODO(voz): Combine local and global guard builders.\n./.venv/lib/python3.13/site-packages/torch/_dynamo/codegen.py:            b) If value.source is None, this is not allowed. TODO - assert this.\n./.venv/lib/python3.13/site-packages/torch/_dynamo/codegen.py:            # introduced. TODO sort out the invariants among side effect,\n./.venv/lib/python3.13/site-packages/torch/_dynamo/repro/after_dynamo.py:    # TODO: Figure out why torch.compile'd hash isn't work on this codepath\n./.venv/lib/python3.13/site-packages/torch/_dynamo/repro/after_dynamo.py:            # TODO: improve these names with FQN\n./.venv/lib/python3.13/site-packages/torch/_dynamo/repro/after_dynamo.py:    # TODO: factor this out\n./.venv/lib/python3.13/site-packages/torch/_dynamo/repro/after_dynamo.py:    # TODO: It's inconsistent to pass SymInt inputs but REAL tensors.\n./.venv/lib/python3.13/site-packages/torch/_dynamo/repro/after_dynamo.py:            # TODO: disable clone\n./.venv/lib/python3.13/site-packages/torch/_dynamo/repro/after_aot.py:        # TODO: why do we need to deepcopy the original graph?\n./.venv/lib/python3.13/site-packages/torch/_dynamo/repro/after_aot.py:            # TODO: Failures here are troublesome because no real inputs,\n./.venv/lib/python3.13/site-packages/torch/_dynamo/repro/after_aot.py:            # TODO: improve these names with FQN\n./.venv/lib/python3.13/site-packages/torch/_dynamo/repro/after_aot.py:    # TODO: factor this out\n./.venv/lib/python3.13/site-packages/torch/_dynamo/repro/after_aot.py:    # TODO: speed this up\n./.venv/lib/python3.13/site-packages/torch/_dynamo/repro/after_aot.py:    # TODO: The logic for cloning inputs/models here is intentionally\n./.venv/lib/python3.13/site-packages/torch/_dynamo/repro/after_aot.py:    # TODO: check eager determinism\n./.venv/lib/python3.13/site-packages/torch/_dynamo/repro/after_aot.py:# TODO: lazily load the inputs or something, rather than cloning them\n./.venv/lib/python3.13/site-packages/torch/_dynamo/repro/after_aot.py:    # TODO: make this an option for --analyze too\n./.venv/lib/python3.13/site-packages/torch/_dynamo/testing.py:            # TODO: shouldn't this be f_locals/f_globals from frame?\n./.venv/lib/python3.13/site-packages/torch/_dynamo/polyfills/itertools.py:        # TODO: use indices = itertools.count() and merge implementation with the else branch\n./.venv/lib/python3.13/site-packages/torch/onnx/verification.py:# TODO: Update deprecation messages to recommend the new classes\n./.venv/lib/python3.13/site-packages/torch/onnx/verification.py:# TODO(justinchuby): Add type checking by narrowing down the return type when input is None\n./.venv/lib/python3.13/site-packages/torch/onnx/verification.py:            # TODO: Remove `check_shape` option once every shape inconsistent issue is addressed.\n./.venv/lib/python3.13/site-packages/torch/onnx/verification.py:        # TODO: remove this and treat mutating model separately. See #77679\n./.venv/lib/python3.13/site-packages/torch/onnx/verification.py:    # TODO: refactor utils.py to remove duplicated code of context setup. See #78834\n./.venv/lib/python3.13/site-packages/torch/onnx/verification.py:    # TODO: Below is doing aten graph to onnx. It should be abstracted as a\n./.venv/lib/python3.13/site-packages/torch/onnx/verification.py:        # TODO(#77679): remove this and treat mutating model separately.\n./.venv/lib/python3.13/site-packages/torch/onnx/verification.py:    # TODO: Only copy the argument if mutation is detected in Graph.\n./.venv/lib/python3.13/site-packages/torch/onnx/verification.py:            # TODO: A more compact graph printer.\n./.venv/lib/python3.13/site-packages/torch/onnx/verification.py:    # TODO: Copied from utils.py `export` until `_optimize_graph`.\n./.venv/lib/python3.13/site-packages/torch/onnx/symbolic_helper.py:    # TODO(justinchuby): Replace insinstance with _is_value once we figure out mypy\n./.venv/lib/python3.13/site-packages/torch/onnx/symbolic_helper.py:            # TODO(justinchuby): Only single output is supported for now. We may want to\n./.venv/lib/python3.13/site-packages/torch/onnx/symbolic_helper.py:        # TODO(justinchuby): Check if dtype is indeed a int.\n./.venv/lib/python3.13/site-packages/torch/onnx/symbolic_helper.py:# TODO: remove these once we support Type's in the JIT IR and we can once again\n./.venv/lib/python3.13/site-packages/torch/onnx/_onnx_supported_ops.py:        # TODO(thiagocrepaldi): handle overload_name?\n./.venv/lib/python3.13/site-packages/torch/onnx/_onnx_supported_ops.py:        # TODO(thiagocrepaldi): handle overload_name?\n./.venv/lib/python3.13/site-packages/torch/onnx/symbolic_opset18.py:# TODO(justinchuby): Support multiple quantized args in output\n./.venv/lib/python3.13/site-packages/torch/onnx/symbolic_opset18.py:# TODO(justinchuby): Support multiple quantized args in output\n./.venv/lib/python3.13/site-packages/torch/onnx/symbolic_opset10.py:    # TODO(justinchuby): Extract all the cast ops into a helper function.\n./.venv/lib/python3.13/site-packages/torch/onnx/utils.py:# TODO(justinchuby): Remove dependency to this global variable from constant_fold.cpp\n./.venv/lib/python3.13/site-packages/torch/onnx/utils.py:    # TODO: can we simplify this to always return a tuple of Tensor or None?\n./.venv/lib/python3.13/site-packages/torch/onnx/utils.py:            # TODO(justinchuby): Create a way to check if an op is fully supported.\n./.venv/lib/python3.13/site-packages/torch/onnx/utils.py:                # TODO Wrap almost identical attrs assignment or comment the difference.\n./.venv/lib/python3.13/site-packages/torch/onnx/symbolic_opset13.py:    # TODO: So far we don\"t have a module using this method. We\"ll keep\n./.venv/lib/python3.13/site-packages/torch/onnx/symbolic_opset15.py:                        TODO: test coverage for mixed types inputs.\n./.venv/lib/python3.13/site-packages/torch/onnx/symbolic_opset15.py:                        TODO: bfloat16 support.\n./.venv/lib/python3.13/site-packages/torch/onnx/symbolic_opset15.py:                        TODO: optional start/end attribute.\n./.venv/lib/python3.13/site-packages/torch/onnx/_experimental.py:    # TODO(justinchuby): Deprecate and remove this class.\n./.venv/lib/python3.13/site-packages/torch/onnx/ops/__init__.py:    # TODO: Parse domain\n./.venv/lib/python3.13/site-packages/torch/onnx/symbolic_opset17.py:        )  # TODO(#145944): add compatibility with align_to_window option.\n./.venv/lib/python3.13/site-packages/torch/onnx/_internal/_exporter_legacy.py:    # TODO: Design the passes API\n./.venv/lib/python3.13/site-packages/torch/onnx/_internal/_exporter_legacy.py:    # TODO: Import here to prevent circular dependency\n./.venv/lib/python3.13/site-packages/torch/onnx/_internal/io_adapter.py:# TODO: make_fx lose stack info https://github.com/pytorch/pytorch/issues/90276\n./.venv/lib/python3.13/site-packages/torch/onnx/_internal/io_adapter.py:# TODO(XuehaiPan): Dynamo does not support `dummy_leaf = object()` as a sentinel value in the frame.\n./.venv/lib/python3.13/site-packages/torch/onnx/_internal/onnxruntime.py:    # TODO: select a good default based on the capabilities of the host\n./.venv/lib/python3.13/site-packages/torch/onnx/_internal/onnxruntime.py:    # TODO(justinchuby): Refactor this function\n./.venv/lib/python3.13/site-packages/torch/onnx/_internal/onnxruntime.py:        # TODO(justinchuby): Simplify\n./.venv/lib/python3.13/site-packages/torch/onnx/_internal/onnxruntime.py:    # TODO(wschin): Make it to inference session level flag.\n./.venv/lib/python3.13/site-packages/torch/onnx/_internal/onnxruntime.py:        # TODO(wschin): this is a naive implementation of cache without proper guard\n./.venv/lib/python3.13/site-packages/torch/onnx/_internal/onnxruntime.py:            # TODO(wschin): enable external allocators.\n./.venv/lib/python3.13/site-packages/torch/onnx/_internal/onnxruntime.py:                # TODO(wschin): use a better way to identify fused submodule\n./.venv/lib/python3.13/site-packages/torch/onnx/_internal/exporter/_core.py:            # TODO(justinchuby): Maybe keep it as None?\n./.venv/lib/python3.13/site-packages/torch/onnx/_internal/exporter/_core.py:        # TODO: Log the message here to expose false positives\n./.venv/lib/python3.13/site-packages/torch/onnx/_internal/exporter/_core.py:                # TODO: Get IR function directly when onnxscript is updated\n./.venv/lib/python3.13/site-packages/torch/onnx/_internal/exporter/_core.py:    # TODO: Decide if we should keep mutated buffers as inputs/outputs\n./.venv/lib/python3.13/site-packages/torch/onnx/_internal/exporter/_core.py:    # TODO(justinchuby): Remove the hack\n./.venv/lib/python3.13/site-packages/torch/onnx/_internal/exporter/_core.py:            # TODO(justinchuby): The threshold is arbitrary right now\n./.venv/lib/python3.13/site-packages/torch/onnx/_internal/exporter/_registration.py:    # TODO(justinchuby): Handle arbitrary custom ops\n./.venv/lib/python3.13/site-packages/torch/onnx/_internal/exporter/_registration.py:        # TODO(justinchuby): Remove this once torchlib is migrated to PyTorch\n./.venv/lib/python3.13/site-packages/torch/onnx/_internal/exporter/_tensors.py:    # TODO: Implement indexing\n./.venv/lib/python3.13/site-packages/torch/onnx/_internal/exporter/_dispatching.py:        # TODO: Maybe just check dtype? Being more strict here for now\n./.venv/lib/python3.13/site-packages/torch/onnx/_internal/exporter/_dispatching.py:                # TODO: Handle None attributes\n./.venv/lib/python3.13/site-packages/torch/onnx/_internal/exporter/_dispatching.py:    # TODO: Handle when node does not have a target\n./.venv/lib/python3.13/site-packages/torch/onnx/_internal/exporter/_building.py:            # TODO(justinchuby): Implement type promotion logic here.\n./.venv/lib/python3.13/site-packages/torch/onnx/_internal/exporter/_building.py:            # TODO(justinchuby): Cast the ir.Value here if needed\n./.venv/lib/python3.13/site-packages/torch/onnx/_internal/exporter/_building.py:            # TODO(justinchuby): Handle cast\n./.venv/lib/python3.13/site-packages/torch/onnx/_internal/exporter/_building.py:            # TODO(justinchuby): Remove this once IsScalar and Rank are removed\n./.venv/lib/python3.13/site-packages/torch/onnx/_internal/exporter/_building.py:            # TODO(after torchlib migration): Remove traceable function handling\n./.venv/lib/python3.13/site-packages/torch/onnx/_internal/exporter/_schemas.py:    # TODO: Add other properties too\n./.venv/lib/python3.13/site-packages/torch/onnx/_internal/exporter/_schemas.py:    # TODO: Upstream this to IR\n./.venv/lib/python3.13/site-packages/torch/onnx/_internal/exporter/_schemas.py:        # TODO: Double check the separator for overload\n./.venv/lib/python3.13/site-packages/torch/onnx/_internal/exporter/_schemas.py:                        # TODO: Handle variadic\n./.venv/lib/python3.13/site-packages/torch/onnx/_internal/exporter/_schemas.py:                        # TODO: Use ir_convenience instead to handle int as float\n./.venv/lib/python3.13/site-packages/torch/onnx/_internal/exporter/_schemas.py:                            # TODO: Handle variadic\n./.venv/lib/python3.13/site-packages/torch/onnx/_internal/exporter/_torchlib/_torchlib_registry.py:            # TODO(justinchuby): Simplify the logic and remove the private attribute\n./.venv/lib/python3.13/site-packages/torch/onnx/_internal/exporter/_ir_passes.py:    # TODO: Ensure the names do not have duplicates\n./.venv/lib/python3.13/site-packages/torch/onnx/_internal/exporter/_ir_passes.py:        # TODO(justinchuby): Remove this hack and improved onnxscript\n./.venv/lib/python3.13/site-packages/torch/onnx/_internal/exporter/_capture_strategies.py:    # TODO: Remove the patches once dynamo supports these functions.\n./.venv/lib/python3.13/site-packages/torch/onnx/_internal/exporter/_testing.py:    # TODO(justinchuby): Include output names in the error message\n./.venv/lib/python3.13/site-packages/torch/onnx/_internal/exporter/_compat.py:            # TODO(justinchuby): Support complex inputs with annotations\n./.venv/lib/python3.13/site-packages/torch/onnx/_internal/exporter/_onnx_program.py:    # TODO(#151064): Use dlpack when ORT properly supports it\n./.venv/lib/python3.13/site-packages/torch/onnx/_internal/exporter/_onnx_program.py:        # TODO(justinchuby): Allow different inference options\n./.venv/lib/python3.13/site-packages/torch/onnx/_internal/jit_utils.py:# TODO(justinchuby): Move more of the symbolic helper functions here and expose\n./.venv/lib/python3.13/site-packages/torch/onnx/_internal/jit_utils.py:# TODO: Expose this to user when migrating symbolic helper functions to here.\n./.venv/lib/python3.13/site-packages/torch/onnx/_internal/registration.py:    # TODO(justinchuby): Add @functools.lru_cache(maxsize=None) if lookup time becomes\n./.venv/lib/python3.13/site-packages/torch/onnx/_internal/fx/onnxfunction_dispatcher.py:            # TODO(titaiwang): aten::sym_size has overload, but fx graph is using\n./.venv/lib/python3.13/site-packages/torch/onnx/_internal/fx/passes/decomp.py:        # TODO: May need revisit for user fake mode export + dynamic shape scenario.\n./.venv/lib/python3.13/site-packages/torch/onnx/_internal/fx/passes/functionalization.py:        # TODO: May need revisit for user fake mode export + dynamic shape scenario.\n./.venv/lib/python3.13/site-packages/torch/onnx/_internal/fx/serialization.py:# TODO: generalize to allow more checkpoints formats (torch or gguf)\n./.venv/lib/python3.13/site-packages/torch/onnx/_internal/fx/_pass.py:        # TODO: Figure out how to retrieve commit hash.\n./.venv/lib/python3.13/site-packages/torch/onnx/_internal/fx/_pass.py:    TODO(bowbao): Add more overridable methods in call hierarchy\n./.venv/lib/python3.13/site-packages/torch/onnx/_internal/fx/_pass.py:    TODO(bowbao): Create an example once more overridable methods are added.\n./.venv/lib/python3.13/site-packages/torch/onnx/_internal/fx/patcher.py:# TODO: Remove after https://github.com/huggingface/safetensors/pull/318\n./.venv/lib/python3.13/site-packages/torch/onnx/_internal/fx/patcher.py:    TODO: Should this really be a global patcher? Can we make it a local patcher?\n./.venv/lib/python3.13/site-packages/torch/onnx/_internal/fx/fx_onnx_interpreter.py:    # TODO: aten::sym_size has overload, but fx graph is using\n./.venv/lib/python3.13/site-packages/torch/onnx/_internal/fx/fx_onnx_interpreter.py:        # TODO: Fix FakeTensorMode limitation asap\n./.venv/lib/python3.13/site-packages/torch/onnx/_internal/fx/fx_onnx_interpreter.py:        # TODO(wechi): Support call_method.\n./.venv/lib/python3.13/site-packages/torch/onnx/_internal/fx/fx_onnx_interpreter.py:        # TODO: We may want to consider other naming styles. The goal is to be stable and\n./.venv/lib/python3.13/site-packages/torch/onnx/symbolic_opset9.py:        # TODO: remove this as onnx opset 11 spec allows negative axes\n./.venv/lib/python3.13/site-packages/torch/onnx/symbolic_opset9.py:    # TODO(justinchuby): Looks like this op is deprecated in torch\n./.venv/lib/python3.13/site-packages/torch/onnx/symbolic_opset9.py:    # TODO: remove this as onnx opset 11 spec allows negative axes\n./.venv/lib/python3.13/site-packages/torch/onnx/symbolic_opset9.py:# TODO(justinchuby): Support multiple quantized args in output\n./.venv/lib/python3.13/site-packages/torch/onnx/symbolic_opset9.py:# TODO(justinchuby): Support multiple quantized args in output\n./.venv/lib/python3.13/site-packages/torch/onnx/symbolic_opset9.py:# TODO(justinchuby): Support multiple quantized args in output\n./.venv/lib/python3.13/site-packages/torch/onnx/symbolic_opset9.py:# TODO(justinchuby): Support multiple quantized args in output\n./.venv/lib/python3.13/site-packages/torch/onnx/symbolic_opset9.py:    # TODO: remove this as onnx opset 11 spec allows negative axes\n./.venv/lib/python3.13/site-packages/torch/onnx/symbolic_opset9.py:            # TODO: If indexing is supported natively in ONNX in future opsets,\n./.venv/lib/python3.13/site-packages/torch/onnx/symbolic_opset9.py:        # TODO: Might need a fix in torch group_norm module\n./.venv/lib/python3.13/site-packages/torch/onnx/symbolic_opset9.py:# TODO: It would be better to export this as a chunk directly, as this is\n./.venv/lib/python3.13/site-packages/torch/onnx/symbolic_opset9.py:# TODO: Once we have proper scoping, stop reimplementing chunk, delete this\n./.venv/lib/python3.13/site-packages/torch/onnx/symbolic_opset9.py:        # TODO(justinchuby): Use a public method in the helper module\n./.venv/lib/python3.13/site-packages/torch/profiler/_utils.py:            # TODO: find a better way to identify cudaLaunchKernel\n./.venv/lib/python3.13/site-packages/torch/profiler/_utils.py:            # TODO: find a better way to identify CUDA Kernel\n./.venv/lib/python3.13/site-packages/torch/profiler/_utils.py:# TODO(dberard) - deprecate / remove workaround for CUDA >= 12, when\n./.venv/lib/python3.13/site-packages/torch/profiler/_memory_profiler.py:        # TODO(robieta): Move away from load bearing names\n./.venv/lib/python3.13/site-packages/torch/profiler/_memory_profiler.py:        # TODO(robieta):\n./.venv/lib/python3.13/site-packages/torch/profiler/_memory_profiler.py:        # TODO: Write a faster serialize (orjson not available in CI)\n./.venv/lib/python3.13/site-packages/torch/profiler/_pattern_matcher.py:        # TODO: We should also check tensor identities\n./.venv/lib/python3.13/site-packages/torch/profiler/_pattern_matcher.py:        # TODO: Check if tensor is reused\n./.venv/lib/python3.13/site-packages/torch/profiler/_pattern_matcher.py:        # TODO: fixme! Due to lifetime issues of the function name, this field might\n./.venv/lib/python3.13/site-packages/torch/profiler/_pattern_matcher.py:        # TODO: We should also check if the loader is bottleneck.\n./.venv/lib/python3.13/site-packages/torch/profiler/_pattern_matcher.py:        # TODO: We should also check if the optimizer's numerical behavior will change.\n./.venv/lib/python3.13/site-packages/torch/_higher_order_ops/torchbind.py:# TODO: this is not really sufficient. While passes (hopefully) check\n./.venv/lib/python3.13/site-packages/torch/_higher_order_ops/flex_attention.py:            # TODO: So far only the input mutations are checked\n./.venv/lib/python3.13/site-packages/torch/_higher_order_ops/flex_attention.py:    # TODO: Figure out a better way to handle this for NJT than using sum()\n./.venv/lib/python3.13/site-packages/torch/_higher_order_ops/flex_attention.py:# TODO: Rework DispatchKey.Autograd to py_autograd_impl\n./.venv/lib/python3.13/site-packages/torch/_higher_order_ops/invoke_subgraph.py:# TODO (@anijain2305) - Delete this function when base_hop uses invoke_subgraph infra\n./.venv/lib/python3.13/site-packages/torch/_higher_order_ops/invoke_subgraph.py:# TODO (@anijain2305) - Delete this function when base_hop uses invoke_subgraph infra\n./.venv/lib/python3.13/site-packages/torch/_higher_order_ops/triton_kernel_wrap.py:    # TODO: remove this when the Triton issue above is fixed\n./.venv/lib/python3.13/site-packages/torch/_higher_order_ops/triton_kernel_wrap.py:    # TODO(oulgen): Preexisting bug, if two kernel inputs are views of each\n./.venv/lib/python3.13/site-packages/torch/_higher_order_ops/triton_kernel_wrap.py:    # TODO(oulgen): For performance reasons, we want to ensure that these\n./.venv/lib/python3.13/site-packages/torch/_higher_order_ops/triton_kernel_wrap.py:    # TODO(oulgen): For performance reasons, we want to ensure that these\n./.venv/lib/python3.13/site-packages/torch/_higher_order_ops/associative_scan.py:        # TODO: find torch alternative for slice_along dim for torch.jit.script to work\n./.venv/lib/python3.13/site-packages/torch/_higher_order_ops/associative_scan.py:    # TODO: Support Autograd\n./.venv/lib/python3.13/site-packages/torch/_higher_order_ops/associative_scan.py:    # TODO: Unify handling of pytrees for control flow ops, such as cond, while_loop, etc.\n./.venv/lib/python3.13/site-packages/torch/_higher_order_ops/utils.py:        # TODO: Investigate here further which node is exactly aliasing\n./.venv/lib/python3.13/site-packages/torch/_higher_order_ops/utils.py:        # TODO: Investigate here further which node is exactly mutating the inputs\n./.venv/lib/python3.13/site-packages/torch/_higher_order_ops/utils.py:# TODO: The parameter use_output_and_grad_bw is required because some operations\n./.venv/lib/python3.13/site-packages/torch/_higher_order_ops/utils.py:# TODO: Return a more detailed information as to which node\n./.venv/lib/python3.13/site-packages/torch/_higher_order_ops/wrap.py:            # TODO: We want to use the same `checkpoint(Interpreter(gmod).run, *args, **kwargs)` here\n./.venv/lib/python3.13/site-packages/torch/_higher_order_ops/base_hop.py:        # TODO: this should probably route through FakeTensorMode to reuse caching\n./.venv/lib/python3.13/site-packages/torch/_higher_order_ops/base_hop.py:            # TODO: turn this into an error.\n./.venv/lib/python3.13/site-packages/torch/_higher_order_ops/base_hop.py:        # TODO: Something special needs to happen with min cut partitioner\n./.venv/lib/python3.13/site-packages/torch/_higher_order_ops/executorch_call_delegate.py:    # TODO: support autograd\n./.venv/lib/python3.13/site-packages/torch/_higher_order_ops/cond.py:        # TODO: we need to materialize the bw graphs because dynamo is unable to\n./.venv/lib/python3.13/site-packages/torch/_higher_order_ops/out_dtype.py:# TODO to figure out a more generic approach\n./.venv/lib/python3.13/site-packages/torch/_higher_order_ops/flat_apply.py:    # TODO: The following can be updated to support non-graphable outputs and pytrees.\n./.venv/lib/python3.13/site-packages/torch/_higher_order_ops/auto_functionalize.py:    # TODO is there cases can we use slice even if stride or len(sizes) are not equal?\n./.venv/lib/python3.13/site-packages/torch/_higher_order_ops/scan.py:    # TODO: Support _inductor lowering\n./.venv/lib/python3.13/site-packages/torch/_higher_order_ops/scan.py:    # TODO: Unify handling of pytrees for control flow ops, such as cond, while_loop, etc.\n./.venv/lib/python3.13/site-packages/torch/_higher_order_ops/scan.py:        # TODO: we need to materialize the bw graphs because dynamo is unable to\n./.venv/lib/python3.13/site-packages/torch/_library/utils.py:    TODO: torchgen/model.py's FunctionSchema.parse is the source of truth for this,\n./.venv/lib/python3.13/site-packages/torch/_library/utils.py:    # TODO: need to double check the semantics of the \"types\" argument to torch_dispatch.\n./.venv/lib/python3.13/site-packages/torch/_library/utils.py:    # TODO: check that I got these args correct (in C++, we pass in \"0000\"??)\n./.venv/lib/python3.13/site-packages/torch/_library/infer_schema.py:    # TODO: Once our minimum version is py3.10+ pass `eval_str=True` to\n./.venv/lib/python3.13/site-packages/torch/_library/custom_ops.py:# TODO: Merge this function with torch.amp.autocast_mode._cast, and refactor it\n./.venv/lib/python3.13/site-packages/torch/_library/fake_class_registry.py:# TODO: add this check at compile time for __obj_flatten__.\n./.venv/lib/python3.13/site-packages/torch/nn/attention/__init__.py:# TODO: Consider using this for sdpa regardless of subclasses\n./.venv/lib/python3.13/site-packages/torch/nn/attention/flex_attention.py:            # TODO: support CPU for training and return lse\n./.venv/lib/python3.13/site-packages/torch/nn/attention/flex_attention.py:    \"\"\"TODO: Remove once non cuda/cpu devices support is added\n./.venv/lib/python3.13/site-packages/torch/nn/attention/bias.py:                    is_causal=True,  # TODO: Flash accepts causal = True and for this particular op it means lower right\n./.venv/lib/python3.13/site-packages/torch/nn/modules/padding.py:# TODO: grad_output size asserts in THNN\n./.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:    # TODO: Change `*args` to `*` and remove the corresponding warning in docs when BC allows.\n./.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:        # TODO: Remove `args` and the parsing logic when BC allows.\n./.venv/lib/python3.13/site-packages/torch/nn/modules/normalization.py:# TODO: ContrastiveNorm2d\n./.venv/lib/python3.13/site-packages/torch/nn/modules/normalization.py:# TODO: DivisiveNorm2d\n./.venv/lib/python3.13/site-packages/torch/nn/modules/normalization.py:# TODO: SubtractiveNorm2d\n./.venv/lib/python3.13/site-packages/torch/nn/modules/loss.py:# TODO: L1HingeEmbeddingCriterion\n./.venv/lib/python3.13/site-packages/torch/nn/modules/loss.py:# TODO: MSECriterion weight\n./.venv/lib/python3.13/site-packages/torch/nn/modules/loss.py:# TODO: ClassSimplexCriterion\n./.venv/lib/python3.13/site-packages/torch/nn/modules/batchnorm.py:            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n./.venv/lib/python3.13/site-packages/torch/nn/modules/linear.py:# TODO: fail fast on quantization API usage error, then remove this class\n./.venv/lib/python3.13/site-packages/torch/nn/modules/linear.py:# TODO: PartialLinear - maybe in sparse?\n./.venv/lib/python3.13/site-packages/torch/nn/modules/conv.py:        padding_mode: str = \"zeros\",  # TODO: refine this type\n./.venv/lib/python3.13/site-packages/torch/nn/modules/conv.py:        padding_mode: str = \"zeros\",  # TODO: refine this type\n./.venv/lib/python3.13/site-packages/torch/nn/modules/conv.py:# TODO: Deprecate and remove the following alias `_ConvTransposeMixin`.\n./.venv/lib/python3.13/site-packages/torch/nn/modules/conv.py:# TODO: Conv2dLocal\n./.venv/lib/python3.13/site-packages/torch/nn/modules/conv.py:# TODO: Conv2dMap\n./.venv/lib/python3.13/site-packages/torch/nn/modules/conv.py:# TODO: ConvTranspose2dMap\n./.venv/lib/python3.13/site-packages/torch/nn/modules/conv.py:        padding_mode: str = \"zeros\",  # TODO: refine this type\n./.venv/lib/python3.13/site-packages/torch/nn/modules/rnn.py:# TODO: remove the overriding implementations for LSTM and GRU when TorchScript\n./.venv/lib/python3.13/site-packages/torch/nn/modules/rnn.py:            ret = input  # TODO: remove when jit supports exception flow\n./.venv/lib/python3.13/site-packages/torch/nn/modules/activation.py:        # TODO: check in THNN (if inplace == True, then assert value <= threshold)\n./.venv/lib/python3.13/site-packages/torch/nn/parallel/comm.py:    # TODO: When `len(inputs) == 1` and all inputs are on `destination`, just\n./.venv/lib/python3.13/site-packages/torch/nn/parallel/distributed.py:    # TODO (rohan-varma): keep_low_precision_grads: bool = False\n./.venv/lib/python3.13/site-packages/torch/nn/parallel/distributed.py:    # TODO (rohan-varma): APIs to allow users to run batchnorm and layernorm\n./.venv/lib/python3.13/site-packages/torch/nn/parallel/distributed.py:        # TODO: Expand to remote RRefs.\n./.venv/lib/python3.13/site-packages/torch/nn/parallel/distributed.py:        # TODO: make DDP uneven inputs context manager support buffer\n./.venv/lib/python3.13/site-packages/torch/nn/parallel/distributed.py:                # TODO: This is a temporary work around to enable DDP + TP.\n./.venv/lib/python3.13/site-packages/torch/nn/parallel/distributed.py:            # params. TODO (rohan-varma): Make this compose with general\n./.venv/lib/python3.13/site-packages/torch/nn/parallel/distributed.py:                        # TODO: when zero_grad(set_to_none=False) or in grad\n./.venv/lib/python3.13/site-packages/torch/nn/parallel/distributed.py:            # TODO (rohan-varma) test this codepath.\n./.venv/lib/python3.13/site-packages/torch/nn/parallel/distributed.py:        # TODO: DDPSink is currently enabled for unused parameter detection and\n./.venv/lib/python3.13/site-packages/torch/nn/parallel/data_parallel.py:    # TODO: update notes/cuda.rst when this class handles 8+ GPUs well\n./.venv/lib/python3.13/site-packages/torch/nn/utils/parametrize.py:                        # TODO: Fix this for tensor subclasses that are parameters:\n./.venv/lib/python3.13/site-packages/torch/nn/utils/memory_format.py:    # TODO: expand this to `_ConvNd` when channels_last support is extended\n./.venv/lib/python3.13/site-packages/torch/nn/utils/memory_format.py:    # TODO: expand this to `_ConvNd` when channels_last support is extended\n./.venv/lib/python3.13/site-packages/torch/nn/utils/prune.py:    # TODO: consider removing this check and allowing users to specify\n./.venv/lib/python3.13/site-packages/torch/nn/utils/stateless.py:    # TODO allow kwargs such as unsafe and others for parametrization\n./.venv/lib/python3.13/site-packages/torch/nn/utils/weight_norm.py:    # TODO Make return type more specific\n./.venv/lib/python3.13/site-packages/torch/nn/utils/rnn.py:        # TODO: Re-enable this check (.type isn't supported in TorchScript)\n./.venv/lib/python3.13/site-packages/torch/nn/functional.py:    # TODO: Properly support no-batch-dim inputs. For now, these are NOT supported; passing\n./.venv/lib/python3.13/site-packages/torch/nn/functional.py:            # TODO: Remove this once script supports type() calls\n./.venv/lib/python3.13/site-packages/torch/nn/functional.py:    # TODO: make use of reduce like below when JIT is ready with the missing features:\n./.venv/lib/python3.13/site-packages/torch/nn/functional.py:# TODO: Fix via https://github.com/pytorch/pytorch/issues/75798\n./.venv/lib/python3.13/site-packages/torch/nn/functional.py:        # TODO finish disentangling control flow so we don't do in-projections when statics are passed\n./.venv/lib/python3.13/site-packages/torch/nn/functional.py:        # TODO finish disentangling control flow so we don't do in-projections when statics are passed\n./.venv/lib/python3.13/site-packages/torch/nn/_reduction.py:        ret = -1  # TODO: remove once JIT exceptions support control flow\n./.venv/lib/python3.13/site-packages/torch/_ops.py:        # TODO: The cache is NOT currently used by HigherOrderOperator, but it should!\n./.venv/lib/python3.13/site-packages/torch/_ops.py:                # TODO(voz): Should we replace setting DispatchKey.Python entirely with setting mode keys?\n./.venv/lib/python3.13/site-packages/torch/_ops.py:                        # TODO(rzou): we should support torch_dispatch calling convention too.\n./.venv/lib/python3.13/site-packages/torch/_ops.py:                    # TODO(rzou): we should support torch_dispatch calling convention too.\n./.venv/lib/python3.13/site-packages/torch/_ops.py:                # TODO: We also need to handle tensor subclasses here\n./.venv/lib/python3.13/site-packages/torch/_ops.py:                # TODO(voz): We should walk all the nodes here / turn it into a list, topmode is ok for now.\n./.venv/lib/python3.13/site-packages/torch/_ops.py:        # TODO: We could potentially have lots of debugging wrappers against\n./.venv/lib/python3.13/site-packages/torch/_ops.py:    # TODO: add more methods to expose information about input and output arguments\n./.venv/lib/python3.13/site-packages/torch/_ops.py:        # TODO: we should be calling the fallback for these, but a fallthrough is almost close\n./.venv/lib/python3.13/site-packages/torch/_ops.py:            # TODO: disallow access to overloads registered by JIT\n./.venv/lib/python3.13/site-packages/torch/_ops.py:    # TODO: use this to make a __dir__\n./.venv/lib/python3.13/site-packages/torch/nested/__init__.py:            # TODO: Just use nt.to(layout=layout) when it exists.\n./.venv/lib/python3.13/site-packages/torch/nested/__init__.py:        # TODO: switch to as_nested_tensor(tensor) when it is available\n./.venv/lib/python3.13/site-packages/torch/nested/__init__.py:            # TODO: Truly support offsets=None at some point?\n./.venv/lib/python3.13/site-packages/torch/nested/_internal/ops.py:            # TODO: write a kernel for this\n./.venv/lib/python3.13/site-packages/torch/nested/_internal/ops.py:            # TODO: We probably want the output to have the same ragged structure / nested int.\n./.venv/lib/python3.13/site-packages/torch/nested/_internal/ops.py:        # TODO: eventually do a direct copy when this is possible\n./.venv/lib/python3.13/site-packages/torch/nested/_internal/ops.py:    # TODO: Back these with proper kernels (e.g. grouped GEMM)\n./.venv/lib/python3.13/site-packages/torch/nested/_internal/ops.py:    # TODO: Do this for all other views!\n./.venv/lib/python3.13/site-packages/torch/nested/_internal/ops.py:    # TODO: make this more efficient\n./.venv/lib/python3.13/site-packages/torch/nested/_internal/ops.py:    # TODO: Handle inference mode properly.\n./.venv/lib/python3.13/site-packages/torch/nested/_internal/ops.py:    # TODO: Handle the rest of output_size\n./.venv/lib/python3.13/site-packages/torch/nested/_internal/ops.py:    # TODO: Support this if needed; determine if NJT buffers need be unwrapped as dense.\n./.venv/lib/python3.13/site-packages/torch/nested/_internal/nested_tensor.py:    # TODO: Revisit this when @properties are better supported by PT2. I think the ideal\n./.venv/lib/python3.13/site-packages/torch/nested/_internal/nested_tensor.py:    # reasons. TODO: Remove these!\n./.venv/lib/python3.13/site-packages/torch/nested/_internal/nested_tensor.py:    # TODO: Remove this in favor of the default tensor subclass serialization logic.\n./.venv/lib/python3.13/site-packages/torch/nested/_internal/nested_tensor.py:# TODO: Remove ViewBufferFromNested, ViewNestedFromBuffer, and buffer_from_jagged once the\n./.venv/lib/python3.13/site-packages/torch/nested/_internal/nested_tensor.py:        # TODO: An alternative way to construct offsets is to use F.pad. This avoids creating\n./.venv/lib/python3.13/site-packages/torch/nested/_internal/sdpa.py:        # TODO: Figure out whether masks are actually supported for this layout or not\n./.venv/lib/python3.13/site-packages/torch/nested/_internal/sdpa.py:        # TODO: Explore performance impact of copying\n./.venv/lib/python3.13/site-packages/torch/nested/_internal/sdpa.py:        # TODO: Explore performance impact of copying\n./.venv/lib/python3.13/site-packages/torch/nested/_internal/sdpa.py:        # TODO: Explore performance impact when compiling\n./.venv/lib/python3.13/site-packages/torch/nested/_internal/sdpa.py:# TODO: Next iteration should add test cases and check it works\n./.venv/lib/python3.13/site-packages/torch/nested/_internal/sdpa.py:    # [TODO] K and V have to have the same Nnz, should probably torch_check\n./.venv/lib/python3.13/site-packages/torch/nested/_internal/sdpa.py:# TODO: coalesce with torch/nn/utils/attention.py\n./.venv/lib/python3.13/site-packages/torch/nested/_internal/sdpa.py:    # TODO: Investigate why math.sqrt() isn't properly handled by Dynamo?\n./.venv/lib/python3.13/site-packages/torch/masked/_ops.py:    assert mask.dense_dim() == input.dense_dim()  # TODO: eliminate this restriction\n./.venv/lib/python3.13/site-packages/torch/masked/_ops.py:    # TODO: implement sparse CSR specific where operator for efficiency\n./.venv/lib/python3.13/site-packages/torch/masked/_ops.py:            # TODO: compute count analytically\n./.venv/lib/python3.13/site-packages/torch/masked/_ops.py:            # TODO: compute count analytically\n./.venv/lib/python3.13/site-packages/torch/masked/_ops.py:        # TODO: replace torch.subtract/divide/square/maximum with\n./.venv/lib/python3.13/site-packages/torch/masked/_ops.py:    # TODO: eliminate mask_input as unnecessary when using masked divide.\n./.venv/lib/python3.13/site-packages/torch/masked/_ops.py:        # TODO: replace torch.maximum with masked maximum when available.\n./.venv/lib/python3.13/site-packages/torch/masked/_ops.py:        # TODO: replace torch.divide with masked divide when available.\n./.venv/lib/python3.13/site-packages/torch/masked/maskedtensor/reductions.py:        # TODO: autograd.Function doesn't support kwarg\n./.venv/lib/python3.13/site-packages/torch/_custom_op/autograd.py:# TODO(#101191): Use the actual C++ autograd not implemented fallback,\n./.venv/lib/python3.13/site-packages/torch/_utils.py:# TODO: Once we decide to break serialization FC, `storage` no longer needs to\n./.venv/lib/python3.13/site-packages/torch/_utils.py:                # TODO: Validation currently involves an expensive traversal\n./.venv/lib/python3.13/site-packages/torch/_utils.py:# TODO: Once we decide to break serialization FC, `storage` no longer needs to\n./.venv/lib/python3.13/site-packages/torch/_lazy/__init__.py:    # TODO(whc) expand this to include backend hooks and align with XLA backend needs\n./.venv/lib/python3.13/site-packages/torch/_lazy/extract_compiled_graph.py:        # TODO: This solution is no ideal since we may miss some factory methods. In future\n./.venv/lib/python3.13/site-packages/torch/_lazy/extract_compiled_graph.py:    # TODO: this part is TS backend specific for now and will be generalized to\n./.venv/lib/python3.13/site-packages/torch/_lazy/computation.py:    TODO: This API is currently ts backend specific. We are working on\n./.venv/lib/python3.13/site-packages/torch/_lazy/computation.py:    TODO: This API is currently ts backend specific. We are working on\n./.venv/lib/python3.13/site-packages/torch/fx/node.py:# TODO: Either refactor this into 2 functions 1 dce for functional graphs and 1 dce for all graphs,\n./.venv/lib/python3.13/site-packages/torch/fx/node.py:                # TODO: THIS IS BROKEN: _get_qualified_name calls `__name__`\n./.venv/lib/python3.13/site-packages/torch/fx/_lazy_graph_module.py:    # TODO: we shold handle __reduce_deploy__ the same way as __reduce_package__,\n./.venv/lib/python3.13/site-packages/torch/fx/passes/backends/cudagraphs.py:    # TODO: why is submodules passed here\n./.venv/lib/python3.13/site-packages/torch/fx/passes/backends/cudagraphs.py:    # TODO: single node partition may be wrong due to the pessimization\n./.venv/lib/python3.13/site-packages/torch/fx/passes/runtime_assert.py:    # TODO: Request simplification on runtime asserts before emitting them\n./.venv/lib/python3.13/site-packages/torch/fx/passes/runtime_assert.py:                # TODO: Remove relaxing assert on unbacked_symint https://github.com/pytorch/pytorch/issues/119689\n./.venv/lib/python3.13/site-packages/torch/fx/passes/runtime_assert.py:                        # TODO: use ra.msg here, but it's pretty\n./.venv/lib/python3.13/site-packages/torch/fx/passes/runtime_assert.py:                    # TODO: some CSE when generating these nodes can probably\n./.venv/lib/python3.13/site-packages/torch/fx/passes/runtime_assert.py:                            # TODO: need to assert divisibility\n./.venv/lib/python3.13/site-packages/torch/fx/passes/runtime_assert.py:                        # TODO(pianpwk): calling sym_constrain_range_for_size or adding bound asserts\n./.venv/lib/python3.13/site-packages/torch/fx/passes/fake_tensor_prop.py:                # TODO: How is it possible that we get a non fake tensor?  We\n./.venv/lib/python3.13/site-packages/torch/fx/passes/pass_manager.py:        # TODO(alexbeloi): add constraint management/validation\n./.venv/lib/python3.13/site-packages/torch/fx/passes/reinplace.py:# TODO: this should be beefed up to be able to properly re-inplace with:\n./.venv/lib/python3.13/site-packages/torch/fx/passes/reinplace.py:# TODO: we should also figure this info out using torchgen.\n./.venv/lib/python3.13/site-packages/torch/fx/passes/reinplace.py:                # TODO: later, add the optimization for handling `copy_()` calls in the graph.\n./.venv/lib/python3.13/site-packages/torch/fx/passes/split_module.py:        # TODO currently placeholders/parameters aren't put into random partitions,\n./.venv/lib/python3.13/site-packages/torch/fx/passes/utils/matcher_utils.py:        # TODO: assert pattern is a connected graph\n./.venv/lib/python3.13/site-packages/torch/fx/passes/utils/matcher_utils.py:        # TODO: use a more efficient way to check if gn is matched before: two-way dict\n./.venv/lib/python3.13/site-packages/torch/fx/passes/utils/fuser_utils.py:            # TODO: do we really need copy the get_attr node into the graph?\n./.venv/lib/python3.13/site-packages/torch/fx/passes/utils/source_matcher_utils.py:        # TODO: Bypass \"torch_fn\" when \"source_fn_stack\" because now \"torch_fn\" can\n./.venv/lib/python3.13/site-packages/torch/fx/passes/_tensorify_python_scalars.py:# TODO: refactor\n./.venv/lib/python3.13/site-packages/torch/fx/passes/_tensorify_python_scalars.py:# TODO: make sure this runs before CPU->CUDA pass for cudagraph friendliness\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/symbolic_shapes.py:        # TODO: There's a ref-cycle here (wrapped_f -> cumulative_cache_info\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/symbolic_shapes.py:    # TODO: do boolean equality test too, see\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/symbolic_shapes.py:        # (TODO: should be a helper for this, maybe sym_eq?  That\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/symbolic_shapes.py:    # TODO: Apparently, returning an OrderedSet here breaks\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/symbolic_shapes.py:        # TODO: Determine if this is correct\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/symbolic_shapes.py:        # TODO: DivideByKey needs to test divisibility at runtime!\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/symbolic_shapes.py:    TODO: Make Dynamo handle this appropriately if this is seen in Dynamo-ed\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/symbolic_shapes.py:    TODO: I didn't support min/max because I didn't have a use case where this\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/symbolic_shapes.py:        and isinstance(upper_bound, int)  # TODO: relax\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/symbolic_shapes.py:    that doesn't have a lot of safety guarantees (TODO: provide higher level\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/symbolic_shapes.py:        # TODO: check perf implications of this\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/symbolic_shapes.py:        # TODO: better printing for -oo and oo\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/symbolic_shapes.py:    # TODO: add storage offset and stride symbolic_context\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/symbolic_shapes.py:# TODO(voz): Shape env validation\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/symbolic_shapes.py:    # TODO(voz): consider a weakref to the shape_env here\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/symbolic_shapes.py:    # TODO: remove this try catch (esp for unbacked_only)\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/symbolic_shapes.py:# TODO: Deduplicate this with torch/_prims_common/__init__.py\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/symbolic_shapes.py:        # TODO(avik): https://github.com/pytorch/pytorch/issues/101093\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/symbolic_shapes.py:        # TODO: Do something nontrivial when upper_bound is expression\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/symbolic_shapes.py:        # TODO: Shouldn't we install a guard if the symbol is backed?  Or is the\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/symbolic_shapes.py:        # TODO: this does not install a deferred runtime assert yet\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/symbolic_shapes.py:        # TODO: Maybe dedupe this with _maybe_guard_rel?\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/symbolic_shapes.py:            # TODO: Actually, we can support this as long as one of them is a symbol.\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/symbolic_shapes.py:        # TODO: make this configurable from outside symbolic_context; we made a symbolic_context\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/symbolic_shapes.py:        # TODO: This should be DYNAMIC, using DUCK for BC\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/symbolic_shapes.py:                # TODO: storage_offset handling?\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/symbolic_shapes.py:        # TODO: Make this more efficient by binding all the size/stride/offsets\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/symbolic_shapes.py:                # TODO: type this better\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/symbolic_shapes.py:                        # TODO: With int_oo, I think this condition is a noop\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/symbolic_shapes.py:        # TODO: maybe it's guaranteed x in is var_to_range?\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/symbolic_shapes.py:                # TODO We could further canonicalize Eq ordering the lhs and rhs somehow\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/symbolic_shapes.py:                # TODO try to get rid of CleanDiv since it breaks the invariant thats simplifications of sympy\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/symbolic_shapes.py:        # TODO: compute hint might have gotten broken here\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/symbolic_shapes.py:            for s in sorted(fs, key=str)  # TODO: speed up sort?\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/symbolic_shapes.py:        # TODO it would seem that this pass is not necessary given the\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/symbolic_shapes.py:    # TODO: overload for allow_none literal\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/symbolic_shapes.py:        # TODO: in a Dynamo context, having user code, and having the\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/symbolic_shapes.py:            # TODO: Help text about how to use our runtime tests to fix this\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/symbolic_shapes.py:                # TODO: Rework all of this, the constraint logic is very\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/symbolic_shapes.py:            # TODO: Should we propagate size-like-ness?\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/symbolic_shapes.py:        # FIRST one most useful (TODO: Maybe we could consider tracking all of\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/symbolic_shapes.py:                        # TODO: Maybe trivial solutions for int should also be\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/symbolic_shapes.py:                            d = q / sympy.gcd(q, c)  # TODO: CleanDiv?\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/symbolic_shapes.py:        # TODO: split conjunctions and evaluate them separately\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/symbolic_shapes.py:            # TODO: does this even worked with unbacked :think:\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/symbolic_shapes.py:                    # TODO: maybe reconcile this with use of counterfactual hints\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/symbolic_shapes.py:                # TODO: dedupe this with _maybe_evaluate_static\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/symbolic_shapes.py:                        # TODO: better logging\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/symbolic_shapes.py:                # TODO: If we successfully eliminate a symbol via equality, it\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/symbolic_shapes.py:        # TODO: split conjunctions and evaluate them separately\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/symbolic_shapes.py:            # TODO: assert bool(static_expr)\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/symbolic_shapes.py:            # TODO: Do this in a way that is less janky than int(s.name[1:])\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/graph_gradual_typechecker.py:    # TODO. We leave it like this till we add a type to represent tensor sizes\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/accelerator_partitioner.py:                # TODO: add different size support for sparse_nn_partition\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/_config.py:# TODO: Perhaps consider allowing unions for the configs below (so you can hit\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/validator.py:            # TODO: Probably OK to relax this and allow lower precision\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/meta_tracer.py:            # TODO\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/merge_matmul.py:        # TODO: Properly handle aliasing caused by get_attr. For now,\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/proxy_tensor.py:            # TODO: This doesn't properly track storages.  A more robust\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/proxy_tensor.py:    # TODO: we could use types to test this\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/proxy_tensor.py:        # TODO: maybe constant SymInts should also be allowed?  Not sure if\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/proxy_tensor.py:        # TODO: inductor lowering for with_effects needs to be updated to propagate\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/proxy_tensor.py:# TODO: Make downstream users of this work with OperatorBase\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/proxy_tensor.py:            # TODO(tmanlaibaatar): we should systematically couple it with expoert verifier,\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/proxy_tensor.py:# TODO: I'm not sure what the point of this class is; you can just\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/proxy_tensor.py:        # TODO handle case where the first character of target is '*'\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/proxy_tensor.py:        # TODO: We need to explicitly import torch._dynamo before calling dispatch_trace,\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/proxy_tensor.py:                # TODO: it would be nice to line these up with the names\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/proxy_tensor.py:                # TODO: Would be nice to fix this at the source...\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/proxy_tensor.py:        # TODO: kind of a bad way to do it, should maybe figure out a better way\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/proxy_tensor.py:# TODO: this is a legacy name, there is only ever one proxy mode as it's an\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/proxy_tensor.py:        # TODO: properly compute types\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/schema_type_annotation.py:                # TODO: can we emit the union of these? What are the implications on TorchScript\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/migrate_gradual_types/constraint_generator.py:    # TODO: add the extra check mentioned here:\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/migrate_gradual_types/constraint_generator.py:        # TODO: review this rule; should input = dyn; output = dyn be included here?\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/migrate_gradual_types/constraint_generator.py:            # TODO: we should figure out why there is a key-error here.\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/migrate_gradual_types/constraint_generator.py:# TODO normalize index\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/migrate_gradual_types/constraint_generator.py:        # TODO generate add constraints for scalar addition\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/migrate_gradual_types/constraint_transformation.py:    TODO: we have to check if this is the case for all HF models\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/optimization.py:# TODO: Determine whether this can be removed after type inference.\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/sym_node.py:# TODO: An incomplete list\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/sym_node.py:        # TODO: use the file/line for some useful diagnostic on why a\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/sym_node.py:        # TODO: use the file/line for some useful diagnostic on why a\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/sym_node.py:        # TODO: use the file/line for some useful diagnostic on why a\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/sym_node.py:        # TODO: file/line here is very important, because the assert has been\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/sym_node.py:        # TODO: use the file/line for some useful diagnostic on why a\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/sym_node.py:# TODO: this probably needs the sizes-strides eval functions\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/sym_node.py:# NB: There is a TODO in C++ to allow omitting the batch dim.  If that\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/sym_node.py:    # TODO: These could also be done with indicators, maybe it is better\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/sym_node.py:    # TODO: let C++ also take advantage of this\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/sym_node.py:                # TODO: consider constant prop here\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/sym_node.py:        # TODO: consider constant prop here\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/sym_node.py:            # TODO: Remove the args construction below if a different sentinel is used by FX.\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/sym_node.py:    # TODO: This is technically hotpath, but in the ideal end state\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/sym_node.py:            # TODO: this is an awful implementation\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/sym_node.py:        # TODO: Remove eq and other relations from this list.\n./.venv/lib/python3.13/site-packages/torch/fx/experimental/sym_node.py:            # TODO: remove these\n./.venv/lib/python3.13/site-packages/torch/fx/_symbolic_trace.py:            # TODO: binary search\n./.venv/lib/python3.13/site-packages/torch/fx/_symbolic_trace.py:            # TODO: type annotations for *args and **kwargs\n./.venv/lib/python3.13/site-packages/torch/fx/_graph_pickler.py:    # TODO: This list is pretty pessimistic right now. What's the full list?\n./.venv/lib/python3.13/site-packages/torch/fx/_graph_pickler.py:        # TODO: make common w/ _output_from_cache_entry() in fake_tensor.py?\n./.venv/lib/python3.13/site-packages/torch/fx/_graph_pickler.py:            # TODO: raise a BypassFxGraphCache so we will just bypass this one...\n./.venv/lib/python3.13/site-packages/torch/fx/_graph_pickler.py:        # TODO: Do we really need all of this?\n./.venv/lib/python3.13/site-packages/torch/fx/operator_schemas.py:        # TODO: Figure out if this is safe. It seems like when generating the type signatures for\n./.venv/lib/python3.13/site-packages/torch/fx/proxy.py:    \"quantization_tag\",  # TODO deprecated\n./.venv/lib/python3.13/site-packages/torch/fx/proxy.py:    \"_numeric_debug_handle\",  # TODO deprecated\n./.venv/lib/python3.13/site-packages/torch/fx/proxy.py:        # TODO node_name_to_scope will be depreciated in favor of\n./.venv/lib/python3.13/site-packages/torch/fx/proxy.py:                # TODO: Define how to symbolically trace HigherOrderOperators\n./.venv/lib/python3.13/site-packages/torch/_subclasses/schema_check_mode.py:                # TODO: This is only OK if can't have NaN quantized; idk if\n./.venv/lib/python3.13/site-packages/torch/_subclasses/functional_tensor.py:            # TODO: right now, _make_wrapper_subclass's dynamic shape interaction is not great.\n./.venv/lib/python3.13/site-packages/torch/_subclasses/functional_tensor.py:    # TODO(sparse-team): fixes #133174 but can we do without the relay?\n./.venv/lib/python3.13/site-packages/torch/_subclasses/functional_tensor.py:    # TODO: pull these from aot autograd\n./.venv/lib/python3.13/site-packages/torch/_subclasses/meta_utils.py:        # TODO: move \"assert_eq(m1.layout, m2.layout)\" out of sparse\n./.venv/lib/python3.13/site-packages/torch/_subclasses/meta_utils.py:        # TODO: test if is resizable (no direct query for this atm)\n./.venv/lib/python3.13/site-packages/torch/_subclasses/meta_utils.py:        # TODO: audit AutogradMeta to see if it matches\n./.venv/lib/python3.13/site-packages/torch/_subclasses/meta_utils.py:        # TODO: test forward AD\n./.venv/lib/python3.13/site-packages/torch/_subclasses/meta_utils.py:            # TODO: TBH, functorch wrapped tensors probably should have\n./.venv/lib/python3.13/site-packages/torch/_subclasses/meta_utils.py:                # TODO: It's pretty suspicious that functional tensors don't have\n./.venv/lib/python3.13/site-packages/torch/_subclasses/meta_utils.py:        # TODO: Is it important to enable torch.inference_mode before querying\n./.venv/lib/python3.13/site-packages/torch/_subclasses/meta_utils.py:            # TODO: I actually think recursing here is correct, but we have at\n./.venv/lib/python3.13/site-packages/torch/_subclasses/meta_utils.py:        # TODO: how to check _TensorT?\n./.venv/lib/python3.13/site-packages/torch/_subclasses/meta_utils.py:            # TODO: make a dedicated UnknownSource for this?\n./.venv/lib/python3.13/site-packages/torch/_subclasses/meta_utils.py:                    # TODO: deduplicate this\n./.venv/lib/python3.13/site-packages/torch/_subclasses/meta_utils.py:                    # TODO: Change this logic to use view replay for consistency?\n./.venv/lib/python3.13/site-packages/torch/_subclasses/meta_utils.py:                    # TODO: Handle this better in Dynamo?\n./.venv/lib/python3.13/site-packages/torch/_subclasses/meta_utils.py:                    # TODO: This doesn't seem right, where's the MKLDNN'ness\n./.venv/lib/python3.13/site-packages/torch/_subclasses/meta_utils.py:                        # TODO: why aren't the recursive calls going to\n./.venv/lib/python3.13/site-packages/torch/_subclasses/meta_utils.py:                                # TODO: Actually this all probably doesn't\n./.venv/lib/python3.13/site-packages/torch/_subclasses/meta_utils.py:                            # TODO: is_leaf/requires_grad?\n./.venv/lib/python3.13/site-packages/torch/_subclasses/meta_utils.py:                    # TODO: Use a valid grad-specific symbolic context instead of recycling\n./.venv/lib/python3.13/site-packages/torch/_subclasses/meta_utils.py:        # TODO: zero tensors?  We appear to have eliminated them by\n./.venv/lib/python3.13/site-packages/torch/_subclasses/meta_utils.py:        # TODO: This can probably be simplified quite a bit\n./.venv/lib/python3.13/site-packages/torch/_subclasses/meta_utils.py:        # TODO: return the description for later\n./.venv/lib/python3.13/site-packages/torch/_subclasses/fake_impls.py:    # TODO: no real reason to restrict multiple outputs\n./.venv/lib/python3.13/site-packages/torch/_subclasses/fake_impls.py:        # TODO: file issue\n./.venv/lib/python3.13/site-packages/torch/_subclasses/fake_impls.py:    # TODO: I think this does the wrong thing if r is inp\n./.venv/lib/python3.13/site-packages/torch/_subclasses/fake_impls.py:    # TODO: remove me\n./.venv/lib/python3.13/site-packages/torch/_subclasses/fake_impls.py:        # TODO: consider a memo\n./.venv/lib/python3.13/site-packages/torch/_subclasses/fake_impls.py:            # TODO: We can make this a little more faithful with best effort\n./.venv/lib/python3.13/site-packages/torch/_subclasses/fake_impls.py:            # TODO: Minor optimization: track if the shapes\n./.venv/lib/python3.13/site-packages/torch/_subclasses/fake_impls.py:            # TODO: we don't need the compute type\n./.venv/lib/python3.13/site-packages/torch/_subclasses/fake_impls.py:        # TODO: is_non-overlapping_and_dense (not bound from Python\n./.venv/lib/python3.13/site-packages/torch/_subclasses/fake_utils.py:                # TODO: enable_python_dispatcher() here\n./.venv/lib/python3.13/site-packages/torch/_subclasses/fake_tensor.py:# TODO: Hack to unblock https://github.com/pytorch/pytorch/pull/108186\n./.venv/lib/python3.13/site-packages/torch/_subclasses/fake_tensor.py:                    # TODO: callback might be used in recursive contexts, in\n./.venv/lib/python3.13/site-packages/torch/_subclasses/fake_tensor.py:    # TODO: Generalize this as needed, e.g., into a trie of memos, if\n./.venv/lib/python3.13/site-packages/torch/_subclasses/fake_tensor.py:    def __torch_dispatch__(  # type: ignore[override] # TODO\n./.venv/lib/python3.13/site-packages/torch/_subclasses/fake_tensor.py:        # TODO: This is a temporary measure, see\n./.venv/lib/python3.13/site-packages/torch/_subclasses/fake_tensor.py:        # TODO: support caching sparse outputs?\n./.venv/lib/python3.13/site-packages/torch/_subclasses/fake_tensor.py:                # TODO: Is this really needed?\n./.venv/lib/python3.13/site-packages/torch/_subclasses/fake_tensor.py:                    # TODO: Remove these exclusions, so that we can remove\n./.venv/lib/python3.13/site-packages/torch/_subclasses/fake_tensor.py:        # TODO - we should be use the prim aten impl\n./.venv/lib/python3.13/site-packages/torch/_subclasses/fake_tensor.py:        # TODO - fix prims complex ops\n./.venv/lib/python3.13/site-packages/torch/_subclasses/fake_tensor.py:    # TODO: also check metadata change on inputs\n./.venv/lib/python3.13/site-packages/torch/distributed/_tensor/__init__.py:    # TODO: _shards_wrapper/_utils here mainly for checkpoint BC, remove them\n./.venv/lib/python3.13/site-packages/torch/distributed/_tensor/placement_types.py:TODO: throw warnings when this module imported\n./.venv/lib/python3.13/site-packages/torch/distributed/_tensor/api.py:TODO: throw warnings when this module imported\n./.venv/lib/python3.13/site-packages/torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py:    # TODO: Importing inside function to avoid circular import issue between FSDP and\n./.venv/lib/python3.13/site-packages/torch/distributed/algorithms/_optimizer_overlap/optimizer_overlap.py:    # TODO: register_fsdp once FSDP supports communication hook.\n./.venv/lib/python3.13/site-packages/torch/distributed/algorithms/ddp_comm_hooks/optimizer_overlap_hooks.py:            # not average. TODO: (rohan-varma) the div factor may be different\n./.venv/lib/python3.13/site-packages/torch/distributed/algorithms/ddp_comm_hooks/optimizer_overlap_hooks.py:            # TODO (rohan-varma): upcast as needed for DDP mixed precision,\n./.venv/lib/python3.13/site-packages/torch/distributed/algorithms/ddp_comm_hooks/powerSGD_hook.py:        # TODO: The above procedure does two matmul+allreduce steps per iteration --\n./.venv/lib/python3.13/site-packages/torch/distributed/algorithms/ddp_comm_hooks/powerSGD_hook.py:        # TODO: The above procedure does two matmul+allreduce steps per iteration --\n./.venv/lib/python3.13/site-packages/torch/distributed/_shard/sharding_spec/chunk_sharding_spec_ops/embedding_bag.py:    # TODO: Make the result a PartialTensor and move the logic below there.\n./.venv/lib/python3.13/site-packages/torch/distributed/_shard/sharding_spec/chunk_sharding_spec_ops/embedding.py:    # TODO: Make the result a PartialTensor.\n./.venv/lib/python3.13/site-packages/torch/distributed/_shard/sharding_spec/_internals.py:        # TODO: Can we improve this error message to point out the gaps?\n./.venv/lib/python3.13/site-packages/torch/distributed/_shard/sharding_spec/chunk_sharding_spec.py:        # TODO: support named dimension\n./.venv/lib/python3.13/site-packages/torch/distributed/_shard/sharding_spec/api.py:        # TODO: figure out a generic and efficient way to scatter the shards for EnumerableShardingSpec\n./.venv/lib/python3.13/site-packages/torch/distributed/_shard/sharded_tensor/_ops/tensor_ops.py:# TODO: set grad with a ShardedTensor that consists of all local grads\n./.venv/lib/python3.13/site-packages/torch/distributed/_shard/sharded_tensor/api.py:            # TODO make it as a view of out tensor\n./.venv/lib/python3.13/site-packages/torch/distributed/_shard/sharded_tensor/api.py:        # TODO: make this a __torch_function__ op once ShardedTensor becomes a\n./.venv/lib/python3.13/site-packages/torch/distributed/_shard/sharded_tensor/api.py:        # TODO: figure out what the API should behave when some rank have no shard\n./.venv/lib/python3.13/site-packages/torch/distributed/_shard/sharded_optim/api.py:        # TODO: implement state_dict\n./.venv/lib/python3.13/site-packages/torch/distributed/_shard/sharded_optim/api.py:        # TODO: implement load_state_dict\n./.venv/lib/python3.13/site-packages/torch/distributed/_shard/sharded_optim/api.py:        # TODO: implement add_param_group\n./.venv/lib/python3.13/site-packages/torch/distributed/_composable/contract.py:# TODO: we can add additional info to RegistryItem to share across APIs. E.g.,\n./.venv/lib/python3.13/site-packages/torch/distributed/_composable/contract.py:            # TODO: verify that installed distributed paradigms are compatible with\n./.venv/lib/python3.13/site-packages/torch/distributed/_composable/contract.py:                {},  # TODO(@yhcharles): this is a temporary fix, need a better way\n./.venv/lib/python3.13/site-packages/torch/distributed/_composable/fsdp/fully_shard.py:# TODO: For backward compatibility, we are importing the public objects\n./.venv/lib/python3.13/site-packages/torch/distributed/_composable/replicate.py:        # TODO(@fegin): this variable is originally create for testing, we\n./.venv/lib/python3.13/site-packages/torch/distributed/_composable/replicate.py:    # TODO(fegin): using kwargs is not a good idea if we would like to make\n./.venv/lib/python3.13/site-packages/torch/distributed/_composable/replicate.py:            # TODO: This is a temporary work around to enable DDP + TP.\n./.venv/lib/python3.13/site-packages/torch/distributed/optim/named_optimizer.py:    TODO: Add tutorial for _NamedOptimizer.\n./.venv/lib/python3.13/site-packages/torch/distributed/optim/named_optimizer.py:    TODO: Add documentation in the docstring for the public attributes\n./.venv/lib/python3.13/site-packages/torch/distributed/optim/named_optimizer.py:        # TODO(chienchin): This API should be FSDP agnostic and should support\n./.venv/lib/python3.13/site-packages/torch/distributed/optim/named_optimizer.py:        # TODO(chienchin): This API should be FSDP agnostic and should support\n./.venv/lib/python3.13/site-packages/torch/distributed/optim/apply_optimizer_in_backward.py:            # TODO: Remove these attributes once we have a better way of accessing\n./.venv/lib/python3.13/site-packages/torch/distributed/optim/functional_sgd.py:        # TODO: Once step_param interface is robust, refactor step to call\n./.venv/lib/python3.13/site-packages/torch/distributed/optim/functional_adagrad.py:        # TODO: no union or any types in TorchScript, make step a scalar tensor instead\n./.venv/lib/python3.13/site-packages/torch/distributed/optim/optimizer.py:# TODO (wanchaol): remove this once we added TorchScript\n./.venv/lib/python3.13/site-packages/torch/distributed/optim/optimizer.py:# TODO (wanchaol): remove/merge this with ScriptLocalOptimizer once\n./.venv/lib/python3.13/site-packages/torch/distributed/optim/optimizer.py:    # TODO: improve error propagation\n./.venv/lib/python3.13/site-packages/torch/distributed/optim/zero_redundancy_optimizer.py:        # TODO: Manually add `self.param_groups` if using a functional\n./.venv/lib/python3.13/site-packages/torch/distributed/pipelining/microbatch.py:                # TODO: check type of v. If it's a tensor, use chunk (or debug mask).\n./.venv/lib/python3.13/site-packages/torch/distributed/pipelining/microbatch.py:    # TODO: _debug_mask_minibatches\n./.venv/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py:    # TODO(whc) rename to _ActType?\n./.venv/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py:                # TODO make a real 'None action' that prints as empty string and make mypy happy\n./.venv/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py:                    # TODO we can avoid send/recv if the 2 stages are on the same rank.\n./.venv/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py:        TODO: Does not use sorted_batch_isend_irecv(). As a result, this schedule does\n./.venv/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py:            # TODO: assumption that stages only communicate from distances of +1/-1 (no skip connections)\n./.venv/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py:                                # TODO: We are assuming that stage will always receive from stage-1\n./.venv/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py:                                # TODO: We are assuming that stage will always receive from stage+1\n./.venv/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py:            # TODO what level of validation should we offer for compute+comms schedule?\n./.venv/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py:        # TODO should there be an option to dump the compute_only schedule from PipelineScheduleRuntime? It's possible\n./.venv/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py:        TODO: Does not use sorted_batch_isend_irecv(). As a result, this schedule does\n./.venv/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py:                # TODO(whc) it's not actually safe to use _batch_p2p here in the uncommon case the model has skip-connections,\n./.venv/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py:                # TODO(whc) what is the best practice for printing a multiline log?\n./.venv/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py:            # TODO: we don't need to always append, after all 1f1b are finished we can stop appending None\n./.venv/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py:        # TODO: we don't support Zero Bubble with torch.compile so we\n./.venv/lib/python3.13/site-packages/torch/distributed/pipelining/_backward.py:        # TODO: Handle case where intermediate can have multiple outputs\n./.venv/lib/python3.13/site-packages/torch/distributed/pipelining/_backward.py:# TODO: handling requires_grad=False dynamically. Can we analyze this during initial\n./.venv/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py:    TODO: should we be stricter about asserting that stage modules (intermediate and output) all return only Tensor\n./.venv/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py:        # TODO: this is needed for backward_maybe_with_nosync\n./.venv/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py:            # TODO: confirm, do we use this activation as the root of the backward call for the previous stage? does\n./.venv/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py:            # TODO: We may want to change our semantics so we are allowed to ignore\n./.venv/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py:                # TODO: we dont need to save this, add to dw_runner?\n./.venv/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py:                # TODO: figure out a better way to do this:\n./.venv/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py:            # TODO why is there a separate recv_info for each pipeline chunk?\n./.venv/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py:            # TODO- need a mapping of kwarg to position in self.args_recv_info\n./.venv/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py:        # TODO- need a mapping of kwarg to position in self.args_recv_info\n./.venv/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py:        # TODO(whc)\n./.venv/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py:            # TODO: otherwise needs grad accumulation\n./.venv/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py:        # TODO move self.device to an argument from step API (from its input tensors)?\n./.venv/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py:        assert num_microbatches is not None, \"TODO fix num_microbatches\"\n./.venv/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py:        # TODO: create args_recv_info lazily? (same needed for PipelineStage)\n./.venv/lib/python3.13/site-packages/torch/distributed/pipelining/_IR.py:# TODO:\n./.venv/lib/python3.13/site-packages/torch/distributed/pipelining/_IR.py:    # Collect metadata about tuple output values. TODO: move this to split_module or FX IR\n./.venv/lib/python3.13/site-packages/torch/distributed/pipelining/_IR.py:        # TODO: is there a way not to hard wire init?\n./.venv/lib/python3.13/site-packages/torch/distributed/pipelining/_IR.py:        # TODO: investigate\n./.venv/lib/python3.13/site-packages/torch/distributed/pipelining/_IR.py:        # TODO: what does split do with module invocations? does it move the modules\n./.venv/lib/python3.13/site-packages/torch/distributed/pipelining/_IR.py:        # TODO: backport this into split_module\n./.venv/lib/python3.13/site-packages/torch/distributed/pipelining/_IR.py:                    persistent=True,  # TODO: handle non-persistent buffer\n./.venv/lib/python3.13/site-packages/torch/distributed/pipelining/_IR.py:        # TODO? Not sure yet.\n./.venv/lib/python3.13/site-packages/torch/distributed/pipelining/_IR.py:    # TODO: make this implementation out-of-place?\n./.venv/lib/python3.13/site-packages/torch/distributed/_tools/runtime_estimator.py:        # TODO: also check metadata change on inputs\n./.venv/lib/python3.13/site-packages/torch/distributed/_tools/runtime_estimator.py:        # TODO: @sanketpurandare: Flatten tensors by desugaring the tensor subclasses\n./.venv/lib/python3.13/site-packages/torch/distributed/_tools/runtime_estimator.py:        # TODO: @sanketpurandare: Add logic for incorporating communication time\n./.venv/lib/python3.13/site-packages/torch/distributed/_tools/fsdp2_mem_tracker.py:        # TODO(@sanketpurandare): This will need to be modified after this PR (https://github.com/pytorch/pytorch/pull/127786)\n./.venv/lib/python3.13/site-packages/torch/distributed/_tools/fake_collectives.py:            # TODO(@sanketpurandare) - Confirm size computation\n./.venv/lib/python3.13/site-packages/torch/distributed/_tools/fake_collectives.py:            # TODO(@sanketpurandare) - Confirm size computation\n./.venv/lib/python3.13/site-packages/torch/distributed/_tools/fake_collectives.py:            # TODO(@sanketpurandare) - Confirm size computation\n./.venv/lib/python3.13/site-packages/torch/distributed/_tools/sac_estimator.py:        # TODO: Write a better explanation why this needs to be done\n./.venv/lib/python3.13/site-packages/torch/distributed/tensor/_api.py:            # TODO: support uneven sharding when global shape/stride not passed, by\n./.venv/lib/python3.13/site-packages/torch/distributed/tensor/_api.py:            # TODO: See if we need to make this run_check logic\n./.venv/lib/python3.13/site-packages/torch/distributed/tensor/_api.py:            # TODO: return the redistributed local tensor directly without\n./.venv/lib/python3.13/site-packages/torch/distributed/tensor/_api.py:        # TODO: backward is also differentiable now, add a test\n./.venv/lib/python3.13/site-packages/torch/distributed/tensor/_api.py:        # TODO: consider all_gather the local tensors for better debugging\n./.venv/lib/python3.13/site-packages/torch/distributed/tensor/_api.py:    # TODO(xilun): address sharding order\n./.venv/lib/python3.13/site-packages/torch/distributed/tensor/experimental/_attention.py:    # TODO(fegin): figure out why this is a requirement since SDPA does not have\n./.venv/lib/python3.13/site-packages/torch/distributed/tensor/experimental/_attention.py:    # TODO: remove the context parallel strategy from the default propagation\n./.venv/lib/python3.13/site-packages/torch/distributed/tensor/experimental/_attention.py:        # TODO(d4l3k); this should be Shard(2), need to fix Linear layer rules\n./.venv/lib/python3.13/site-packages/torch/distributed/tensor/experimental/_register_sharding.py:        # TODO: handle out variant ops\n./.venv/lib/python3.13/site-packages/torch/distributed/tensor/experimental/_func_map.py:                # TODO: the current code doesn't consider the uneven sharding case\n./.venv/lib/python3.13/site-packages/torch/distributed/tensor/_sharding_prop.py:        # scalar. TODO: figure out a better way to handle this\n./.venv/lib/python3.13/site-packages/torch/distributed/tensor/_op_schema.py:    TODO: make this a frozen dataclass\n./.venv/lib/python3.13/site-packages/torch/distributed/tensor/_op_schema.py:        # TODO: see if we should merge this with args_spec\n./.venv/lib/python3.13/site-packages/torch/distributed/tensor/_tp_conv.py:    # TODO: whether there requires data exchange is currently determined by padding\n./.venv/lib/python3.13/site-packages/torch/distributed/tensor/_utils.py:        # TODO: change this function to correctly address this.\n./.venv/lib/python3.13/site-packages/torch/distributed/tensor/_utils.py:        # TODO: this logic can be applied to contiguous sharding as well\n./.venv/lib/python3.13/site-packages/torch/distributed/tensor/_ops/_pointwise_ops.py:# TODO: add all for_each ops\n./.venv/lib/python3.13/site-packages/torch/distributed/tensor/_ops/_conv_ops.py:    # TODO: actually the output_mask is not respected here, we should\n./.venv/lib/python3.13/site-packages/torch/distributed/tensor/_ops/_math_ops.py:        # TODO: The diagonal ops can have an improved sharding strategy for\n./.venv/lib/python3.13/site-packages/torch/distributed/tensor/_ops/_math_ops.py:        # TODO: support the full F.interpolate set of options.\n./.venv/lib/python3.13/site-packages/torch/distributed/tensor/_ops/_math_ops.py:        # TODO: we can avoid forcing the redistribution once we figure out\n./.venv/lib/python3.13/site-packages/torch/distributed/tensor/_ops/_math_ops.py:            # TODO: we can avoid forcing the redistribution once we figure out\n./.venv/lib/python3.13/site-packages/torch/distributed/tensor/_ops/_math_ops.py:            # TODO: we can avoid forcing the redistribution once we figure out\n./.venv/lib/python3.13/site-packages/torch/distributed/tensor/_ops/_math_ops.py:        # TODO: change the strategy to the following rule.\n./.venv/lib/python3.13/site-packages/torch/distributed/tensor/_ops/_math_ops.py:        # TODO: now grad_out spec follows input spec. we may need\n./.venv/lib/python3.13/site-packages/torch/distributed/tensor/_ops/_math_ops.py:            # TODO: now d_weight spec follows input spec w/ a reduction.\n./.venv/lib/python3.13/site-packages/torch/distributed/tensor/_ops/_math_ops.py:    # TODO: topk on sharded dim requries non-trival reduction, address it later\n./.venv/lib/python3.13/site-packages/torch/distributed/tensor/_ops/utils.py:        # TODO: maybe we should determine is_shardable based on\n./.venv/lib/python3.13/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py:    #   TODO: Ideally we'd like to make sure the output is re-sharded afterwards to keep input sharding.\n./.venv/lib/python3.13/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py:    # TODO: see if we can support input sharding pattern\n./.venv/lib/python3.13/site-packages/torch/distributed/tensor/_ops/_tensor_ops.py:    TODO: exception: when the dtype of second input is \"bool\", then a torch.nonzero needs to be triggered first.\n./.venv/lib/python3.13/site-packages/torch/distributed/tensor/_ops/_einsum_strategy.py:    # TODO: filter out invalid strategies, at this point we generate\n./.venv/lib/python3.13/site-packages/torch/distributed/tensor/_ops/_common_rules.py:                # TODO: further merge the sharding properly (i.e. reshard one input to replicate)\n./.venv/lib/python3.13/site-packages/torch/distributed/tensor/_ops/_common_rules.py:            # TODO: consider a more advanced heuristic to pick the best sharding\n./.venv/lib/python3.13/site-packages/torch/distributed/tensor/_ops/_random_ops.py:            # TODO: figure out how inplace random op should behave when it's partial\n./.venv/lib/python3.13/site-packages/torch/distributed/tensor/_ops/_view_ops.py:        # TODO(whc) this helper is pretty hard to understand, at least it should be better documented if not refactored\n./.venv/lib/python3.13/site-packages/torch/distributed/tensor/_ops/_view_ops.py:            # TODO(whc) dim0 can be sharded or not sharded, can't it?\n./.venv/lib/python3.13/site-packages/torch/distributed/tensor/_ops/_view_ops.py:            # TODO: optimize this. we shouldn't simply blindly replicate\n./.venv/lib/python3.13/site-packages/torch/distributed/tensor/_ops/_matrix_ops.py:    # TODO: add support for these later\n./.venv/lib/python3.13/site-packages/torch/distributed/tensor/_ops/_matrix_ops.py:    # TODO: sdpa might be a good candidate for us to explore decomposed sharding propagation\n./.venv/lib/python3.13/site-packages/torch/distributed/tensor/_ops/_matrix_ops.py:    # TODO(d4l3k); implement a more correct strategy for constant_pad_nd\n./.venv/lib/python3.13/site-packages/torch/distributed/tensor/placement_types.py:    TODO: we should remove _StridedShard placement once we can unify it with Shard\n./.venv/lib/python3.13/site-packages/torch/distributed/tensor/placement_types.py:            # TODO: this is to avoid extra all-gather in dtensor op dispatch\n./.venv/lib/python3.13/site-packages/torch/distributed/tensor/placement_types.py:        # TODO: if the reduce_op is min/max, etc. the _partition_value should be a\n./.venv/lib/python3.13/site-packages/torch/distributed/tensor/_collective_utils.py:    # TODO: enable async op for shard_dim_alltoall\n./.venv/lib/python3.13/site-packages/torch/distributed/tensor/_collective_utils.py:    # TODO: Ideally we should use the meta tensor way\n./.venv/lib/python3.13/site-packages/torch/distributed/tensor/_collective_utils.py:    # TODO: Ideally we should use the meta tensor way\n./.venv/lib/python3.13/site-packages/torch/distributed/tensor/_collective_utils.py:                # TODO: see if we need to tweak this or offer a way for user\n./.venv/lib/python3.13/site-packages/torch/distributed/tensor/_collective_utils.py:        # TODO: see if we want to support this once there's cross mesh communication\n./.venv/lib/python3.13/site-packages/torch/distributed/tensor/parallel/ddp.py:    # TODO: To add perf optimizations to this iterations\n./.venv/lib/python3.13/site-packages/torch/distributed/tensor/parallel/ddp.py:    # TODO: To add test cases and ensure that it works for nested modules\n./.venv/lib/python3.13/site-packages/torch/distributed/tensor/parallel/style.py:        # TODO: figure out dynamo support for instance method and switch this to instance method\n./.venv/lib/python3.13/site-packages/torch/distributed/tensor/parallel/style.py:                # TODO: re-enable the check once we fix the compile path\n./.venv/lib/python3.13/site-packages/torch/distributed/tensor/parallel/style.py:                    # TODO: re-enable the check once we fix the compile path\n./.venv/lib/python3.13/site-packages/torch/distributed/tensor/parallel/_utils.py:    # TODO: Will follow up with dynamo POC to make warnings.warn working with dynamo.\n./.venv/lib/python3.13/site-packages/torch/distributed/tensor/parallel/fsdp.py:            # TODO: this is a short term fix and we should make the get_unflat_views\n./.venv/lib/python3.13/site-packages/torch/distributed/tensor/_random.py:        # TODO: Logs way too much\n./.venv/lib/python3.13/site-packages/torch/distributed/tensor/_redistribute.py:        # TODO: alltoall/permute reshuffling to change device_mesh if they are not the same\n./.venv/lib/python3.13/site-packages/torch/distributed/nn/api/remote_module.py:            # TODO: We need to change this to rpc.remote, and make it async (see the else branch below).\n./.venv/lib/python3.13/site-packages/torch/distributed/nn/jit/templates/remote_module_template.py:# TODO: Merge these two templates together in the future once TorchScript syntax is improved.\n./.venv/lib/python3.13/site-packages/torch/distributed/fsdp/_optim_utils.py:        # TODO: This solution is not general and only apply to PTD TP solution.\n./.venv/lib/python3.13/site-packages/torch/distributed/fsdp/_optim_utils.py:                # TODO: it is unclear if we need to do the same check with\n./.venv/lib/python3.13/site-packages/torch/distributed/fsdp/_exec_order_utils.py:        # TODO (awgu): We can broadcast the metadata of rank 0's `all_handles`\n./.venv/lib/python3.13/site-packages/torch/distributed/fsdp/_exec_order_utils.py:            # TODO (awgu): Since every module has at most one handle in the\n./.venv/lib/python3.13/site-packages/torch/distributed/fsdp/_exec_order_utils.py:                # TODO(voz): Don't graph break on this - dynamo hates the n1 != n2\n./.venv/lib/python3.13/site-packages/torch/distributed/fsdp/_exec_order_utils.py:                # TODO(voz): Don't graph break on this - dynamo hates the i1 != i2\n./.venv/lib/python3.13/site-packages/torch/distributed/fsdp/_common_utils.py:        # TODO: Move all the attributes to this class to enable typing for\n./.venv/lib/python3.13/site-packages/torch/distributed/fsdp/_common_utils.py:    # TODO: This is a temporary hack for differentiate between code paths.\n./.venv/lib/python3.13/site-packages/torch/distributed/fsdp/_common_utils.py:    # TODO: Explicitly replacing the checkpoint wrapper prefix is not ideal as\n./.venv/lib/python3.13/site-packages/torch/distributed/fsdp/_common_utils.py:                    # TODO: Remove this hack once DMP + FSDP is not supported.\n./.venv/lib/python3.13/site-packages/torch/distributed/fsdp/_common_utils.py:                    # TODO: Remove this hack once DMP + FSDP is not supported.\n./.venv/lib/python3.13/site-packages/torch/distributed/fsdp/_common_utils.py:            # TODO: We need to run this mixed precision ignored module in fp32,\n./.venv/lib/python3.13/site-packages/torch/distributed/fsdp/_common_utils.py:        # TODO(voz): Extend a dynamo util to answer the above, unify the codepaths here.\n./.venv/lib/python3.13/site-packages/torch/distributed/fsdp/_unshard_param_utils.py:        # TODO: figure out the case for the composable APIs.\n./.venv/lib/python3.13/site-packages/torch/distributed/fsdp/_unshard_param_utils.py:        # TODO: figure out the case for the composable APIs.\n./.venv/lib/python3.13/site-packages/torch/distributed/fsdp/_unshard_param_utils.py:        # TODO: Rank 0 can broadcast the `FlatParameter` to allow all ranks to\n./.venv/lib/python3.13/site-packages/torch/distributed/fsdp/_traversal_utils.py:    # TODO: Add any other composable APIs that are mutually exclusive.\n./.venv/lib/python3.13/site-packages/torch/distributed/fsdp/_traversal_utils.py:# TODO (awgu): We may be able to remove this function if we retired the\n./.venv/lib/python3.13/site-packages/torch/distributed/fsdp/_fully_shard/_fsdp_param_group.py:            # TODO: Find a way to print the offending FSDP2 module.\n./.venv/lib/python3.13/site-packages/torch/distributed/fsdp/_fully_shard/_fsdp_param.py:TODO:\n./.venv/lib/python3.13/site-packages/torch/distributed/fsdp/_fully_shard/_fsdp_param.py:        # TODO: Remove this padding logic once DTensor pads the local tensor:\n./.venv/lib/python3.13/site-packages/torch/distributed/fsdp/_fully_shard/_fsdp_param.py:        # TODO: Replace the sharded DTensor parameter construction logic with\n./.venv/lib/python3.13/site-packages/torch/distributed/fsdp/_fully_shard/_fsdp_param.py:        # TODO: Simplify the following sharded parameter padding logic after\n./.venv/lib/python3.13/site-packages/torch/distributed/fsdp/_fully_shard/_fsdp_param.py:        # TODO: Prefer this DTensor to be read-only and generalize the\n./.venv/lib/python3.13/site-packages/torch/distributed/fsdp/_fully_shard/_fully_shard.py:        # TODO: Remove this padding logic once DTensor pads the local tensor:\n./.venv/lib/python3.13/site-packages/torch/distributed/fsdp/_state_dict_utils.py:    # TODO: need to check if this is always correct for composable FSDP.\n./.venv/lib/python3.13/site-packages/torch/distributed/fsdp/_state_dict_utils.py:    # TODO: Add DTensor state_dict support for LOCAL_STATE_DICT.\n./.venv/lib/python3.13/site-packages/torch/distributed/fsdp/_state_dict_utils.py:    # TODO: Add DTensor state_dict support for LOCAL_STATE_DICT.\n./.venv/lib/python3.13/site-packages/torch/distributed/fsdp/_state_dict_utils.py:            continue  # TODO: Improve unittesting for state_dict finetuning\n./.venv/lib/python3.13/site-packages/torch/distributed/fsdp/_wrap_utils.py:    # TODO: We may relax this no-nested-wrapping constraint to support manual\n./.venv/lib/python3.13/site-packages/torch/distributed/fsdp/_runtime_utils.py:            # But maybe we need to? TODO(voz): Look into this\n./.venv/lib/python3.13/site-packages/torch/distributed/fsdp/_runtime_utils.py:        # TODO: Do not use the side stream for tensor copies for now; investigate\n./.venv/lib/python3.13/site-packages/torch/distributed/fsdp/_runtime_utils.py:    # TODO: Post-backward prefetching does not support the multiple handles\n./.venv/lib/python3.13/site-packages/torch/distributed/fsdp/_runtime_utils.py:    # TODO: Investigate why `NO_SHARD` breaks correctness when using\n./.venv/lib/python3.13/site-packages/torch/distributed/fsdp/_runtime_utils.py:    # TODO (rohan-varma): When CPU offload and optimizer overlap,\n./.venv/lib/python3.13/site-packages/torch/distributed/fsdp/_runtime_utils.py:                # TODO (rohan-varma): For CPU offload, this unfortunately\n./.venv/lib/python3.13/site-packages/torch/distributed/fsdp/_runtime_utils.py:        # TODO (rohan-varma): this also waits for the overlapped optimizer step to finish\n./.venv/lib/python3.13/site-packages/torch/distributed/fsdp/_runtime_utils.py:            # TODO: This already-resharded check is brittle:\n./.venv/lib/python3.13/site-packages/torch/distributed/fsdp/_flat_param.py:# TODO: Define this for now to avoid circular imports. See if we can remove.\n./.venv/lib/python3.13/site-packages/torch/distributed/fsdp/_flat_param.py:                # TODO (awgu): Gradient accumulation outside `no_sync()`\n./.venv/lib/python3.13/site-packages/torch/distributed/fsdp/_flat_param.py:            # TODO (rohan-varma): test for full precision with keep_low_precision_grads\n./.venv/lib/python3.13/site-packages/torch/distributed/fsdp/_flat_param.py:        # TODO (awgu): We should replace these conditional checks to encode\n./.venv/lib/python3.13/site-packages/torch/distributed/fsdp/_flat_param.py:                # TODO: Change `_unpadded_unsharded_size` if we change the\n./.venv/lib/python3.13/site-packages/torch/distributed/fsdp/_flat_param.py:        # TODO: If we want to handle shared parameters, we need to re-generate\n./.venv/lib/python3.13/site-packages/torch/distributed/fsdp/_init_utils.py:# TODO (awgu): Refactor this later\n./.venv/lib/python3.13/site-packages/torch/distributed/fsdp/_init_utils.py:    # TODO: FSDP's contract for buffers is not well-defined. They are\n./.venv/lib/python3.13/site-packages/torch/distributed/fsdp/_init_utils.py:    # TODO: we need to add additional check once we support FSDP + PiPPy.\n./.venv/lib/python3.13/site-packages/torch/distributed/fsdp/_init_utils.py:            # TODO: We may relax this by taking the FSDP instance's wrapped\n./.venv/lib/python3.13/site-packages/torch/distributed/fsdp/_init_utils.py:    # TODO: We need to establish a contract for FSDP and buffers. For now, we\n./.venv/lib/python3.13/site-packages/torch/distributed/fsdp/_init_utils.py:# TODO: See how to deprecate!\n./.venv/lib/python3.13/site-packages/torch/distributed/_state_dict_utils.py:    # TODO: should we use pytree?\n./.venv/lib/python3.13/site-packages/torch/distributed/_state_dict_utils.py:            # TODO: currently, we cannot handle strided sharding if the dp dimension is not even. For example,\n./.venv/lib/python3.13/site-packages/torch/distributed/_state_dict_utils.py:# TODO: We should consolidate the code here as some not all modules can depend on\n./.venv/lib/python3.13/site-packages/torch/distributed/rpc/backend_registry.py:            # TODO: make async?\n./.venv/lib/python3.13/site-packages/torch/distributed/rpc/backend_registry.py:        # TODO: add try-except and destroy _agent in all processes if any fails.\n./.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:# TODO refactor into enum/strenum\n./.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:        TODO don't expose the map, expose fine grained ops\n./.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:        TODO don't expose the map, expose fine grained ops\n./.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:        TODO don't expose the map, expose fine grained ops\n./.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:        TODO don't expose the map, expose fine grained ops\n./.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:        TODO don't expose group_count, use something else instead\n./.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:            # TODO moco benchmark on CPU initializes pgnccl backend today, triggered this assert in CI before it was\n./.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:# TODO: remove this once the ecosystem moves away from it.\n./.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:    # TODO(yifu): remove this function once ranks + tag is not a supported\n./.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:            # TODO: remove this check after lazy initialization is supported\n./.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:            # TODO: once UCC plugin is fully deprecated, remove\n./.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:        # TODO: This defaults to the old behavior for PythonProcessGroups which overwrites the\n./.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:    # TODO: We need to also support torch inductor for the time estimator.\n./.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:        # TODO(whc) apparently some existing test case for monitored_barrier passes in a timeout in float format?\n./.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:    # TODO: why is group count incremented only in the else path?\n./.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:    # TODO: Use itertools.batched(get_process_group_ranks(group=group), group_size) instead when Python 3.12 is supported.\n./.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:    # TODO copy settings and timeout from default PG\n./.venv/lib/python3.13/site-packages/torch/distributed/_functional_collectives.py:    # TODO this should be done inside AsyncCollectiveTensor to delay the wait() call\n./.venv/lib/python3.13/site-packages/torch/distributed/_functional_collectives.py:    # TODO this should be done inside AsyncCollectiveTensor to delay the wait() call\n./.venv/lib/python3.13/site-packages/torch/distributed/_functional_collectives.py:        # TODO: it should run collective in the whole mesh instead of dim 0\n./.venv/lib/python3.13/site-packages/torch/distributed/_functional_collectives.py:    # TODO(yifu): remove these in functional collective beta release\n./.venv/lib/python3.13/site-packages/torch/distributed/_functional_collectives.py:    group=None,  # TODO add a type,\n./.venv/lib/python3.13/site-packages/torch/distributed/_functional_collectives.py:    op: str = \"sum\",  # TODO type is actually c10d ReduceOp. is this ok?\n./.venv/lib/python3.13/site-packages/torch/distributed/_functional_collectives.py:    group=None,  # TODO add a type\n./.venv/lib/python3.13/site-packages/torch/distributed/_symmetric_memory/__init__.py:    TODO(yifu): the SM-based copy can be avoided with a list-based reduction\n./.venv/lib/python3.13/site-packages/torch/distributed/elastic/agent/server/api.py:    # TODO @kiuk - make entrypoint a required field\n./.venv/lib/python3.13/site-packages/torch/distributed/elastic/agent/server/api.py:        # TODO: BC - specific to static rdzv and can be simplified further\n./.venv/lib/python3.13/site-packages/torch/distributed/elastic/agent/server/api.py:        # TODO after stopping workers, wait at least monitor_interval*2 for\n./.venv/lib/python3.13/site-packages/torch/distributed/elastic/rendezvous/etcd_rendezvous.py:        # TODO: look into using weakref here instead.\n./.venv/lib/python3.13/site-packages/torch/distributed/elastic/rendezvous/etcd_rendezvous.py:# TODO: we should probably handle a few additional errors,\n./.venv/lib/python3.13/site-packages/torch/distributed/elastic/rendezvous/etcd_rendezvous.py:        # TODO: look into using weakref here instead.\n./.venv/lib/python3.13/site-packages/torch/distributed/elastic/rendezvous/etcd_rendezvous.py:        # TODO: implement timeout\n./.venv/lib/python3.13/site-packages/torch/distributed/elastic/rendezvous/api.py:        # TODO swap to collectives comms API\n./.venv/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/api.py:        # TODO log_line_prefixes can be expanded too\n./.venv/lib/python3.13/site-packages/torch/distributed/elastic/utils/distributed.py:            # TODO properly map the exceptions in pybind (c10d/init.cpp)\n./.venv/lib/python3.13/site-packages/torch/distributed/checkpoint/_nested_dict.py:TODO:\n./.venv/lib/python3.13/site-packages/torch/distributed/checkpoint/_nested_dict.py:# TODO: Update Docstring for nested_dict.py\n./.venv/lib/python3.13/site-packages/torch/distributed/checkpoint/state_dict.py:        # TODO: make this faster.\n./.venv/lib/python3.13/site-packages/torch/distributed/checkpoint/state_dict.py:            # TODO: check if value is the same if exists.\n./.venv/lib/python3.13/site-packages/torch/distributed/checkpoint/state_dict.py:# TODO: correct the state_dict function signature.\n./.venv/lib/python3.13/site-packages/torch/distributed/checkpoint/state_dict.py:# TODO: this API is not yet fully tested. Make it private\n./.venv/lib/python3.13/site-packages/torch/distributed/checkpoint/state_dict.py:# TODO: correct the load_state_dict function signature.\n./.venv/lib/python3.13/site-packages/torch/distributed/checkpoint/state_dict.py:# TODO: this API is not yet fully tested. Make it private\n./.venv/lib/python3.13/site-packages/torch/distributed/checkpoint/hf_storage.py:                # TODO: make this more efficient by doing offset reads instead of a\n./.venv/lib/python3.13/site-packages/torch/distributed/checkpoint/storage.py:        TODO: provide an example\n./.venv/lib/python3.13/site-packages/torch/distributed/checkpoint/utils.py:# TODO: integrate with distributed logging flag\n./.venv/lib/python3.13/site-packages/torch/distributed/checkpoint/format_utils.py:        # TODO: read on each host, instead of only the coordinator\n./.venv/lib/python3.13/site-packages/torch/distributed/checkpoint/state_dict_saver.py:    # TODO: test returning `save` here instead.\n./.venv/lib/python3.13/site-packages/torch/distributed/checkpoint/_sharded_tensor_utils.py:# TODO: We need to refactor this code.\n./.venv/lib/python3.13/site-packages/torch/distributed/checkpoint/_dedup_tensors.py:# TODO add docstring for dedup_tensors\n./.venv/lib/python3.13/site-packages/torch/distributed/checkpoint/planner_helpers.py:    # TODO: let state_dict_util._iterate_state_dict() to support in place option\n./.venv/lib/python3.13/site-packages/torch/distributed/checkpoint/_fsspec_filesystem.py:# TODO: add the dcp.async_save mixin\n./.venv/lib/python3.13/site-packages/torch/distributed/checkpoint/state_dict_loader.py:        # TODO: test returning `load` here instead.\n./.venv/lib/python3.13/site-packages/torch/distributed/checkpoint/default_planner.py:# TODO: Update docstrings for default_planner.py\n./.venv/lib/python3.13/site-packages/torch/distributed/checkpoint/optimizer.py:# TODO: Update docstrings for optimizer.py\n./.venv/lib/python3.13/site-packages/torch/distributed/checkpoint/optimizer.py:            # TODO: The ReadItems will have a displaced MetadataIndex, fix it.\n./.venv/lib/python3.13/site-packages/torch/distributed/checkpoint/optimizer.py:            # TODO: we should change _create_sharded_read_items to have more ergonomic API\n./.venv/lib/python3.13/site-packages/torch/distributed/checkpoint/filesystem.py:        # TODO replace with headq\n./.venv/lib/python3.13/site-packages/torch/distributed/checkpoint/filesystem.py:            # TODO: Using the OverlappingCpuLoader with multiple threads creates significant\n./.venv/lib/python3.13/site-packages/torch/distributed/checkpoint/filesystem.py:                # TODO sort by offset and cache the reading\n./.venv/lib/python3.13/site-packages/torch/distributed/checkpoint/_traverse.py:# TODO: update docstring for traverse.py\n./.venv/lib/python3.13/site-packages/torch/distributed/checkpoint/_traverse.py:        # TODO: add local offset for _local_tensor in print_nested.\n./.venv/lib/python3.13/site-packages/torch/distributed/device_mesh.py:                # TODO: we need to handle reconstructing a non-contiguous flattened dim.\n./.venv/lib/python3.13/site-packages/torch/distributed/device_mesh.py:            # TODO: If we decide to restrict flatten initialization once, we should remove\n./.venv/lib/python3.13/site-packages/torch/distributed/device_mesh.py:                    # TODO: this doesn't allow non-contiguous slicing with flatten dim yet. next_idx\n./.venv/lib/python3.13/site-packages/torch/distributed/device_mesh.py:            # TODO(yeounoh) implement DeviceMesh backend and register XLA backend.\n./.venv/lib/python3.13/site-packages/torch/distributed/device_mesh.py:            # TODO: think about how to allow pg options to be passed to world group\n./.venv/lib/python3.13/site-packages/torch/distributed/device_mesh.py:                        # TODO: Add two tests to cover internal tests scenarios and re-enable reuse subgroup if exists.\n./.venv/lib/python3.13/site-packages/torch/distributed/device_mesh.py:                # TODO: compiler + device_mesh slicing.\n./.venv/lib/python3.13/site-packages/torch/cuda/__init__.py:            # TODO(torch_deploy): this accesses linecache, which attempts to read the\n./.venv/lib/python3.13/site-packages/torch/cuda/_utils.py:    # TODO: Should we refactor flags into a common place?\n./.venv/lib/python3.13/site-packages/torch/cuda/_utils.py:            # TODO: Python floats are actually doubles\n./.venv/lib/python3.13/site-packages/torch/cuda/amp/autocast_mode.py:    # TODO: discuss a unified TorchScript-friendly API for autocast\n./.venv/lib/python3.13/site-packages/torch/multiprocessing/reductions.py:    # TODO: Handle distinguishing between subclass and non-subclass versions of NT better\n./.venv/lib/python3.13/site-packages/torch/multiprocessing/reductions.py:    # TODO: Maybe this should be in tensor_classes? :)\n./.venv/lib/python3.13/site-packages/torch/library.py:        # TODO(rzou): We're gonna need to stage this change with torchvision,\n./.venv/lib/python3.13/site-packages/torch/library.py:            # TODO: in future, add more info about where the existing function is registered (this info is\n./.venv/lib/python3.13/site-packages/torch/library.py:            # TODO: in future, add more info about where the existing function is registered (this info is\n./.venv/lib/python3.13/site-packages/torch/quantization/__init__.py:    # 'fuse_fx', 'quantize_fx',  # TODO: add quantize_dynamic_fx\n./.venv/lib/python3.13/site-packages/torch/quantization/fuse_modules.py:# TODO: These functions are not used outside the `fuse_modules.py`\n./.venv/lib/python3.13/site-packages/torch/_deploy.py:                # TODO: Once we decide to break serialization FC, we can\n./.venv/lib/python3.13/site-packages/torch/_deploy.py:            # TODO: Once we decide to break serialization FC, we can\n./.venv/lib/python3.13/site-packages/torch/share/cmake/Torch/TorchConfig.cmake:  # TODO: this list might be incomplete.\n./.venv/lib/python3.13/site-packages/torch/share/cmake/Caffe2/public/cuda.cmake:      # TODO: I'm not sure if this counts as an implementation detail of\n./.venv/lib/python3.13/site-packages/torch/share/cmake/Caffe2/public/mkl.cmake:# TODO: This is a hack, it will not pick up architecture dependent\n./.venv/lib/python3.13/site-packages/torch/_tensor_str.py:                # TODO(#146647): extend this to other dtypes without casts defined, such\n./.venv/lib/python3.13/site-packages/torch/_tensor_str.py:                # TODO(#113663): also add the other float8 dtypes here after arithmetic\n./.venv/lib/python3.13/site-packages/torch/_tensor_str.py:        # TODO(#146647): extend this to other dtypes without casts defined, such\n./.venv/lib/python3.13/site-packages/torch/_tensor_str.py:    # TODO: Remove me when `masked_select` is implemented for FP8\n./.venv/lib/python3.13/site-packages/torch/_tensor_str.py:    # TODO(albanD) This needs to be updated when more than one level is supported\n./.venv/lib/python3.13/site-packages/torch/_tensor_str.py:    # TODO: add an API to map real -> complex dtypes\n./.venv/lib/python3.13/site-packages/torch/_tensor_str.py:            # TODO: This implies that ellipses is valid syntax for allocating\n./.venv/lib/python3.13/site-packages/torch/distributions/uniform.py:        # TODO allow (loc,scale) parameterization to allow independent constraints.\n./.venv/lib/python3.13/site-packages/torch/distributions/kl.py:# TODO: Add Beta-Laplace KL Divergence\n./.venv/lib/python3.13/site-packages/torch/distributions/kl.py:# TODO: Add ContinuousBernoulli-Laplace KL Divergence\n./.venv/lib/python3.13/site-packages/torch/distributions/kl.py:# TODO: Add Exponential-Laplace KL Divergence\n./.venv/lib/python3.13/site-packages/torch/distributions/kl.py:# TODO: Add Gamma-Laplace KL Divergence\n./.venv/lib/python3.13/site-packages/torch/distributions/kl.py:# TODO: Add Gumbel-Laplace KL Divergence\n./.venv/lib/python3.13/site-packages/torch/distributions/kl.py:# TODO: Add Pareto-Laplace KL Divergence\n./.venv/lib/python3.13/site-packages/torch/distributions/kl.py:# TODO: Uniform-Laplace KL Divergence\n./.venv/lib/python3.13/site-packages/torch/distributions/laplace.py:        # TODO: If we ever implement tensor.nextafter, below is what we want ideally.\n./.venv/lib/python3.13/site-packages/torch/distributions/constraint_registry.py:# TODO define a bijection for LowerCholeskyTransform\n./.venv/lib/python3.13/site-packages/torch/__config__.py:# TODO: In principle, we could provide more structured version/config\n./.venv/lib/python3.13/site-packages/torch/_logging/_internal.py:# TODO: Maybe we should allow for some sub-hierarchy so you can control which\n./.venv/lib/python3.13/site-packages/torch/_logging/_internal.py:    # flattens all the qnames together (TODO: consider memoizing?)\n./.venv/lib/python3.13/site-packages/torch/_logging/_internal.py:    # TODO: deal with structured logging that occurs outside of specific compile ids\n./.venv/lib/python3.13/site-packages/torch/_logging/_internal.py:            # TODO: Actually, the rank probably should just be emitted once at\n./.venv/lib/python3.13/site-packages/torch/testing/_comparison.py:        # TODO: Instead of always upcasting to int64, it would be sufficient to cast to the next higher dtype to avoid\n./.venv/lib/python3.13/site-packages/torch/testing/_comparison.py:        # TODO: Remove this conversion as soon as all operations are supported natively by the MPS backend\n./.venv/lib/python3.13/site-packages/torch/testing/_comparison.py:        # TODO: compose all metas into one AssertionError\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_utils.py:# TODO: Expand this class to handle arbitrary settings in addition to boolean flags?\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_utils.py:# TODO: Remove PYTORCH_MIOPEN_SUGGEST_NHWC once ROCm officially supports NHWC in MIOpen\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_utils.py:# TODO: Revisit the relaxed pairs and check how much work it is to fix the tests that would fail without the relaxation.\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_utils.py:        # TODO: sure looks like we unconditionally initialize the context here\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_utils.py:        # TODO: Remove this; this is grandfathered in because we suppressed errors\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_utils.py:    # TODO: add args/kwargs for passing to assertEqual (e.g. rtol, atol)\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_utils.py:            # TODO: default this to True\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_utils.py:            # TODO: compose all metas into one AssertionError\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_utils.py:    # TODO: Support context manager interface\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_utils.py:# TODO: consider more complicated noncontiguity schemes\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_utils.py:# TODO: remove this (prefer make_symmetric_matrices below)\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_utils.py:# TODO: remove this (prefer make_symmetric_pd_matrices below)\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_utils.py:                # TODO(xmfan): even using TemporaryDirectoryName will result in permission error\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_nn.py:    # TODO: reference function\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_nn.py:        # TODO(#50743): Figure out the error. \"RuntimeError: Unrecognized tensor type ID: Batched\"\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_nn.py:            # TODO(#50743): figure out the error\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_nn.py:            # TODO: This code can path can be removed if #61309 is resolved\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_nn.py:# TODO : Fix these discrepancies\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_nn.py:            # TODO: compare structure (ensure analytic jacobian has correct shape)\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_nn.py:                # TODO: do this with in-memory files as soon as torch.save will support it\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_nn.py:                # TODO: torch.complex32 when properly supported\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_nn.py:    # TODO: check that criterions don't ignore grad_output\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_distributed.py:# TODO (kwen2501): what is the purpose of this decorator?  Tests with this\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_distributed.py:        # TODO: we should pipe the exception of the failed subprocess here.\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_distributed.py:        # TODO: get test name from kwargs\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_distributed.py:        # TODO: figure out a better way to do this\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_quantization.py:    # TODO(future PR): consider combining with skipIfNoQNNPACK,\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_quantization.py:            # TODO: make img_data a single example instead of a list\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_quantization.py:        # TODO: remove this check and define two fuse_modules function on this module\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_quantization.py:# TODO: self.fc should be self.conv\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_quantization.py:# TODO: self.fc should be self.conv\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_quantization.py:# TODO: self.fc should be self.conv\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_quantization.py:        # TODO: remove this check and define two fuse_modules function on this module\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_quantization.py:        # TODO: remove this check and define two fuse_model function on this module\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_device_type.py:# TODO: remove \"allow_xpu\" option after Interl GPU support all test case instantiate by this function.\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_device_type.py:        raise unittest.SkipTest(\"TODO: Memory availability checks for XLA?\")\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_device_type.py:        raise unittest.SkipTest(\"TODO: Memory availability checks for Intel GPU?\")\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_device_type.py:# TODO: the \"all\" in the name isn't true anymore for quite some time as we have also have for example XLA and MPS now.\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/inductor_utils.py:# TODO: Remove HAS_MPS condition  when `HAS_GPU` includes HAS_MPS\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/jit_utils.py:        # TODO: check gradients for parameters, not just inputs\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/jit_utils.py:# TODO(suo) remove\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/jit_utils.py:# TODO: Remove me once https://bugs.python.org/issue42666 is resolved\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/jit_utils.py:        # TODO: inplace tests currently fail, fix and add inplace variant\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/jit_utils.py:    # TODO: find better way to standardize on op registration itself..\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_methods_invocations.py:        # TODO: exclude_zeros can be removed after https://github.com/pytorch/pytorch/issues/73638 is fixed\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_methods_invocations.py:        # TODO: exclude_zeros can be removed after https://github.com/pytorch/pytorch/issues/73638 is fixed\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_methods_invocations.py:# TODO: add reduction kwargs\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_methods_invocations.py:        # TODO: no layout\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_methods_invocations.py:    # TODO: no layout\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_methods_invocations.py:    # TODO: FIXME\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_methods_invocations.py:            # TODO: this can be simplified after https://github.com/pytorch/pytorch/issues/69316 is fixed\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_methods_invocations.py:    # TODO: @krshrimali, once to_numpy method in SampleInput class is modified to take None inputs,\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_methods_invocations.py:    # TODO: can't switch `to.device` overload to use positional arguments\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_methods_invocations.py:# TODO: add reference inputs for where(condition) signature\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_methods_invocations.py:        # TODO: remove once the issue is resolved\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_methods_invocations.py:                       # TODO: Fix test_out_arg_all_dtypes as torch.empty_like(expected_output) where expected_output=op(input)\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_methods_invocations.py:               # TODO: update sample inputs with for_inplace_variant kwarg to support this test\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_methods_invocations.py:               # TODO: update sample inputs with for_inplace_variant kwarg to support this test\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_methods_invocations.py:                        # TODO: FIXME: RuntimeError: not implemented for 'ComplexFloat'\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_methods_invocations.py:           # TODO: some signatures of median do support out\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_methods_invocations.py:           # TODO: some signatures of nanmedian do support out\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_methods_invocations.py:           # TODO: some signatures of var_mean do support out\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_methods_invocations.py:           # TODO: some signatures of var_mean do support out\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_methods_invocations.py:           # TODO: some signatures of std_mean do support out\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_methods_invocations.py:           # TODO: some signatures of var_mean do support out\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_methods_invocations.py:            # TODO: FIXME: RuntimeError: \"max_elementwise_cuda\" not implemented for 'ComplexFloat'\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_methods_invocations.py:            # TODO: FIXME: RuntimeError: \"max_elementwise_cuda\" not implemented for 'ComplexFloat'\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_methods_invocations.py:            # TODO: FIXME: RuntimeError: \"min_elementwise_cuda\" not implemented for 'ComplexFloat'\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_methods_invocations.py:            # TODO: FIXME: RuntimeError: \"min_elementwise_cuda\" not implemented for 'ComplexFloat'\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_methods_invocations.py:                        # TODO: FIXME: RuntimeError: \"bitwise_or_cuda\" not implemented for 'Half'\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_methods_invocations.py:                        # TODO: FIXME: RuntimeError: \"bitwise_xor_cuda\" not implemented for 'Half'\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_methods_invocations.py:           # TODO: add shape checks\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_methods_invocations.py:           # TODO: add shape checks\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_methods_invocations.py:           # TODO: investigate nondeterminism\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_methods_invocations.py:            # TODO Need to understand what this is testing and why it doesn't work\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_methods_invocations.py:            # TODO skip this for now since we can't skip on runtime arch support\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_methods_invocations.py:        # TODO: Skip because it produces a CUDA illegal memory access for some reason\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_methods_invocations.py:    # TODO: combine this with the nn.functional.silu OpInfo when\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_methods_invocations.py:        # TODO(whc) should not need sample_inputs_func, but without it\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_methods_invocations.py:                    # TODO: FIXME\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_methods_invocations.py:    # TODO: FIXME, ideally by implemented grad for both inputs\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_methods_invocations.py:    # TODO: FIXME, ideally by implementing grad for both inputs\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_methods_invocations.py:                        # TODO: FIXME tolerance is too high\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_methods_invocations.py:           # TODO(@heitorschueroff) update SampleInput to handle such cases\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_methods_invocations.py:    # TODO(@kshitij12345): Refactor similar to `mvlgamma` entries.\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_methods_invocations.py:               # TODO: same as this?\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_methods_invocations.py:    OpInfo('trapz',  # TODO: in the future, 'trapz' should be made a proper alias of 'trapezoid'\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_methods_invocations.py:               # TODO: FIXME: complex inputs requiring grad error in forward\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_methods_invocations.py:               # TODO: implement csr.to_sparse(sample_dim) where sampled_dim is 1.\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_methods_invocations.py:        # TODO Benchmark again with the new implementation\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_methods_invocations.py:            # TODO: Investigate why more granular skips in the test don't work in CI\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_methods_invocations.py:            # TODO skip this for now since we can't skip on runtime arch support (taken from scaled_dot_product_attention)\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_methods_invocations.py:    # TODO: delete this OpInfo once we add meta support for grid_sampler_3d\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_methods_invocations.py:            # TODO: RuntimeError: no _refs support for torch.rand_like\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_methods_invocations.py:            DecorateInfo(unittest.skip(\"TODO: RuntimeError: no _refs support for torch.rand_like\"),\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_methods_invocations.py:            # TODO: RuntimeError: no _refs support for torch.rand_like\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_methods_invocations.py:            DecorateInfo(unittest.skip(\"TODO: RuntimeError: no _refs support for torch.rand_like\"),\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_methods_invocations.py:            # TODO: RuntimeError: no _refs support for torch.rand_like\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_methods_invocations.py:            DecorateInfo(unittest.skip(\"TODO: RuntimeError: no _refs support for torch.rand_like\"),\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_methods_invocations.py:            # TODO: RuntimeError: no _refs support for torch.rand_like\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_methods_invocations.py:            DecorateInfo(unittest.skip(\"TODO: RuntimeError: no _refs support for torch.rand_like\"),\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_methods_invocations.py:            # TODO: RuntimeError: no _refs support for torch.rand_like\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_methods_invocations.py:            DecorateInfo(unittest.skip(\"TODO: RuntimeError: no _refs support for torch.rand_like\"),\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_methods_invocations.py:            # TODO: RuntimeError: no _refs support for torch.rand_like\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_methods_invocations.py:            DecorateInfo(unittest.skip(\"TODO: RuntimeError: no _refs support for torch.rand_like\"),\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_methods_invocations.py:            # TODO: RuntimeError: no _refs support for torch.rand_like\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_methods_invocations.py:            DecorateInfo(unittest.skip(\"TODO: RuntimeError: no _refs support for torch.rand_like\"),\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_methods_invocations.py:            # TODO torch.ops.aten.copy is not in _refs\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_methods_invocations.py:            # TODO copy doesn't have prim refs\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_methods_invocations.py:    PythonRefInfo(  # TODO: Port this to an UnaryOpInfo\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_methods_invocations.py:        # TODO: Uses minimum and clamp\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_methods_invocations.py:        # TODO: If self already has the correct dtype and device, then self is\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_methods_invocations.py:        # TODO: If self already has the correct dtype and device, then self is\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_methods_invocations.py:        # TODO: If self already has the correct dtype and device, then self is\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_methods_invocations.py:        # TODO: If self already has the correct dtype and device, then self is\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_methods_invocations.py:        # TODO: If self already has the correct dtype and device, then self is\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_methods_invocations.py:        # TODO: If self already has the correct dtype and device, then self is\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_methods_invocations.py:        # TODO: If self already has the correct dtype and device, then self is\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_methods_invocations.py:        # TODO: If self already has the correct dtype and device, then self is\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_methods_invocations.py:        # TODO: If self already has the correct dtype and device, then self is\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_methods_invocations.py:        # TODO: If self already has the correct dtype and device, then self is\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_methods_invocations.py:        # TODO: If self already has the correct dtype and device, then self is\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_methods_invocations.py:        # TODO: If self already has the correct dtype and device, then self is\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_methods_invocations.py:        # TODO: If self already has the correct dtype and device, then self is\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_quantized.py:# TODO: Update all quantization tests to use this decorator.\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_quantized.py:    # TODO document this better\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_quantized.py:    # TODO: can the branch floating point comparisons below be done without\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/hop_db.py:    \"dynamo_bypassing_wrapper\",  # TODO(soulitzer)\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_modules.py:    # TODO: Uncomment when negative weights is supported.\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_modules.py:        # TODO: add pos_weight to the definition here and corresponding SampleInputs\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_mps.py:            # TODO: remove these once downstream function 'aten::_linalg_svd.U' have been implemented\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_mps.py:            # TODO: remove these once downstream function 'aten::_linalg_svd.U' have been implemented\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/hypothesis_utils.py:    # TODO: Maybe embed the enforced zero_point in the `torch.iinfo`.\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_subclass.py:# TODO: Move LoggingTensor here.\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/distributed/_tensor/common_dtensor.py:        # TODO: dist tensor need to support quantized and sparse\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/distributed/_tensor/common_dtensor.py:        # TODO: add multi mesh choices\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/distributed/multi_threaded_pg.py:TODO:\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/distributed/distributed_test.py:            # TODO: now that nccl send/recv is supported, there does not seem to\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/distributed/distributed_test.py:            # TODO: move this test to use torch.profiler once kineto issues are\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/distributed/distributed_test.py:            # TODO: Instead we should probably go through _rank_not_in_group\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/distributed/distributed_test.py:            # TODO: NCCL backend does not work correctly for bitwise reduction ops\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/distributed/distributed_test.py:            # TODO: enable this for general training use cases:\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/distributed/distributed_test.py:            # TODO(#54879): Provide ability to wait and report all failed ranks\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/distributed/nn/api/remote_module_test.py:            # TODO: Once the RPC backend can support directly sending GPU tensors, the expected device type should be \"cuda:0\".\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/distributed/nn/api/remote_module_test.py:            # TODO: Once the RPC backend can support directly sending GPU tensors, the expected device type should be \"cuda:0\".\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/distributed/nn/api/remote_module_test.py:        # TODO: Once the RPC backend can support directly sending GPU tensors, the expected device type should be \"cuda:0\".\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/distributed/rpc/rpc_test.py:    # TODO: use torch.futures.collect_all\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/distributed/rpc/rpc_test.py:        # TODO: with TCP init, rank 0 raises Address already in use because\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/distributed/rpc/rpc_test.py:        # TODO: enable timeouts for rpc.remote/RRef (https://github.com/pytorch/pytorch/issues/33803)\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/distributed/rpc/rpc_test.py:        # TODO: Cuda RPC is failing due to:\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/distributed/rpc/dist_autograd_test.py:        # TODO, need more investigation\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/distributed/rpc/jit/rpc_test.py:        # TODO, need more investigation\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/distributed/rpc/jit/rpc_test.py:            # TODO: Can't get a reliable time for this profiling event since\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/dynamo_test_failures.py:# TODO: due to case sensitivity problems, for now list these files by hand\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/jit_metaprogramming_utils.py:    # flaky test - TODO fix\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/jit_metaprogramming_utils.py:    # TODO: delete this list once we make all nn_tests work\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/logging_tensor.py:# TODO: TensorBase should work\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/logging_tensor.py:            # TODO: clone storage aliasing\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/opinfo/refs.py:        \"aliases\": None,  # TODO add a check for alias coverage\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/opinfo/refs.py:        \"inplace_variant\": None,  # TODO: add a check for inplace coverage\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/opinfo/core.py:            # TODO: Warn if used\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/opinfo/core.py:        # TODO: After migration, start adding warnings here\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/opinfo/core.py:    # TODO: rename this to supports_bwgrad_bwgrad to be consistent with below\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/opinfo/core.py:    # TODO: rename supports_sparse to supports_sparse_coo\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/opinfo/core.py:    # TODO(@heitorschueroff) Once all reduction operators are using\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/opinfo/core.py:    # TODO(@heitorschueroff) Once all reduction operators are using ReductionOpInfo\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/opinfo/core.py:# TODO: in the future generalize the reference generators to handle n-ary elementwise operations\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/opinfo/definitions/sparse.py:                # TODO: remove this if-block after gh-98495 is fixed.\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/opinfo/definitions/sparse.py:            # TODO: remove this if-block after gh-98495 is fixed.\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/opinfo/definitions/sparse.py:            # TODO: remove this if-block after gh-98495 is fixed.\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/opinfo/definitions/linalg.py:        # TODO: backward uses in-place operations that vmap doesn't like\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/opinfo/definitions/linalg.py:            # TODO: is this really needed?\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/opinfo/definitions/signal.py:            # TODO: same as this?\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/opinfo/definitions/special.py:# TODO: Consolidate `i0e` with sample_inputs_unary when `make_tensor`,\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/opinfo/definitions/special.py:        # TODO: eliminate low after gh-106692 is fixed:\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/opinfo/definitions/special.py:    # TODO: FIXME\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/opinfo/definitions/nested.py:        # TODO: look in kwargs too?\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/opinfo/definitions/nested.py:            # TODO: Cover this in the set of error inputs\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/opinfo/definitions/nested.py:                # TODO: Reducing on ragged dim and non-batch dim is not supported;\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/opinfo/definitions/nested.py:    # TODO: write this!\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/opinfo/definitions/nested.py:    # TODO: write this!\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/opinfo/definitions/nested.py:    # TODO: write this!\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/opinfo/definitions/nested.py:    # TODO: write this!\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/opinfo/definitions/nested.py:    # TODO: add Tensor case\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/opinfo/definitions/nested.py:        # TODO (need factory functions):\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/opinfo/definitions/nested.py:        # TODO (need factory functions):\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/opinfo/definitions/nested.py:        # TODO: Handle this with error_inputs\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/opinfo/definitions/nested.py:            # TODO: Handle these via error_inputs.\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/opinfo/definitions/nested.py:        # TODO: Handle this with error_inputs\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/opinfo/definitions/nested.py:    # TODO: Translate the rest of the OpInfos\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_optimizers.py:        ),  # TODO: Move out to testing in param_group?\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_optimizers.py:# TODO: consider tensor LR! See multi_tensor_optimizer_configs in test_optim.py --> tensor LR should work\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_optimizers.py:        ),  # TODO: Move out to testing in param_group?\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_fsdp.py:    # TODO: FSDP non-recursive wrapping\n./.venv/lib/python3.13/site-packages/torch/testing/_internal/common_fsdp.py:        # TODO: Disable checking the parameters for pure FP16 due to floating\n./.venv/lib/python3.13/site-packages/torch/contrib/_tensorboard_vis.py:        # TODO: handle attrs\n./.venv/lib/python3.13/site-packages/torch/amp/grad_scaler.py:                    # TODO: is there a way to split by device and dtype without appending in the inner loop?\n./.venv/lib/python3.13/site-packages/torch/sparse/_triton_ops.py:        # TODO: investigate if contiguity along other axes than the\n./.venv/lib/python3.13/site-packages/torch/sparse/semi_structured.py:            # TODO in the future we can add in padding to support sparse dimensions that aren't perfect multiples\n./.venv/lib/python3.13/site-packages/torch/_C/_profiler.pyi:    # TODO(robieta): remove in favor of `self.typed`\n./.venv/lib/python3.13/site-packages/torch/_C/__init__.pyi:    # TODO: __reduce__\n./.venv/lib/python3.13/site-packages/torch/_C/__init__.pyi:    # TODO: __reduce__\n./.venv/lib/python3.13/site-packages/torch/_C/__init__.pyi:# TODO: where\n./.venv/lib/python3.13/site-packages/torch/_prims_common/__init__.py:# TODO: Type[torch.SymInt], Type[torch.SymFloat]\n./.venv/lib/python3.13/site-packages/torch/_prims_common/__init__.py:# TODO: This needs a lot more type annotations\n./.venv/lib/python3.13/site-packages/torch/_prims_common/__init__.py:            # TODO: We should check that the symbols are consistent\n./.venv/lib/python3.13/site-packages/torch/_prims_common/__init__.py:# TODO: look at using torch.testing.assert_close instead with an option\n./.venv/lib/python3.13/site-packages/torch/_prims_common/__init__.py:        # TODO: we should review why this happens and see about fixing it\n./.venv/lib/python3.13/site-packages/torch/_prims_common/__init__.py:            # TODO: Check the symbols are consistent with each other\n./.venv/lib/python3.13/site-packages/torch/_prims_common/__init__.py:    # TODO: are these necessary?\n./.venv/lib/python3.13/site-packages/torch/_prims_common/__init__.py:        # TODO: type error here is real, replace with sym_complex\n./.venv/lib/python3.13/site-packages/torch/_prims_common/__init__.py:    # TODO: sym_complex_float?\n./.venv/lib/python3.13/site-packages/torch/_prims_common/__init__.py:# TODO: maybe unify with can_cast_to?\n./.venv/lib/python3.13/site-packages/torch/_prims_common/__init__.py:# TODO: when NumberType contains the sym types, can simplify this\n./.venv/lib/python3.13/site-packages/torch/_prims_common/__init__.py:# TODO: document type promotion kinds\n./.venv/lib/python3.13/site-packages/torch/_prims_common/__init__.py:    # TODO: maybe inform the user of channels_last_3d if rank of the tensor is 5?\n./.venv/lib/python3.13/site-packages/torch/_prims_common/__init__.py:    # TODO: a better way to handle this would be with a new op, \"_unsafe_as_strided\"\n./.venv/lib/python3.13/site-packages/torch/_prims_common/wrappers.py:# TODO: implement ref.cast with an option to enforce safe casting\n./.venv/lib/python3.13/site-packages/torch/_prims_common/wrappers.py:# TODO: handle tuples of tensors\n./.venv/lib/python3.13/site-packages/torch/_prims_common/wrappers.py:            # TODO: There is a subtle bug here: prims like copy_to\n./.venv/lib/python3.13/site-packages/torch/_prims_common/wrappers.py:# TODO: when tracing this will add torch tensors and not TensorMeta objects\n./.venv/lib/python3.13/site-packages/torch/_prims_common/wrappers.py:# TODO: this wrapper is currently untested\n./.venv/lib/python3.13/site-packages/torch/_tensor.py:            # TODO: skipping storage copy is wrong for meta, as meta\n./.venv/lib/python3.13/site-packages/torch/_tensor.py:                    # TODO: Once we decide to break serialization FC, no longer\n./.venv/lib/python3.13/site-packages/torch/_tensor.py:            # TODO: remove hasattr, it's a hack to support versions of torch that\n./.venv/lib/python3.13/site-packages/torch/_tensor.py:            # TODO: Once we decide to break serialization FC, no longer\n./.venv/lib/python3.13/site-packages/torch/_tensor.py:                # Ideally, we'd use a private API for this instead. TODO: Switch to this if\n./.venv/lib/python3.13/site-packages/torch/_tensor.py:                # TODO: Once we decide to break serialization FC, no longer\n./.venv/lib/python3.13/site-packages/torch/_tensor.py:            # TODO: remove hasattr, it's a hack to support versions of torch that\n./.venv/lib/python3.13/site-packages/torch/_tensor.py:            # TODO mypy doesn't support @property, see: https://github.com/python/mypy/issues/6185\n./.venv/lib/python3.13/site-packages/torch/utils/hipify/cuda_to_hip_mappings.py:        # TODO: Undo this special-case; see the header for motivation behind this\n./.venv/lib/python3.13/site-packages/torch/utils/hipify/hipify_python.py:    TODO:\n./.venv/lib/python3.13/site-packages/torch/utils/bundled_inputs.py:        # TODO: Should we do this even for non-contiguous tensors?\n./.venv/lib/python3.13/site-packages/torch/utils/bundled_inputs.py:        # TODO: Provide more useful diagnostics.\n./.venv/lib/python3.13/site-packages/torch/utils/checkpoint.py:            # TODO: we can probably make this check stricter by checking that\n./.venv/lib/python3.13/site-packages/torch/utils/checkpoint.py:            # version counter. TODO: Use reentrant_dispatch instead of\n./.venv/lib/python3.13/site-packages/torch/utils/checkpoint.py:        # TODO HOPs don't mutate -> this is always true today but will not be true forever\n./.venv/lib/python3.13/site-packages/torch/utils/cpp_extension.py:    # TODO generalize with_cuda as specific device type.\n./.venv/lib/python3.13/site-packages/torch/utils/_python_dispatch.py:# TODO: Limitations and things about enable_torch_dispatch_mode we should fix before exposing it:\n./.venv/lib/python3.13/site-packages/torch/utils/weak.py:        # TODO, add _fix_weakref type binding\n./.venv/lib/python3.13/site-packages/torch/utils/backend_registration.py:# TODO: Should use `torch._C._get_privateuse1_backend_name()` to get\n./.venv/lib/python3.13/site-packages/torch/utils/backend_registration.py:            # TODO mypy doesn't support @property, see: https://github.com/python/mypy/issues/6185\n./.venv/lib/python3.13/site-packages/torch/utils/tensorboard/summary.py:    # TODO: expose other parameters in the future.\n./.venv/lib/python3.13/site-packages/torch/utils/tensorboard/writer.py:        # TODO: See if we can remove this in the future if we are\n./.venv/lib/python3.13/site-packages/torch/utils/tensorboard/_pytorch_graph.py:        # TODO; Specify a __slots__ for this class or potentially\n./.venv/lib/python3.13/site-packages/torch/utils/tensorboard/_pytorch_graph.py:        # TODO: See if we can remove this in the future\n./.venv/lib/python3.13/site-packages/torch/utils/tensorboard/_pytorch_graph.py:        # TODO: compute correct memory usage and CPU time once\n./.venv/lib/python3.13/site-packages/torch/utils/tensorboard/_pytorch_graph.py:    # TODO: See if we can extract GPU vs CPU information from the PyTorch model\n./.venv/lib/python3.13/site-packages/torch/utils/model_dump/__init__.py:    - Fix various TODO comments in this file and the JS.\n./.venv/lib/python3.13/site-packages/torch/utils/model_dump/__init__.py:            # TODO: Undo at least that second hack.  We should support string states.\n./.venv/lib/python3.13/site-packages/torch/utils/model_dump/__init__.py:                # TODO: Handle this case better.  TorchScript ranges are in bytes,\n./.venv/lib/python3.13/site-packages/torch/utils/model_dump/__init__.py:                # TODO: handle errors here and just ignore the file?\n./.venv/lib/python3.13/site-packages/torch/utils/model_dump/code.js:      // TODO: Maybe check that compressed_size === file_size.\n./.venv/lib/python3.13/site-packages/torch/utils/model_dump/code.js:  // TODO: Better formatting.  Right-align this.\n./.venv/lib/python3.13/site-packages/torch/utils/model_dump/code.js:      // TODO: Maybe show simple lists and tuples on one line.\n./.venv/lib/python3.13/site-packages/torch/utils/model_dump/code.js:      // TODO: Maybe show simple lists and tuples on one line.\n./.venv/lib/python3.13/site-packages/torch/utils/model_dump/code.js:      // TODO: Maybe show simple (empty?) dicts on one line.\n./.venv/lib/python3.13/site-packages/torch/utils/model_dump/code.js:    // TODO: Check stride and indicate if the tensor is channels-last or non-contiguous\n./.venv/lib/python3.13/site-packages/torch/utils/model_dump/code.js:    // TODO: Check size, stride, offset, and numel and indicate if\n./.venv/lib/python3.13/site-packages/torch/utils/model_dump/code.js:    // TODO: Maybe show key?\n./.venv/lib/python3.13/site-packages/torch/utils/model_dump/code.js:        // TODO: Less copy/paste between this and normal dicts.\n./.venv/lib/python3.13/site-packages/torch/utils/model_dump/code.js:  // TODO: Add human-readable sizes?\n./.venv/lib/python3.13/site-packages/torch/utils/model_dump/code.js:  // TODO: Add sorting options?\n./.venv/lib/python3.13/site-packages/torch/utils/model_dump/code.js:  // TODO: Add hierarchical collapsible tree?\n./.venv/lib/python3.13/site-packages/torch/utils/model_dump/code.js:// TODO: Maybe track by dtype as well.\n./.venv/lib/python3.13/site-packages/torch/utils/model_dump/code.js:// TODO: Maybe distinguish between visible size and storage size.\n./.venv/lib/python3.13/site-packages/torch/utils/_cxx_pytree.py:    # TODO(XuehaiPan): remove this condition when we make Python pytree out-of-box support\n./.venv/lib/python3.13/site-packages/torch/utils/benchmark/utils/valgrind_wrapper/timer_interface.py:    # TODO(#105471): Rename the count field\n./.venv/lib/python3.13/site-packages/torch/utils/benchmark/utils/valgrind_wrapper/timer_interface.py:                # TODO: Figure out if we can use torch.serialization.add_safe_globals here\n./.venv/lib/python3.13/site-packages/torch/utils/mkldnn.py:            # TODO: Remove this once ScriptModule supports registering None buffer\n./.venv/lib/python3.13/site-packages/torch/utils/mkldnn.py:            # TODO: Remove this once ScriptModule supports registering None buffer\n./.venv/lib/python3.13/site-packages/torch/utils/_content_store.py:        # TODO: make storage support buffer protocol so this isn't\n./.venv/lib/python3.13/site-packages/torch/utils/_content_store.py:    # TODO: factor this into a random utility\n./.venv/lib/python3.13/site-packages/torch/utils/_content_store.py:    # TODO: offer some sort of non-blocking API to speed things up\n./.venv/lib/python3.13/site-packages/torch/utils/_content_store.py:        # TODO: consider not using torch.save for this; we don't actually\n./.venv/lib/python3.13/site-packages/torch/utils/_content_store.py:        # TODO: Support more advanced snapshotting of requires_grad/grad/etc\n./.venv/lib/python3.13/site-packages/torch/utils/data/graph.py:# TODO(VitalyFedyunin): Make sure it works without dill module installed\n./.venv/lib/python3.13/site-packages/torch/utils/data/_utils/worker.py:# TODO: Implement `SeedSequence` like object for `torch.random`\n./.venv/lib/python3.13/site-packages/torch/utils/data/datapipes/_typing.py:# TODO: Use TypeAlias when Python 3.6 is deprecated\n./.venv/lib/python3.13/site-packages/torch/utils/data/datapipes/_typing.py:# TODO: When PyTorch drops the support for Python 3.6, it can be converted\n./.venv/lib/python3.13/site-packages/torch/utils/data/datapipes/_typing.py:        # TODO: the statements below are not reachable by design as there is a bug and typing is low priority for now.\n./.venv/lib/python3.13/site-packages/torch/utils/data/datapipes/_typing.py:    # TODO: Fix isinstance bug\n./.venv/lib/python3.13/site-packages/torch/utils/data/datapipes/_typing.py:    # TODO: Fix isinstance bug\n./.venv/lib/python3.13/site-packages/torch/utils/data/datapipes/_typing.py:    # TODO: Fix isinstance bug\n./.venv/lib/python3.13/site-packages/torch/utils/data/datapipes/_typing.py:    # TODO:\n./.venv/lib/python3.13/site-packages/torch/utils/data/datapipes/_decorator.py:    # TODO: Lambda for picking\n./.venv/lib/python3.13/site-packages/torch/utils/data/datapipes/_decorator.py:    # TODO:\n./.venv/lib/python3.13/site-packages/torch/utils/data/datapipes/gen_pyi.py:    TODO: The current implementation of this script only generates interfaces for built-in methods. To generate\n./.venv/lib/python3.13/site-packages/torch/utils/data/datapipes/dataframe/datapipes.py:            # TODO(VitalyFedyunin): Replacing with TorchArrow only API, as we are dropping pandas as followup\n./.venv/lib/python3.13/site-packages/torch/utils/data/datapipes/dataframe/datapipes.py:        ):  # TODO(VitalyFedyunin): Replace with better iterable exception\n./.venv/lib/python3.13/site-packages/torch/utils/data/datapipes/dataframe/dataframes.py:# TODO(VitalyFedyunin): Add error when two different traces get combined\n./.venv/lib/python3.13/site-packages/torch/utils/data/datapipes/dataframe/dataframes.py:#  TODO(VitalyFedyunin): Extract this list from the DFIterDataPipe registred functions\n./.venv/lib/python3.13/site-packages/torch/utils/data/datapipes/dataframe/dataframes.py:    # TODO: All operations are shared across entire InitialCapture, need to figure out what if we join two captures\n./.venv/lib/python3.13/site-packages/torch/utils/data/datapipes/dataframe/dataframes.py:        # TODO(VitalyFedyunin): Currently can't pickle (why?)\n./.venv/lib/python3.13/site-packages/torch/utils/data/datapipes/dataframe/dataframes.py:        # TODO(VitalyFedyunin): Make this calculation thread safe (as currently it updates pointer)\n./.venv/lib/python3.13/site-packages/torch/utils/data/datapipes/dataframe/dataframes.py:    # TODO(VitalyFedyunin): Add tests\n./.venv/lib/python3.13/site-packages/torch/utils/data/datapipes/dataframe/dataframes.py:    # TODO(VitalyFedyunin): Need to join context if one of them are empty because we used capture\n./.venv/lib/python3.13/site-packages/torch/utils/data/datapipes/dataframe/dataframes.py:        # TODO: Check if args or kwargs have more than one different context\n./.venv/lib/python3.13/site-packages/torch/utils/data/datapipes/dataframe/dataframes.py:            # TODO: Allow CaptureA to take context from mock\n./.venv/lib/python3.13/site-packages/torch/utils/data/datapipes/dataframe/dataframes.py:        # TODO(VitalyFedyunin): Do not use provate function here, copy own implementation instead.\n./.venv/lib/python3.13/site-packages/torch/utils/data/datapipes/dataframe/dataframes.py:        # TODO: VitalyFedyunin execute kwargs and maybe nested structures\n./.venv/lib/python3.13/site-packages/torch/utils/data/datapipes/dataframe/dataframes.py:    # TODO(VitalyFedyunin): This should be atomic and thread safe\n./.venv/lib/python3.13/site-packages/torch/utils/data/datapipes/dataframe/dataframes.py:        # TODO(VitalyFedyunin): Make this calculation thread safe (as currently it updates pointer)\n./.venv/lib/python3.13/site-packages/torch/utils/data/datapipes/dataframe/dataframes.py:    # TODO(VitalyFedyunin): Must implement all special functions of datapipes\n./.venv/lib/python3.13/site-packages/torch/utils/data/datapipes/iter/callable.py:    # TODO(VitalyFedyunin): Verify that item is any sort of batch\n./.venv/lib/python3.13/site-packages/torch/utils/data/datapipes/iter/callable.py:        # TODO(VitalyFedyunin): Compact all batch dataframes into one\n./.venv/lib/python3.13/site-packages/torch/utils/data/datapipes/iter/callable.py:            # TODO(VitalyFedyunin): Add default collation into df_wrapper\n./.venv/lib/python3.13/site-packages/torch/utils/data/datapipes/iter/callable.py:    # TODO(VitalyFedyunin): We can dynamically extract types from the tuple_values here\n./.venv/lib/python3.13/site-packages/torch/utils/data/datapipes/iter/callable.py:    # TODO(VitalyFedyunin): Instead of ignoring mypy error, make sure tuple_names is not empty\n./.venv/lib/python3.13/site-packages/torch/utils/data/datapipes/iter/callable.py:        # TODO(VitalyFedyunin): Replace `Callable[..., Any]` with `Callable[[IColumn], Any]`\n./.venv/lib/python3.13/site-packages/torch/utils/data/datapipes/iter/callable.py:        # TODO(VitalyFedyunin): Replace with `Dict[Union[str, IColumn], Union[Callable, Enum]]`\n./.venv/lib/python3.13/site-packages/torch/utils/data/datapipes/iter/callable.py:                # TODO(VitalyFedyunin): Validate passed dictionary\n./.venv/lib/python3.13/site-packages/torch/utils/data/datapipes/iter/fileopener.py:        # TODO: enforce typing for each instance based on mode, otherwise\n./.venv/lib/python3.13/site-packages/torch/utils/data/datapipes/iter/combinatorics.py:        # TODO: Performance optimization\n./.venv/lib/python3.13/site-packages/torch/utils/data/datapipes/_hook_iterator.py:            # TODO: Add try-except to in-place reduce traceback from the Exception\n./.venv/lib/python3.13/site-packages/torch/utils/data/datapipes/_hook_iterator.py:                # TODO: Simplify the traceback message to skip over `response = gen.send(None)`\n./.venv/lib/python3.13/site-packages/torch/utils/data/datapipes/utils/decoder.py:                # TODO: xinyu, figure out why Nvidia do this?\n./.venv/lib/python3.13/site-packages/torch/utils/data/datapipes/utils/snapshot.py:# TODO: Caveats\n./.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:                # TODO(https://github.com/pytorch/pytorch/issues/76750)\n./.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:        # TODO: add limited pickling support for sharing an iterator\n./.venv/lib/python3.13/site-packages/torch/utils/_sympy/functions.py:# TODO: In Triton, // rounds to zero, but in Python, it is floor division.\n./.venv/lib/python3.13/site-packages/torch/utils/_sympy/functions.py:                        # TODO if https://github.com/triton-lang/triton/issues/619 is fixed\n./.venv/lib/python3.13/site-packages/torch/utils/_sympy/functions.py:    # TODO: microoptimization is to avoid overflowing into arbitrary precision\n./.venv/lib/python3.13/site-packages/torch/utils/_sympy/functions.py:# TODO: As an indicator, this != 0 implies == 1 (and vice versa).\n./.venv/lib/python3.13/site-packages/torch/utils/_sympy/functions.py:            # TODO: Inability to access size-obliviousness sucks: if we have a\n./.venv/lib/python3.13/site-packages/torch/utils/_sympy/reference.py:        raise NotImplementedError(\"TODO: truncdiv\")\n./.venv/lib/python3.13/site-packages/torch/utils/_sympy/reference.py:# sharing (TODO: considering splitting out a BaseReferenceAnalysis).\n./.venv/lib/python3.13/site-packages/torch/utils/_sympy/reference.py:        # TODO: maybe composite implicit autograd doesn't work here?\n./.venv/lib/python3.13/site-packages/torch/utils/_sympy/reference.py:        # TODO: https://github.com/pytorch/pytorch/pull/133654\n./.venv/lib/python3.13/site-packages/torch/utils/_sympy/reference.py:        # TODO: This is wrong, CPython has a custom implementation of true\n./.venv/lib/python3.13/site-packages/torch/utils/_sympy/value_ranges.py:        # TODO: when the bounds have free variables, this may be\n./.venv/lib/python3.13/site-packages/torch/utils/_sympy/value_ranges.py:        # TODO: We should tighten value ranges\n./.venv/lib/python3.13/site-packages/torch/utils/_sympy/value_ranges.py:        # TODO: We should tighten value ranges\n./.venv/lib/python3.13/site-packages/torch/utils/_sympy/printers.py:    # TODO: Not sure this works with Triton, even when base/exp are integral\n./.venv/lib/python3.13/site-packages/torch/utils/_sympy/printers.py:        # TODO: This is only accurate up to 2**53\n./.venv/lib/python3.13/site-packages/torch/utils/_sympy/printers.py:    # TODO: PowByNatural: we need to implement our own int-int pow.  Do NOT\n./.venv/lib/python3.13/site-packages/torch/utils/_sympy/printers.py:            # TODO: float vs double\n./.venv/lib/python3.13/site-packages/torch/utils/_sympy/printers.py:        # TODO: dispatch to llrint depending on index type\n./.venv/lib/python3.13/site-packages/torch/utils/_sympy/symbol.py:    # TODO: maybe put the assumptions here directly\n./.venv/lib/python3.13/site-packages/torch/utils/_sympy/interp.py:# TODO: Dedupe this with SYMPY_INTERP\n./.venv/lib/python3.13/site-packages/torch/utils/_sympy/interp.py:    # TODO add CeilDiv (it doesn't appear in the index_expr)\n./.venv/lib/python3.13/site-packages/torch/utils/_sympy/interp.py:    # TODO default to some decompositions if the interpreter doesn't have them\n./.venv/lib/python3.13/site-packages/torch/utils/_sympy/interp.py:        CleanDiv: \"floordiv\",  # TODO: hmm?\n./.venv/lib/python3.13/site-packages/torch/utils/_sympy/interp.py:        # TODO: There is a hazard here, if we have float * float it will\n./.venv/lib/python3.13/site-packages/torch/utils/_sympy/interp.py:        PythonMod: \"mod\",  # TODO: this is wrong\n./.venv/lib/python3.13/site-packages/torch/utils/_sympy/interp.py:        # TODO: Inductor can generate these, but it's ill-specified which\n./.venv/lib/python3.13/site-packages/torch/utils/_sympy/interp.py:        # TODO: do the rest of the opaque unary functions...\n./.venv/lib/python3.13/site-packages/torch/utils/_sympy/interp.py:    # TODO: This is kind of pointless, we shouldn't be generating sympy.sin\n./.venv/lib/python3.13/site-packages/torch/utils/_pytree.py:            # TODO: change this warning to an error after OSS/internal stabilize\n./.venv/lib/python3.13/site-packages/torch/utils/_pytree.py:# TODO(angelayi): remove this function after OSS/internal stabilize\n./.venv/lib/python3.13/site-packages/torch/utils/_pytree.py:# TODO(angelayi): remove this function after OSS/internal stabilize\n./.venv/lib/python3.13/site-packages/torch/utils/dlpack.py:# TODO: add a typing.Protocol to be able to tell Mypy that only objects with\n./.venv/lib/python3.13/site-packages/torch/utils/_traceback.py:                # TODO: This creates a temporary file for every frame, but we\n./.venv/lib/python3.13/site-packages/torch/utils/_traceback.py:            # TODO: Maybe indicate that the traceback was elided?\n./.venv/lib/python3.13/site-packages/torch/_meta_registrations.py:    # TODO: Ideally, we'd insert a deferred runtime assert here, but if we are\n./.venv/lib/python3.13/site-packages/torch/_meta_registrations.py:        # TODO: handle out\n./.venv/lib/python3.13/site-packages/torch/_meta_registrations.py:# TODO: Deduplicate this with canonicalize_dim\n./.venv/lib/python3.13/site-packages/torch/_meta_registrations.py:    # TODO: Query cudnnGetRNNTrainingReserveSize (expose to python)\n./.venv/lib/python3.13/site-packages/torch/functional.py:    # TODO Move this to C++ once the jit has better support for torch.Size.\n./.venv/lib/python3.13/site-packages/torch/functional.py:    # TODO: type dim as BroadcastingList when\n./.venv/lib/python3.13/site-packages/torch/functional.py:    # TODO: when https://github.com/pytorch/pytorch/issues/33782 is fixed\n./.venv/lib/python3.13/site-packages/torch/include/c10/util/typeid.h:// TODO: This file is still in the caffe2 namespace, despite living\n./.venv/lib/python3.13/site-packages/torch/include/c10/util/typeid.h:// TODO Disallow this and rather use std::unordered_map/set everywhere\n./.venv/lib/python3.13/site-packages/torch/include/c10/util/ApproximateClock.h:// TODO: We should use\n./.venv/lib/python3.13/site-packages/torch/include/c10/util/intrusive_ptr.h:   * TODO: https://github.com/pytorch/pytorch/issues/56482\n./.venv/lib/python3.13/site-packages/torch/include/c10/util/int128.h:// TODO(xiaofeng): Define GOOGLE_PROTOBUF_HAS_CONSTEXPR when constexpr is\n./.venv/lib/python3.13/site-packages/torch/include/c10/util/int128.h:// TODO: perhaps it would be nice to have int128, a signed 128-bit type?\n./.venv/lib/python3.13/site-packages/torch/include/c10/util/string_view.h:    // TODO: split out\n./.venv/lib/python3.13/site-packages/torch/include/c10/util/string_view.h:    // TODO At some point this should probably be done, including tricks\n./.venv/lib/python3.13/site-packages/torch/include/c10/util/complex.h:// TODO(@zasdfgbnm): c10::complex<c10::Half> is not currently supported,\n./.venv/lib/python3.13/site-packages/torch/include/c10/util/complex.h:// TODO(@zasdfgbnm): implement them as c10::conj\n./.venv/lib/python3.13/site-packages/torch/include/c10/util/complex.h:// TODO(@zasdfgbnm): implement it by ourselves\n./.venv/lib/python3.13/site-packages/torch/include/c10/util/ArrayRef.h:  // TODO Make this explicit\n./.venv/lib/python3.13/site-packages/torch/include/c10/util/TypeList.h:  // TODO Direct implementation might be faster\n./.venv/lib/python3.13/site-packages/torch/include/c10/util/C++17.h:// TODO This is an incomplete implementation of std::apply, not working for\n./.venv/lib/python3.13/site-packages/torch/include/c10/util/complex_utils.h:// TODO: Write in more idiomatic C++17\n./.venv/lib/python3.13/site-packages/torch/include/c10/util/MathConstants.h:// TODO: Replace me with inline constexpr variable when C++17 becomes available\n./.venv/lib/python3.13/site-packages/torch/include/c10/util/Float8_e8m0fnu.h:// TODO(#146647): do we need to special case OPENCL?\n./.venv/lib/python3.13/site-packages/torch/include/c10/util/Float8_e8m0fnu.h:  // TODO(#146647): maybe rewrite without control flow\n./.venv/lib/python3.13/site-packages/torch/include/c10/util/Float8_e8m0fnu-inl.h:// TODO(#146647): Can we remove the below warning?\n./.venv/lib/python3.13/site-packages/torch/include/c10/util/Float8_e8m0fnu-inl.h:  // TODO(#146647): maybe rewrite without control flow\n./.venv/lib/python3.13/site-packages/torch/include/c10/util/Float8_e5m2fnuz-inl.h:  // TODO(future): we are mapping neg_zero to both inf and NaN, this is\n./.venv/lib/python3.13/site-packages/torch/include/c10/util/TypeIndex.h:  // TODO Disallow this and rather use std::unordered_map/set everywhere\n./.venv/lib/python3.13/site-packages/torch/include/c10/util/IdWrapper.h:  // TODO Making operator== noexcept if underlying type is noexcept equality\n./.venv/lib/python3.13/site-packages/torch/include/c10/util/IdWrapper.h:  // TODO Making operator!= noexcept if operator== is noexcept doesn't work with\n./.venv/lib/python3.13/site-packages/torch/include/c10/util/llvmMathExtras.h:  // TODO: Use std::bit_cast once C++20 becomes available.\n./.venv/lib/python3.13/site-packages/torch/include/c10/util/Exception.h:// TODO: Brian Vaughan observed that we might be able to get this to work on\n./.venv/lib/python3.13/site-packages/torch/include/c10/util/Exception.h:// TODO: We're going to get a lot of similar looking string literals\n./.venv/lib/python3.13/site-packages/torch/include/c10/util/TypeCast.h:// TODO(#146647): Can we make all these template specialization happen\n./.venv/lib/python3.13/site-packages/torch/include/c10/util/TypeCast.h:// Trigger tests for D25440771. TODO: Remove this line any time you want.\n./.venv/lib/python3.13/site-packages/torch/include/c10/util/hash.h:// TODO: Compare vs OpenSSL and/or CryptoPP implementations\n./.venv/lib/python3.13/site-packages/torch/include/c10/util/Deprecated.h:// TODO Is there some way to implement this?\n./.venv/lib/python3.13/site-packages/torch/include/c10/core/impl/SizesAndStrides.h:  // TODO: different iterator types for sizes & strides to prevent\n./.venv/lib/python3.13/site-packages/torch/include/c10/core/impl/InlineDeviceGuard.h:  // TODO: Consider reading Tensor and TensorList constructors here, when\n./.venv/lib/python3.13/site-packages/torch/include/c10/core/impl/InlineStreamGuard.h:    // TODO: make a version that takes an impl argument.  Unfortunately,\n./.venv/lib/python3.13/site-packages/torch/include/c10/core/Storage.h:  // TODO: remove later\n./.venv/lib/python3.13/site-packages/torch/include/c10/core/ScalarType.h:// TODO: To add unsigned int types here, we must define accumulate type.\n./.venv/lib/python3.13/site-packages/torch/include/c10/core/ScalarType.h:    /* TODO: remove once the bug is fixed. */                                \\\n./.venv/lib/python3.13/site-packages/torch/include/c10/core/ScalarType.h:  // TODO(#146647): If we expect to have numeric_limits for everything,\n./.venv/lib/python3.13/site-packages/torch/include/c10/core/SymInt.h:  // TODO: these implementations are not optimal because they allocate a\n./.venv/lib/python3.13/site-packages/torch/include/c10/core/SymBool.h:  // TODO: optimize to union\n./.venv/lib/python3.13/site-packages/torch/include/c10/core/TensorOptions.h:  /// TODO: This function encourages bad behavior (assuming CUDA is\n./.venv/lib/python3.13/site-packages/torch/include/c10/core/TensorOptions.h:  // TODO: Deprecate this\n./.venv/lib/python3.13/site-packages/torch/include/c10/core/TensorOptions.h:  // TODO remove after TensorOptions rationalization\n./.venv/lib/python3.13/site-packages/torch/include/c10/core/TensorOptions.h:  // TODO: MemoryFormat is not implemented in this way\n./.venv/lib/python3.13/site-packages/torch/include/c10/core/CPUAllocator.h:// TODO: rename to c10\n./.venv/lib/python3.13/site-packages/torch/include/c10/core/Contiguity.h:      // TODO dim == 3 case will be enabled once it is fully tested\n./.venv/lib/python3.13/site-packages/torch/include/c10/core/Contiguity.h:      // TODO dim == 4 case will be enabled once it is fully tested\n./.venv/lib/python3.13/site-packages/torch/include/c10/core/DispatchKey.h:  // TODO: put this in BackendComponents\n./.venv/lib/python3.13/site-packages/torch/include/c10/core/DispatchKey.h:  Vulkan, // TODO: put this in BackendComponents\n./.venv/lib/python3.13/site-packages/torch/include/c10/core/DispatchKey.h:  Metal, // TODO: put this in BackendComponents\n./.venv/lib/python3.13/site-packages/torch/include/c10/core/DispatchKey.h:  // TODO: Make Mkldnn a functionality key, so we can give it Meta\n./.venv/lib/python3.13/site-packages/torch/include/c10/core/DispatchKey.h:  // TODO: delete this in favor of Python-implemented fake tensor\n./.venv/lib/python3.13/site-packages/torch/include/c10/core/DispatchKey.h:  // TODO: delete this once torchdim lands in functorch\n./.venv/lib/python3.13/site-packages/torch/include/c10/core/DispatchKey.h:  // TODO: make Autocast a functionality key\n./.venv/lib/python3.13/site-packages/torch/include/c10/core/ScalarTypeToTypeMeta.h:// TODO move to typeid.h (or codemod away) when TypeMeta et al\n./.venv/lib/python3.13/site-packages/torch/include/c10/core/SymFloat.h:  // TODO: optimize to union\n./.venv/lib/python3.13/site-packages/torch/include/c10/core/TensorImpl.h:    // TODO: Replace the link to the documentation once it's available.\n./.venv/lib/python3.13/site-packages/torch/include/c10/core/TensorImpl.h:  // TODO: When Variable is added, delete these constructors\n./.venv/lib/python3.13/site-packages/torch/include/c10/core/TensorImpl.h:  // TODO: does C++14 have a stdlib template for this?\n./.venv/lib/python3.13/site-packages/torch/include/c10/core/TensorImpl.h:    // TODO: maybe this should be toggled by strides\n./.venv/lib/python3.13/site-packages/torch/include/c10/core/TensorImpl.h:      // TODO: provide stride_custom, symmetrically with size_custom.\n./.venv/lib/python3.13/site-packages/torch/include/c10/core/TensorImpl.h:    // TODO: We could add support to Python dispatch here.\n./.venv/lib/python3.13/site-packages/torch/include/c10/core/TensorImpl.h:    // TODO: We could call into aten::size.int instead of\n./.venv/lib/python3.13/site-packages/torch/include/c10/core/TensorImpl.h:    // TODO: We could add support to Python dispatch here.\n./.venv/lib/python3.13/site-packages/torch/include/c10/core/TensorImpl.h:    // TODO: We could call into aten::size.int instead of\n./.venv/lib/python3.13/site-packages/torch/include/c10/core/TensorImpl.h:  // TODO: remove this once we don't automatically enabled Autograd dispatch\n./.venv/lib/python3.13/site-packages/torch/include/c10/core/TensorImpl.h:      // TODO: implement layout() as native function/method so that\n./.venv/lib/python3.13/site-packages/torch/include/c10/core/TensorImpl.h:   * TODO: This should be jettisoned in favor of `set_sizes_and_strides`,\n./.venv/lib/python3.13/site-packages/torch/include/c10/core/TensorImpl.h:   * TODO: This should be jettisoned in favor of `set_sizes_and_strides`,\n./.venv/lib/python3.13/site-packages/torch/include/c10/core/TensorImpl.h:    // TODO: this should probably consult policy\n./.venv/lib/python3.13/site-packages/torch/include/c10/core/TensorImpl.h:    // TODO: at some point, we should kill this field completely.\n./.venv/lib/python3.13/site-packages/torch/include/c10/core/TensorImpl.h:    // TODO: A useful internal assert would be to show that device_opt_ is null\n./.venv/lib/python3.13/site-packages/torch/include/c10/core/TensorImpl.h://    strong refcount           TODO: pack these into one word\n./.venv/lib/python3.13/site-packages/torch/include/c10/core/GeneratorImpl.h: * TODO: Look into changing the threading semantics of Generators in ATen (e.g.,\n./.venv/lib/python3.13/site-packages/torch/include/c10/core/StorageImpl.h:  // TODO: remove later\n./.venv/lib/python3.13/site-packages/torch/include/c10/core/Scalar.h:  // TODO: Support ComplexHalf accessor\n./.venv/lib/python3.13/site-packages/torch/include/c10/core/SymIntArrayRef.h:// TODO: a SymIntArrayRef containing a heap allocated large negative integer\n./.venv/lib/python3.13/site-packages/torch/include/c10/core/SymbolicShapeMeta.h:  // TODO: should the SymBool cases avoid the short circuit?  Need to reason\n./.venv/lib/python3.13/site-packages/torch/include/c10/core/DeviceGuard.h:  /// TODO: The consistency check here is inconsistent with StreamGuard's\n./.venv/lib/python3.13/site-packages/torch/include/c10/core/MemoryFormat.h:      // TODO dim == 3 case will be enabled once it is fully tested\n./.venv/lib/python3.13/site-packages/torch/include/c10/core/MemoryFormat.h:      // TODO dim == 4 case will be enabled once it is fully tested\n./.venv/lib/python3.13/site-packages/torch/include/c10/macros/Macros.h:// TODO: This doesn't assert the message because I (chilli) couldn't figure out\n./.venv/lib/python3.13/site-packages/torch/include/c10/metal/reduction_utils.h:  // TODO: This should be moved to the callee\n./.venv/lib/python3.13/site-packages/torch/include/c10/metal/reduction_utils.h:  // TODO: This should be moved to the callee\n./.venv/lib/python3.13/site-packages/torch/include/c10/metal/reduction_utils.h:  // TODO: This should be moved to the callee\n./.venv/lib/python3.13/site-packages/torch/include/c10/metal/reduction_utils.h:  // TODO: This should be moved to the callee\n./.venv/lib/python3.13/site-packages/torch/include/c10/metal/indexing.h:// TODO: Look like binary_dense_scalar are frequently used specialization that\n./.venv/lib/python3.13/site-packages/torch/include/c10/metal/utils.h:// TODO: Move it to type_traits header may be\n./.venv/lib/python3.13/site-packages/torch/include/c10/metal/special_math.h:    // TODO, replace with expm1 when implemented\n./.venv/lib/python3.13/site-packages/torch/include/c10/metal/special_math.h:  // TODO: what's an \"inverted interval\"? Open on the left\n./.venv/lib/python3.13/site-packages/torch/include/c10/metal/special_math.h:// TODO: Add 512 if/when double will be supported in Metal\n./.venv/lib/python3.13/site-packages/torch/include/c10/metal/random.h:  // TODO: Implement better algorithm for large ranges\n./.venv/lib/python3.13/site-packages/torch/include/c10/cuda/CUDACachingAllocator.h:// TODO: Turn this into an honest to goodness class. I briefly attempted to do\n./.venv/lib/python3.13/site-packages/torch/include/asmjit/arm/a64operand.h:// TODO: [ARM] PC + offset address.\n./.venv/lib/python3.13/site-packages/torch/include/google/protobuf/map_field.h:  // TODO(jieluo) - Checks that enum is member.\n./.venv/lib/python3.13/site-packages/torch/include/google/protobuf/generated_message_table_driven.h:// TODO(ckennelly):  Add a static assertion to ensure that these masks do not\n./.venv/lib/python3.13/site-packages/torch/include/google/protobuf/generated_message_table_driven.h:  // TODO(ckennelly): Do something with this padding.\n./.venv/lib/python3.13/site-packages/torch/include/google/protobuf/generated_message_table_driven.h:  // TODO(ckennelly): Vet these for sign extension.\n./.venv/lib/python3.13/site-packages/torch/include/google/protobuf/generated_message_table_driven.h:// TODO(ckennelly): Consolidate these implementations into a single one, using\n./.venv/lib/python3.13/site-packages/torch/include/google/protobuf/generated_message_table_driven.h:  // TODO(gerbens) This skips the first test if we could use the fast\n./.venv/lib/python3.13/site-packages/torch/include/google/protobuf/util/field_comparator.h:    // TODO(ksroka): Introduce third value to differentiate uses of AlmostEqual\n./.venv/lib/python3.13/site-packages/torch/include/google/protobuf/repeated_field.h:    // TODO(ckennelly):  The compiler loses track of the buffer freshly\n./.venv/lib/python3.13/site-packages/torch/include/google/protobuf/repeated_field.h:  // TODO(kenton):  Breaks C++ aliasing rules.  We should probably remove this\n./.venv/lib/python3.13/site-packages/torch/include/google/protobuf/repeated_field.h:  // TODO(kenton):  Breaks C++ aliasing rules.  We should probably remove this\n./.venv/lib/python3.13/site-packages/torch/include/google/protobuf/map.h:      // TODO(zuguang): optimize this. The temporary copy can be allocated\n./.venv/lib/python3.13/site-packages/torch/include/google/protobuf/parse_context.h:    // TODO(gerbens) We could remove this line and hoist the code to\n./.venv/lib/python3.13/site-packages/torch/include/google/protobuf/parse_context.h:  // systems. TODO(gerbens) do we need to set this as build flag?\n./.venv/lib/python3.13/site-packages/torch/include/google/protobuf/arena_impl.h:    // TODO(haberman): evaluate whether we would gain efficiency by getting rid\n./.venv/lib/python3.13/site-packages/torch/include/google/protobuf/extension_set.h:  // TODO(kenton):  Hardcore memory management accessors\n./.venv/lib/python3.13/site-packages/torch/include/google/protobuf/extension_set.h:    // TODO(kenton):  Use atomic<int> when C++ supports it.\n./.venv/lib/python3.13/site-packages/torch/include/google/protobuf/arena.h:    // TODO(b/151247138): if the pointer would have been aligned already,\n./.venv/lib/python3.13/site-packages/torch/include/google/protobuf/message_lite.h:  // TODO(gerbens) make this a pure abstract function\n./.venv/lib/python3.13/site-packages/torch/include/google/protobuf/compiler/parser.h:  // TODO(kenton):  The LocationRecorder is const because historically locations\n./.venv/lib/python3.13/site-packages/torch/include/google/protobuf/compiler/parser.h:    // TODO(kenton):  See comment on TryConsumeEndOfDeclaration(), above, for\n./.venv/lib/python3.13/site-packages/torch/include/google/protobuf/compiler/importer.h:// TODO(kenton):  Move all SourceTree stuff to a separate file?\n./.venv/lib/python3.13/site-packages/torch/include/google/protobuf/compiler/importer.h:// TODO(kenton):  I feel like this class is not well-named.\n./.venv/lib/python3.13/site-packages/torch/include/google/protobuf/compiler/importer.h:  // TODO(xiaofeng): change this to a pure virtual function.\n./.venv/lib/python3.13/site-packages/torch/include/google/protobuf/io/printer.h:  // TODO(gerbens) I don't see why we need virtuals here. Just a vector of\n./.venv/lib/python3.13/site-packages/torch/include/google/protobuf/io/coded_stream.h:    // TODO(gpike): Is it worth rearranging this? E.g., if the number of fields\n./.venv/lib/python3.13/site-packages/torch/include/google/protobuf/io/coded_stream.h:    return 10;  // TODO(kenton):  Make this a symbolic constant.\n./.venv/lib/python3.13/site-packages/torch/include/google/protobuf/metadata.h:// TODO(b/151117630): Remove this file and all instances where it gets imported.\n./.venv/lib/python3.13/site-packages/torch/include/google/protobuf/generated_message_reflection.h:// TODO(jasonh): Remove this once the compiler change to directly include this\n./.venv/lib/python3.13/site-packages/torch/include/google/protobuf/stubs/port.h:  // TODO(xiaofeng): The PROTOBUF_LITTLE_ENDIAN is not only used for\n./.venv/lib/python3.13/site-packages/torch/include/google/protobuf/stubs/strutil.h:  // TODO: Add a string_ref constructor, eventually\n./.venv/lib/python3.13/site-packages/torch/include/google/protobuf/stubs/stringpiece.h:// TODO(mec): delete this after stringpiece_ssize_type goes 64 bit.\n./.venv/lib/python3.13/site-packages/torch/include/google/protobuf/stubs/mutex.h:// TODO(kenton):  Implement these?  Hard to implement portably.\n./.venv/lib/python3.13/site-packages/torch/include/kineto/AbstractConfig.h:  // TODO: Separate out each profiler type into features?\n./.venv/lib/python3.13/site-packages/torch/include/kineto/Config.h:    // TODO(T94634890): Deprecate requestTimestamp\n./.venv/lib/python3.13/site-packages/torch/include/kineto/output_base.h:// TODO(T90238193)\n./.venv/lib/python3.13/site-packages/torch/include/fbgemm/QuantUtils.h:// TODO: T26263653 fix signed-integer-overflow undefined behavior\n./.venv/lib/python3.13/site-packages/torch/include/fbgemm/Fbgemm.h:   *                    TODO: if Aq_zero_point == 0, allow passing nullptr.\n./.venv/lib/python3.13/site-packages/torch/include/fbgemm/Fbgemm.h:   *                    TODO: if Aq_zero_point == 0, allow passing nullptr.\n./.venv/lib/python3.13/site-packages/torch/include/fbgemm/FbgemmEmbedding.h:      // TODO: add compressed_data_size and check array bound\n./.venv/lib/python3.13/site-packages/torch/include/fbgemm/FloatConversion.h:// TODO: we're disabling native fp16 on Windows to workaround test failures\n./.venv/lib/python3.13/site-packages/torch/include/THC/THCDeviceUtils.cuh:// TODO: Remove this header\n./.venv/lib/python3.13/site-packages/torch/include/THC/THCAtomics.cuh:// TODO: Remove once torchvision has been updated to use the ATen header\n./.venv/lib/python3.13/site-packages/torch/include/ittnotify.h:/* TODO: Temporary for compatibility! */\n./.venv/lib/python3.13/site-packages/torch/include/pybind11/eigen/matrix.h:          // TODO: when Eigen bug #747 is fixed, remove the tests for non-negativity.\n./.venv/lib/python3.13/site-packages/torch/include/pybind11/eigen/tensor.h:    // TODO: Move to std::optional once std::optional has more support\n./.venv/lib/python3.13/site-packages/torch/include/pybind11/pytypes.h:    // TODO PYBIND11_DEPRECATED(\n./.venv/lib/python3.13/site-packages/torch/include/pybind11/pytypes.h:// TODO: After the deprecated constructors are removed, this macro can be simplified by\n./.venv/lib/python3.13/site-packages/torch/include/pybind11/detail/type_caster_base.h:        // TODO: is this still true for pure Python 3.6?\n./.venv/lib/python3.13/site-packages/torch/include/pybind11/detail/common.h:/// Compatibility macros for Python 2 / Python 3 versions TODO: remove\n./.venv/lib/python3.13/site-packages/torch/include/pybind11/pybind11.h:    using module_def = PyModuleDef; // TODO: Can this be removed (it was needed only for Python 2)?\n./.venv/lib/python3.13/site-packages/torch/include/pybind11/pybind11.h:        // TODO: Should be reinterpret_steal for Python 3, but Python also steals it again when\n./.venv/lib/python3.13/site-packages/torch/include/pybind11/pybind11.h:                        // TODO consolidate the erasure code in pybind11_meta_dealloc() in class.h\n./.venv/lib/python3.13/site-packages/torch/include/pybind11/pybind11.h:    // TODO: state captures only the types of Extra, not the values\n./.venv/lib/python3.13/site-packages/torch/include/caffe2/serialize/versions.h:// risk of breaking existing clients. TODO: A better way would be to allow\n./.venv/lib/python3.13/site-packages/torch/include/caffe2/perfkernels/embedding_lookup_idx.h: * TODO: make this API also take \"offsets\" rather than \"lengths\" to match the\n./.venv/lib/python3.13/site-packages/torch/include/oneapi/dnnl/dnnl_types.h:    // TODO: suggest renames\n./.venv/lib/python3.13/site-packages/torch/include/oneapi/dnnl/dnnl_types.h:    // TODO: Align avx10_1 values to internal representation.\n./.venv/lib/python3.13/site-packages/torch/include/oneapi/dnnl/dnnl_types.h:    // TODO: Align avx10_1 values to internal representation.\n./.venv/lib/python3.13/site-packages/torch/include/oneapi/dnnl/dnnl_types.h:    // TODO: Align avx10_1 values to internal representation.\n./.venv/lib/python3.13/site-packages/torch/include/oneapi/dnnl/dnnl_graph.hpp:        // TODO(lvtao): check the size of adims and strides.\n./.venv/lib/python3.13/site-packages/torch/include/oneapi/dnnl/dnnl.hpp:    // TODO (Roma): the code below is only needed because get_primitive_desc\n./.venv/lib/python3.13/site-packages/torch/include/oneapi/dnnl/dnnl_ukernel.hpp:        // TODO: export batch_element to C API later for user to fill it and\n./.venv/lib/python3.13/site-packages/torch/include/oneapi/dnnl/dnnl_ukernel.hpp:        // TODO: export batch_element to C API later for user to fill it and\n./.venv/lib/python3.13/site-packages/torch/include/legacy/ittnotify.h:/* TODO: Temporary for compatibility! */\n./.venv/lib/python3.13/site-packages/torch/include/torch/library.h:  // TODO: This is morally the same thing as KernelRegistrationConfig, but it's\n./.venv/lib/python3.13/site-packages/torch/include/torch/library.h:        // TODO: Don't go through WrapRuntimeKernelFunctor\n./.venv/lib/python3.13/site-packages/torch/include/torch/library.h:        // TODO: Don't go through WrapRuntimeKernelFunctor\n./.venv/lib/python3.13/site-packages/torch/include/torch/library.h:        // TODO: Don't go through WrapRuntimeKernelFunctor\n./.venv/lib/python3.13/site-packages/torch/include/torch/library.h:    // TODO: need to raise an error when you impl a function that has a\n./.venv/lib/python3.13/site-packages/torch/include/torch/library.h:    // TODO: need to raise an error when you impl a function that has a\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/stable/tensor.h:// TODO ASAP: THIS FILE SHOULD BE HEADER ONLY BUT ISN'T ENFORCED:\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/autograd/forward_grad.h:  // TODO(albanD): replace this with a SmallVector\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/autograd/function.h:  /// TODO: it might be possible to handle cases where backward is\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/autograd/variable.h:/// TODO: Eliminate this function as much as possible, as it can be expressed\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/autograd/custom_function.h:  // TODO Add tracing here\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/autograd/generated/variable_factories.h:    at::AutoDispatchBelowAutograd guard;  // TODO: remove\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/autograd/generated/variable_factories.h:    at::AutoDispatchBelowAutograd guard;  // TODO: remove\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/autograd/generated/variable_factories.h:    at::AutoDispatchBelowAutograd guard;  // TODO: remove\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/autograd/generated/variable_factories.h:    at::AutoDispatchBelowAutograd guard;  // TODO: remove\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/autograd/generated/Functions.h:    // TODO(crcrpar): Use `std::move(saved_for)` to avoid incrementing refcount, which would need refactoring.\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/autograd/VariableTypeUtils.h:// TODO: Blegh, bare references\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/autograd/utils/grad_layout_contract.h:    // TODO: Nested Tensor does not have an implementation of detach. The\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/autograd/utils/grad_layout_contract.h:        // TODO: Actually detect views in the accumulateGrad function so that\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/CudaIPCTypes.h:  // TODO: Can be changed to FIFO in order to avoid full traverse on every\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/PyInterpreter.h:// TODO: Move these to a proper namespace\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/profiler/containers.h:  // TODO: cbegin and cend()\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/profiler/collection.h:    TensorListBegin, // TODO: generalize to other lists.\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/profiler/util.h:// TODO: replace with pytorch/rfcs#43 when it is ready.\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/tensor/python_tensor.h:// TODO: This is nuts!  There is no reason to let the default tensor type id\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/api/include/torch/nn/parallel/data_parallel.h:      // TODO: use nccl reduce\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/distributed/autograd/engine/dist_engine.h:  // TODO: 1. Add assert in the dist engine to ensure no GPU NodeTasks during\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/distributed/rpc/rref_proto.h:// TODO: Remove all these messages and use rpc + registered functions instead.\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/distributed/rpc/tensorpipe_agent.h:  // TODO: To achieve better performance we can have a pipe pool per\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/distributed/rpc/rref_impl.h:// TODO: current RRef implementation does not tolerate failures\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/distributed/rpc/rref_impl.h:// TODO: RRef internal messages are not yet idempotent\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/distributed/rpc/rref_impl.h:// TODO: make RRef an IValue, and edit createStackForSchema accordingly\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/distributed/rpc/rref_impl.h:// TODO: make RRef system messages idempotent and retry on failures.\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/distributed/rpc/rref_context.h:  // TODO: make this a context guard\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/distributed/c10d/NCCLUtils.hpp:// TODO: remove these macros from code.\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/distributed/c10d/Work.hpp:// TODO: support different types of failures/errors\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/distributed/c10d/logger.hpp:  // TODO to support single process multiple devices and multi device modules,\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/distributed/c10d/symm_mem/CUDASymmetricMemoryUtils.hpp:    // TODO: implement an efficient one?\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/distributed/c10d/ProcessGroupNCCL.hpp:// TODO: We want to eventually remove this variable and make users to use\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/distributed/c10d/ProcessGroupNCCL.hpp:  // TODO(fduwjj): We need to have an audit on all mutexes we are adding here.\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/distributed/c10d/ProcessGroupNCCL.hpp:  // TODO: we probably need a separate map for P2P comms\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/distributed/c10d/Backend.hpp:  // TODO, do we need to distinguish between remote timeout or remote COMM\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/distributed/c10d/ProcessGroupGlooDetail.hpp:    // TODO: This is a massive hack!  There is some confusion about\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/distributed/c10d/Utils.hpp:    // TODO: see if we should add overflow protection for offset\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/distributed/c10d/reducer.hpp:  // TODO this function makes broadcast communication call and\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/distributed/c10d/reducer.hpp:    // TODO(@pietern)\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/distributed/c10d/ProcessGroup.hpp:    // TODO: HACK for backend name to get sequence number for that backend.\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/distributed/c10d/ProcessGroup.hpp:    // TODO: HACK for backend name to get sequence number for that backend.\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/distributed/c10d/ProcessGroup.hpp:    // TODO: if nccl was specified then use it\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/distributed/c10d/ProcessGroup.hpp:    // TODO: should we add these entries after the backend setting succeeds?\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/distributed/c10d/control_plane/Handlers.hpp:  // TODO: add support for chunked responses\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/inductor/cpp_prefix.h:  // TODO(jgong5): cache cache blocking results\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/inductor/cpp_prefix.h:  // TODO: tune the factor here\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/inductor/aoti_runtime/interface.h:// TODO: Deprecate this API. This was kept for BC compatibility.\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/inductor/aoti_runtime/model.h:    // TODO: Handle shared storage case.\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/inductor/aoti_runtime/mini_array_ref.h:  // TODO Make this explicit\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/lazy/core/helpers.h:// TODO: Consolidate this file with util.h\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/lazy/core/ir_builder.h:// TODO: this should return Value\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/lazy/core/ir.h:  // TODO: Some IR classes share the same opkind, such as Mean and MeanDim, so\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/lazy/core/lazy_graph_executor.h:  // TODO: even though this API is currently used **only** in codegen to\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/lazy/core/lazy_graph_executor.h:  // TODO(alanwaketan): Revisit if all of them need to be accessible to\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/lazy/core/lazy_graph_executor.h:  // TODO(alanwaketan): Add a registry such that we don't need to make all\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/lazy/core/ir_metadata.h:// TODO(whc) is this going to be used outside of in IR decompositions?\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/lazy/core/util.h:// TODO(alanwaketan): Consolidate it with c10::scope_exit.\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/lazy/core/util.h:// TODO(alanwaketan): This is clever, but is there really no std or c10\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/lazy/core/tensor.h:    // TODO(alanwaketan): Remove this ctor. This is a\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/lazy/core/trie.h:  // TODO: Because we don't expect user to explicitly call this function via\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/lazy/ts_backend/ir_builder.h:  // TODO: Scalar node is not currently used by ts_backend. Enable reusing\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/lazy/ts_backend/ir_builder.h:  // TODO: verify if IR node reusing works for Dynamic shape ops\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/lazy/ts_backend/config.h:// TODO(whc) unclear if this is useful, has only been tested as true\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/lazy/ts_backend/ts_node.h:// TODO(whc) once Shape() API is moved to Node base, also make it virtual, and\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/lazy/ts_backend/ts_lowering_context.h:        \"TODO(whc) implement TS computation shapes or change interface\");\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/lazy/ts_backend/ts_lowering_context.h:        \"TODO(whc) implement TS computation shapes or change interface\");\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/lazy/backend/backend_interface.h:  // TODO(whc) need to keep this?\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/lazy/backend/backend_interface.h:  // TODO(whc)\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/utils/python_scalars.h:    // TODO(#146647): simplify below with macros\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/utils/python_scalars.h:    // TODO(#146647): simplify below with macros\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/utils/throughput_benchmark-inl.h:  // TODO: add GUARDED_BY once it is available\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/utils/python_arg_parser.h:// TODO: this can return MaybeOwned\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/utils/python_arg_parser.h: * TODO: we could use different names for the following 'handle_torch_function'\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/jit/mobile/observer.h:  // TODO: Kimish\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/jit/mobile/nnc/context.h:// TODO: support input tensor with dynamic shape (PR #54982)\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/jit/mobile/nnc/context.h:// TODO: support the case where kernel allocates output tensors dynamically.\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/jit/mobile/code.h:  // TODO After we actually export CALL instructions we can remove this.\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/jit/mobile/flatbuffer_loader.h:// no op, TODO(qihan) delete\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/jit/serialization/pickle.h:///  // TODO: when tensors are stored in the pickle, delete this\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/jit/serialization/pickler.h:  // TODO: only use this if necessary (add a pass to find all shared ivalues,\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/jit/serialization/flatbuffer_serializer.h:// TODO(qihan): delete\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/jit/serialization/export.h:// TODO remove these switches once interface call is rolled out.\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/jit/passes/symbolic_shape_cache.h:  // TODO: Consider in the future if it is reasonable to\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/jit/passes/quantization/helper.h:// TODO: remove\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/jit/passes/quantization/helper.h:// TODO: remove\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/jit/passes/quantization/helper.h:// TODO: refactor all current uses of this function to the Opt one\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/jit/passes/quantization/helper.h:// TODO: add a macro to declare the filters\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/jit/passes/quantization/quantization_patterns.h:  // TODO: add %dtype after when https://github.com/pytorch/pytorch/issues/34351\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/jit/passes/value_refinement_utils.h:// TODO: vector may be faster\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/jit/python/python_list.h:  // TODO: Do these make sense?\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/jit/python/pybind_utils.h:      // TODO: this message is not correct anymore, since this InferredType is\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/jit/python/pybind_utils.h:// TODO: Remove once we clean up the GraphExecutor usage.\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/jit/python/pybind_utils.h:  // TODO: we could add __torch_function__ dispatch here but I don't know\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/jit/python/pybind.h:    // TODO: Is there a way to py::cast that doesn't raise an exception on\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/jit/tensorexpr/stmt.h:  // TODO: add memory types.\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/jit/tensorexpr/stmt.h:// TODO: move to this an internal IR.\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/jit/tensorexpr/stmt.h:// TODO: make IR nodes extensible.\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/jit/tensorexpr/loopnest.h:  //     loop variable. TODO: Remove this constraint.\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/jit/tensorexpr/loopnest.h:  //     TODO: Remove this constraint.\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/jit/tensorexpr/loopnest.h:  // TODO: Add an IR verifier check to detect invalidly compressed buffers.\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/jit/tensorexpr/loopnest.h:// TODO: Revisit this once we decide on how dependencies analysis should look\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/jit/tensorexpr/graph_opt.h:// TODO: add error reporting for graphs that can't be converted.\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/jit/tensorexpr/expr.h:  // TODO: unique_name\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/jit/tensorexpr/expr.h:  // TODO: unique_name\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/jit/tensorexpr/codegen.h:  // TODO: Figure out how to unify these call interfaces.\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/jit/tensorexpr/ir.h:// TODO: add TORCH_API\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/jit/tensorexpr/ir.h:// TODO: add TORCH_API\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/jit/tensorexpr/reduction.h:  // TODO possible to remove this arg by deferring the init value until we\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/jit/tensorexpr/external_functions_registry.h:// case we need to run aten ops (TODO: support different devices). The first\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/jit/tensorexpr/mem_dependency_checker.h:  // TODO: this will return only the AccessInfo for A. It's included for\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/jit/frontend/tree_views.h:// TODO: supports only single comprehension for now\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/jit/frontend/tree_views.h:  // TODO: no ifs for now\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/jit/frontend/tree_views.h:// TODO: supports only single comprehension for now\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/jit/frontend/tree_views.h:  // TODO: no ifs for now\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/jit/frontend/sugared_value.h:  // TODO @wconstab refactor to use ModuleValue::asTuple instead of new API\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/jit/frontend/sugared_value.h:  // TODO holding this thing is creepy\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/jit/frontend/source_range.h:    // TODO: std::optional<>::value returns an rvalue ref so can't use it here??\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/jit/api/function_impl.h:  // TODO: add more executors\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/jit/api/compilation_unit.h:    // TODO: class types cannot be redefined because we have no way right now\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/jit/ir/ir.h:  // TODO: make this more const correct\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/jit/ir/ir.h:  // TODO: return iterator\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/jit/ir/ir.h:  // TODO: return iterator\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/jit/ir/subgraph_matcher.h: *  - Pattern graph nodes cannot alias. TODO: the check not implemented yet.\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/jit/ir/subgraph_matcher.h: * found matches, no nodes in the subgraph alias with each other). TODO: check\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/jit/runtime/instruction.h:  // TODO: check for overflow\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/jit/runtime/static/memory_planner.h:    // TODO: Improve this once D31357486 is landed.\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/jit/runtime/register_ops_utils.h:    // TODO: remove when possible, since it just slows down\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/jit/runtime/operator.h: * TODO Instead of doing it this way, we should only have pure-jit ops in\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/jit/runtime/operator.h:    // TODO: some sort of caching mechanism?\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/jit/runtime/operator.h:                  // TODO What if it gets set later?\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/jit/codegen/fuser/kernel_spec.h:// TODO: allow abstract kernels to use multiple generated kernels\n./.venv/lib/python3.13/site-packages/torch/include/torch/csrc/jit/codegen/fuser/kernel_spec.h:// TODO: allow abstract kernels to reuse generated kernels from common pool\n./.venv/lib/python3.13/site-packages/torch/include/torch/custom_class.h:      // TODO: we need to figure out how to profile calls to custom functions\n./.venv/lib/python3.13/site-packages/torch/include/torch/custom_class_detail.h:  // TODO We shouldn't use c10::impl stuff directly here. We should use the\n./.venv/lib/python3.13/site-packages/torch/include/ATen/CUDAFunctions.h:// TODO Undo all logic introduced for Note [Avoiding Include Cycles In Static Dispatch]\n./.venv/lib/python3.13/site-packages/torch/include/ATen/cudnn/Descriptors.h:// TODO: Add constructors for all of the descriptors\n./.venv/lib/python3.13/site-packages/torch/include/ATen/cudnn/Descriptors.h:  // TODO: Figure out why const-correctness doesn't work here\n./.venv/lib/python3.13/site-packages/torch/include/ATen/cudnn/Descriptors.h:        \"TODO: support more cuDNN activation modes\");\n./.venv/lib/python3.13/site-packages/torch/include/ATen/cpu/vec/vec128/vec128_half_neon.h:    // TODO\n./.venv/lib/python3.13/site-packages/torch/include/ATen/cpu/vec/vec128/vec128_reduced_precision_common_neon.h:  // this should be removed. TODO (kimishpatel)\n./.venv/lib/python3.13/site-packages/torch/include/ATen/cpu/vec/vec128/vec128_float_neon.h:    // TODO\n./.venv/lib/python3.13/site-packages/torch/include/ATen/cpu/vec/vec128/vec128_float_neon.h:  // this should be removed. TODO (kimishpatel)\n./.venv/lib/python3.13/site-packages/torch/include/ATen/cpu/vec/vec512/vec512_float.h:// TODO: rewrite with ATEN vectorized (need to add unpack and shuffle)\n./.venv/lib/python3.13/site-packages/torch/include/ATen/cpu/vec/vec512/vec512_float.h:// TODO(jgong5): rewrite with ATEN vectorized (need to add unpack and shuffle)\n./.venv/lib/python3.13/site-packages/torch/include/ATen/cpu/vec/vec512/vec512_complex_double.h:  // TODO: hadd_pd() & hsub_pd() may have scope for improvement.\n./.venv/lib/python3.13/site-packages/torch/include/ATen/cpu/vec/vec512/vec512_complex_double.h:    // TODO: The vectorized implementation requires special handling for the\n./.venv/lib/python3.13/site-packages/torch/include/ATen/cpu/vec/vec512/vec512_complex_double.h:    // TODO: The vectorized implementation requires special handling for the\n./.venv/lib/python3.13/site-packages/torch/include/ATen/cpu/vec/vec512/vec512_complex_double.h:  // TODO: The vectorized implementation requires special handling for the case\n./.venv/lib/python3.13/site-packages/torch/include/ATen/cpu/vec/vec512/vec512_complex_double.h:  // TODO: The vectorized implementation requires special handling for the case\n./.venv/lib/python3.13/site-packages/torch/include/ATen/cpu/vec/vec512/vec512_complex_double.h:  // TODO: The vectorized implementation requires special handling for the case\n./.venv/lib/python3.13/site-packages/torch/include/ATen/cpu/vec/vec512/vec512_int.h:    // TODO<leslie> We can use _mm512_zextsi128_si512 in the furture,\n./.venv/lib/python3.13/site-packages/torch/include/ATen/cpu/vec/vec512/vec512_bfloat16.h:// TODO(Leslie): Add the AVX2 Version of transpose_mxn for BFloat16 and Float16\n./.venv/lib/python3.13/site-packages/torch/include/ATen/cpu/vec/vec512/vec512_complex_float.h:  // TODO: hadd_pd() & hsub_pd() may have scope for improvement.\n./.venv/lib/python3.13/site-packages/torch/include/ATen/cpu/vec/vec512/vec512_complex_float.h:    // TODO: The vectorized implementation requires special handling for the\n./.venv/lib/python3.13/site-packages/torch/include/ATen/cpu/vec/vec512/vec512_complex_float.h:    // TODO: The vectorized implementation requires special handling for the\n./.venv/lib/python3.13/site-packages/torch/include/ATen/cpu/vec/vec512/vec512_complex_float.h:  // TODO: The vectorized implementation requires special handling for the case\n./.venv/lib/python3.13/site-packages/torch/include/ATen/cpu/vec/vec512/vec512_complex_float.h:  // TODO: The vectorized implementation requires special handling for the case\n./.venv/lib/python3.13/site-packages/torch/include/ATen/cpu/vec/vec512/vec512_complex_float.h:  // TODO: The vectorized implementation requires special handling for the case\n./.venv/lib/python3.13/site-packages/torch/include/ATen/cpu/vec/vec256/vec256_complex_float.h:    // TODO: The vectorized implementation requires special handling for the\n./.venv/lib/python3.13/site-packages/torch/include/ATen/cpu/vec/vec256/vec256_complex_float.h:    // TODO: The vectorized implementation requires special handling for the\n./.venv/lib/python3.13/site-packages/torch/include/ATen/cpu/vec/vec256/vec256_complex_float.h:  // TODO: The vectorized implementation requires special handling for the case\n./.venv/lib/python3.13/site-packages/torch/include/ATen/cpu/vec/vec256/vec256_complex_float.h:  // TODO: The vectorized implementation requires special handling for the case\n./.venv/lib/python3.13/site-packages/torch/include/ATen/cpu/vec/vec256/vec256_complex_float.h:  // TODO: The vectorized implementation requires special handling for the case\n./.venv/lib/python3.13/site-packages/torch/include/ATen/cpu/vec/vec256/vec256_mask.h:// TODO: add specialization of VecMaskLoad for bfloat16/half and int8/uint8\n./.venv/lib/python3.13/site-packages/torch/include/ATen/cpu/vec/vec256/vec256_complex_double.h:    // TODO: The vectorized implementation requires special handling for the\n./.venv/lib/python3.13/site-packages/torch/include/ATen/cpu/vec/vec256/vec256_complex_double.h:    // TODO: The vectorized implementation requires special handling for the\n./.venv/lib/python3.13/site-packages/torch/include/ATen/cpu/vec/vec256/vec256_complex_double.h:  // TODO: The vectorized implementation requires special handling for the case\n./.venv/lib/python3.13/site-packages/torch/include/ATen/cpu/vec/vec256/vec256_complex_double.h:  // TODO: The vectorized implementation requires special handling for the case\n./.venv/lib/python3.13/site-packages/torch/include/ATen/cpu/vec/vec256/vec256_complex_double.h:  // TODO: The vectorized implementation requires special handling for the case\n./.venv/lib/python3.13/site-packages/torch/include/ATen/cpu/vec/vec256/vec256.h:  // TODO: can we support caching this?\n./.venv/lib/python3.13/site-packages/torch/include/ATen/cpu/vec/vec256/vec256.h:  // TODO: can we support caching this?\n./.venv/lib/python3.13/site-packages/torch/include/ATen/cpu/vec/vec256/vec256_int.h:    // TODO<leslie> We can use _mm256_zextsi128_si256 in the furture,\n./.venv/lib/python3.13/site-packages/torch/include/ATen/cpu/vec/vec256/vec256_float.h:// TODO: rewrite with ATEN vectorized (need to add unpack and shuffle)\n./.venv/lib/python3.13/site-packages/torch/include/ATen/core/symbol.h:  // TODO: eliminate me\n./.venv/lib/python3.13/site-packages/torch/include/ATen/core/dispatch/DispatchKeyExtractor.h:  // TODO: It's a bit irritating that we have to do logical ORs here, it would\n./.venv/lib/python3.13/site-packages/torch/include/ATen/core/dispatch/Dispatcher.h:  // TODO: This will only be useful if we write a backend fallback that plumbs\n./.venv/lib/python3.13/site-packages/torch/include/ATen/core/List.h:  // TODO Test use_count\n./.venv/lib/python3.13/site-packages/torch/include/ATen/core/operator_name.h:// TODO: consider storing namespace separately too\n./.venv/lib/python3.13/site-packages/torch/include/ATen/core/operator_name.h:  // TODO: These two functions below are slow!  Fix internal data structures so\n./.venv/lib/python3.13/site-packages/torch/include/ATen/core/function_schema.h:  // TODO remove the mutation here\n./.venv/lib/python3.13/site-packages/torch/include/ATen/core/dynamic_type.h:  // TODO Change Ptr to DynamicTypePtr when all migrations are done.\n./.venv/lib/python3.13/site-packages/torch/include/ATen/core/NestedIntSymNodeImpl.h:  // the higher-level API in python instead (TODO: actually introduce that).\n./.venv/lib/python3.13/site-packages/torch/include/ATen/core/boxing/BoxedKernel.h:   * TODO: This will only be useful if we write a backend fallback that plumbs\n./.venv/lib/python3.13/site-packages/torch/include/ATen/core/boxing/impl/boxing.h:  // TODO Reuse stack vector instead of allocating?\n./.venv/lib/python3.13/site-packages/torch/include/ATen/core/boxing/impl/test_helpers.h:  // TODO: We add this to simulate the ideal case where we only have Autograd\n./.venv/lib/python3.13/site-packages/torch/include/ATen/core/boxing/impl/make_boxed_from_unboxed_functor.h:using Stack = torch::jit::Stack; // TODO Instead of this, move torch::jit::Stack\n./.venv/lib/python3.13/site-packages/torch/include/ATen/core/boxing/impl/make_boxed_from_unboxed_functor.h:// TODO: it probably would be good to tighten this up quite a bit more with\n./.venv/lib/python3.13/site-packages/torch/include/ATen/core/boxing/impl/make_boxed_from_unboxed_functor.h:  // TODO static_assert(AllowDeprecatedTypes, \"You tried to register a kernel\n./.venv/lib/python3.13/site-packages/torch/include/ATen/core/boxing/impl/make_boxed_from_unboxed_functor.h:// TODO Delete this once kernels don't do that anymore\n./.venv/lib/python3.13/site-packages/torch/include/ATen/core/boxing/KernelFunction.h:using Stack = torch::jit::Stack; // TODO Instead of this, move torch::jit::Stack\n./.venv/lib/python3.13/site-packages/torch/include/ATen/core/boxing/KernelFunction.h:   * TODO: This will only be useful if we write a backend fallback that plumbs\n./.venv/lib/python3.13/site-packages/torch/include/ATen/core/TensorBase.h:  /// TODO: it's not in native_functions.yaml yet as it's not exposed to python\n./.venv/lib/python3.13/site-packages/torch/include/ATen/core/TensorBase.h:  // TODO(#97856) Make this return a const pointer. This currently\n./.venv/lib/python3.13/site-packages/torch/include/ATen/core/TensorBase.h:  // TODO(#97856) Make this return a const pointer. This is currently\n./.venv/lib/python3.13/site-packages/torch/include/ATen/core/op_registration/adaption.h:  // TODO: Remove this once the following issue is addressed:\n./.venv/lib/python3.13/site-packages/torch/include/ATen/core/op_registration/op_registration.h:        // TODO Do schema inference without relying on WrapFunctionIntoFunctor\n./.venv/lib/python3.13/site-packages/torch/include/ATen/core/op_registration/op_registration.h:        // TODO Do schema inference without relying on WrapFunctionIntoFunctor\n./.venv/lib/python3.13/site-packages/torch/include/ATen/core/op_registration/op_registration.h:        // TODO Do schema inference without relying on WrapFunctionIntoFunctor\n./.venv/lib/python3.13/site-packages/torch/include/ATen/core/op_registration/op_registration.h:        // TODO Do schema inference without relying on WrapFunctionIntoFunctor\n./.venv/lib/python3.13/site-packages/torch/include/ATen/core/op_registration/op_registration.h:        // TODO Do schema inference without relying on WrapFunctionIntoRuntimeFunctor\n./.venv/lib/python3.13/site-packages/torch/include/ATen/core/op_registration/op_registration.h:        // TODO Do schema inference without relying on WrapFunctionIntoRuntimeFunctor\n./.venv/lib/python3.13/site-packages/torch/include/ATen/core/op_registration/op_registration.h:       // TODO Do schema inference without relying on WrapFunctionIntoRuntimeFunctor\n./.venv/lib/python3.13/site-packages/torch/include/ATen/core/op_registration/op_registration.h:        // TODO Do schema inference without relying on WrapFunctionIntoRuntimeFunctor\n./.venv/lib/python3.13/site-packages/torch/include/ATen/core/op_registration/op_registration.h:        // TODO Do schema inference without relying on WrapFunctionIntoRuntimeFunctor\n./.venv/lib/python3.13/site-packages/torch/include/ATen/core/op_registration/op_allowlist.h:// TODO: unify to C10_MOBILE. In theory this header could be used in OSS.\n./.venv/lib/python3.13/site-packages/torch/include/ATen/core/List_inl.h:  // TODO Use list_element_from?\n./.venv/lib/python3.13/site-packages/torch/include/ATen/core/List_inl.h:  // TODO Use list_element_from?\n./.venv/lib/python3.13/site-packages/torch/include/ATen/core/TensorBody.h:  // TODO: temporarily disabled\n./.venv/lib/python3.13/site-packages/torch/include/ATen/core/TensorBody.h:  // TODO: Deprecate me\n./.venv/lib/python3.13/site-packages/torch/include/ATen/core/TensorBody.h:  // TODO: The Python version also accepts arguments\n./.venv/lib/python3.13/site-packages/torch/include/ATen/core/TensorBody.h:  // TODO: remove following two after at::kDouble and its friends are TypeMeta's.\n./.venv/lib/python3.13/site-packages/torch/include/ATen/core/class_type.h:  // TODO: once modules support arbitrary ivalue attributes, we don't need this\n./.venv/lib/python3.13/site-packages/torch/include/ATen/core/class_type.h:  // TODO: This is better represented as an OrderedDict, but alas it is not yet\n./.venv/lib/python3.13/site-packages/torch/include/ATen/core/ivalue.h:   * TODO: need to support customizing equality\n./.venv/lib/python3.13/site-packages/torch/include/ATen/core/ivalue.h:    // TODO: Find way to expose alias info for opaque tensors.\n./.venv/lib/python3.13/site-packages/torch/include/ATen/core/ivalue.h:    // TODO (after Tensor merge) If we pass in a Blob holding a Tensor, extract\n./.venv/lib/python3.13/site-packages/torch/include/ATen/core/ivalue.h:        // TODO: Find way to expose alias info for opaque tensors.\n./.venv/lib/python3.13/site-packages/torch/include/ATen/core/ivalue.h:  // TODO: There are several places that recurse over IValue. This is fragile.\n./.venv/lib/python3.13/site-packages/torch/include/ATen/core/LegacyTypeDispatch.h:// TODO: Clean up what remains here\n./.venv/lib/python3.13/site-packages/torch/include/ATen/core/LegacyTypeDispatch.h:// TODO: AutoNonVariableTypeMode should be removed in release 1.10.\n./.venv/lib/python3.13/site-packages/torch/include/ATen/core/TransformationHelper.h:  // TODO: must be investigated and unified!!!\n./.venv/lib/python3.13/site-packages/torch/include/ATen/core/stack.h:// TODO move this to c10 namespace\n./.venv/lib/python3.13/site-packages/torch/include/ATen/core/PythonOpRegistrationTrampoline.h:// TODO: this can probably live in c10\n./.venv/lib/python3.13/site-packages/torch/include/ATen/core/PhiloxRNGEngine.h:    // TODO(min-jean-cho) change to Polar method, a more efficient version of Box-Muller method\n./.venv/lib/python3.13/site-packages/torch/include/ATen/core/PhiloxRNGEngine.h:    // TODO(voz) We use std:: below, and thus need a separate impl for CUDA.\n./.venv/lib/python3.13/site-packages/torch/include/ATen/core/Generator.h: * TODO: Look into changing the threading semantics of Generators in ATen (e.g., making\n./.venv/lib/python3.13/site-packages/torch/include/ATen/core/jit_type.h:// TODO: investigate making this SingletonOrSharedTypePtr<TensorType>\n./.venv/lib/python3.13/site-packages/torch/include/ATen/core/jit_type.h:  // TODO: static_assert that a templated function exists, and throw a friendly\n./.venv/lib/python3.13/site-packages/torch/include/ATen/core/jit_type.h:  // TODO: static_assert that a templated function exists, and throw a friendly\n./.venv/lib/python3.13/site-packages/torch/include/ATen/core/ivalue_inl.h:// TODO this is deprecated but we don't throw a warning because a lot of ops in\n./.venv/lib/python3.13/site-packages/torch/include/ATen/core/blob.h:  // TODO(jerryzh): add a Get(c10::DeviceType) function?\n./.venv/lib/python3.13/site-packages/torch/include/ATen/core/blob.h:    // TODO: after we add Get<Tensor>(c10::DeviceType)\n./.venv/lib/python3.13/site-packages/torch/include/ATen/core/blob.h:      // TODO Re-enable logging\n./.venv/lib/python3.13/site-packages/torch/include/ATen/mps/MPSAllocator.h:// TODO: Unify the logic with CUDACachingAllocator and remove redundant code.\n./.venv/lib/python3.13/site-packages/torch/include/ATen/mps/MPSAllocator.h:    // TODO: check the caching performance of write-combined mode\n./.venv/lib/python3.13/site-packages/torch/include/ATen/mps/MPSGuardImpl.h:// TODO: Move the MPSGuardImpl to inherit from NoOpDeviceGuardImpl\n./.venv/lib/python3.13/site-packages/torch/include/ATen/mps/MPSGuardImpl.h:    // TODO: Currently setting only device 0\n./.venv/lib/python3.13/site-packages/torch/include/ATen/mps/MPSGuardImpl.h:      // TODO: extend it for multi-device case\n./.venv/lib/python3.13/site-packages/torch/include/ATen/cpp_custom_type_hack.h:  at::AutoDispatchBelowADInplaceOrView guard; // TODO: remove\n./.venv/lib/python3.13/site-packages/torch/include/ATen/Utils.h:    // TODO: is this necessary?  We used to treat nullptr-vs-not in IntList\n./.venv/lib/python3.13/site-packages/torch/include/ATen/LegacyVmapTransforms.h:// a logical BatchedTensor. (TODO(rzou): some of these are not yet implemented).\n./.venv/lib/python3.13/site-packages/torch/include/ATen/CompositeExplicitAutogradNonFunctionalFunctions.h:// TODO Undo all logic introduced for Note [Avoiding Include Cycles In Static Dispatch]\n./.venv/lib/python3.13/site-packages/torch/include/ATen/CompositeImplicitAutogradFunctions.h:// TODO Undo all logic introduced for Note [Avoiding Include Cycles In Static Dispatch]\n./.venv/lib/python3.13/site-packages/torch/include/ATen/detail/PrivateUse1HooksInterface.h:    // TODO(FFFrog): Perserved for BC and will be removed in the future.\n./.venv/lib/python3.13/site-packages/torch/include/ATen/detail/CUDAHooksInterface.h:// TODO: Consider putting the stub definitions in another class, so that one\n./.venv/lib/python3.13/site-packages/torch/include/ATen/CompositeExplicitAutogradFunctions.h:// TODO Undo all logic introduced for Note [Avoiding Include Cycles In Static Dispatch]\n./.venv/lib/python3.13/site-packages/torch/include/ATen/quantized/QTensorImpl.h:  // TODO: Expose in PyTorch Frontend\n./.venv/lib/python3.13/site-packages/torch/include/ATen/functorch/LegacyVmapTransforms.h:// a logical BatchedTensor. (TODO(rzou): some of these are not yet implemented).\n./.venv/lib/python3.13/site-packages/torch/include/ATen/functorch/BatchedTensorImpl.h:// TODO: should probably contain more (or all?) backend keys\n./.venv/lib/python3.13/site-packages/torch/include/ATen/functorch/DynamicLayer.h:// TODO: we can excise DynamicLayer in favor of Interpreter,\n./.venv/lib/python3.13/site-packages/torch/include/ATen/functorch/PlumbingHelper.h:  // TODO: should really check this\n./.venv/lib/python3.13/site-packages/torch/include/ATen/cuda/Atomic.cuh:  // TODO: remove this special case for HIP when issue is fixed:\n./.venv/lib/python3.13/site-packages/torch/include/ATen/cuda/Atomic.cuh:  // TODO: remove this special case for HIP when issue is fixed:\n./.venv/lib/python3.13/site-packages/torch/include/ATen/cuda/cub.cuh:  // TODO: use thrust::discard_iterator to handle null keys_out when https://github.com/NVIDIA/cub/issues/406 is fixed.\n./.venv/lib/python3.13/site-packages/torch/include/ATen/cuda/detail/CUDAHooks.h:// TODO: No need to have this whole header, we can just put it all in\n./.venv/lib/python3.13/site-packages/torch/include/ATen/cuda/detail/IntegerDivider.cuh:    // TODO: gcc/clang has __builtin_clz() but it's not portable.\n./.venv/lib/python3.13/site-packages/torch/include/ATen/cuda/CUDAApplyUtils.cuh:// TODO: combine with TensorArg?  So far that's been for debugging, and this is functional...\n./.venv/lib/python3.13/site-packages/torch/include/ATen/CompositeImplicitAutogradNestedTensorFunctions.h:// TODO Undo all logic introduced for Note [Avoiding Include Cycles In Static Dispatch]\n./.venv/lib/python3.13/site-packages/torch/include/ATen/NestedTensorImpl.h:  // TODO: don't expose private implementation details like this; in\n./.venv/lib/python3.13/site-packages/torch/include/ATen/NestedTensorImpl.h:  // TODO: don't expose private implementation details like this\n./.venv/lib/python3.13/site-packages/torch/include/ATen/NestedTensorImpl.h:  // TODO: numel_custom and is_contiguous_custom can be profitably overridden\n./.venv/lib/python3.13/site-packages/torch/include/ATen/NestedTensorImpl.h:  // TODO: maybe we can remove this metadata since\n./.venv/lib/python3.13/site-packages/torch/include/ATen/MetaFunctions.h:// TODO Undo all logic introduced for Note [Avoiding Include Cycles In Static Dispatch]\n./.venv/lib/python3.13/site-packages/torch/include/ATen/FunctionalTensorWrapper.h:  // TODO: maybe it's possible to arrange for that to happen automatically\n./.venv/lib/python3.13/site-packages/torch/include/ATen/TensorIndexing.h:  // TODO: implement negative step\n./.venv/lib/python3.13/site-packages/torch/include/ATen/TensorIndexing.h:  // TODO: check scalarType\n./.venv/lib/python3.13/site-packages/torch/include/ATen/native/hip/ck_gemm_template.h:    // TODO: Leaving for now, will use later\n./.venv/lib/python3.13/site-packages/torch/include/ATen/native/MaxPooling.h:// TODO(Heitor) Template by dimension\n./.venv/lib/python3.13/site-packages/torch/include/ATen/native/mkldnn/xpu/detail/Attr.h:          // TODO [Asymmetric]:\n./.venv/lib/python3.13/site-packages/torch/include/ATen/native/mkldnn/xpu/detail/DnnlExt.h:// TODO: bias types maybe not right\n./.venv/lib/python3.13/site-packages/torch/include/ATen/native/mkldnn/xpu/detail/DnnlExt.h:// TODO: use template specialization on struct\n./.venv/lib/python3.13/site-packages/torch/include/ATen/native/cpu/UpSampleKernelAVXAntialias.h:// TODO: This file only supports AVX2. We could split the AVX kernels into\n./.venv/lib/python3.13/site-packages/torch/include/ATen/native/cpu/UpSampleKernelAVXAntialias.h:  // TODO: we may want to merge that into the fallback code (currently called\n./.venv/lib/python3.13/site-packages/torch/include/ATen/native/cpu/UpSampleKernelAVXAntialias.h:  // TODO: we may want to merge that into the fallback code (currently called\n./.venv/lib/python3.13/site-packages/torch/include/ATen/native/cpu/UpSampleKernelAVXAntialias.h:// future improvement that can be done: look for the TODOs in this file.\n./.venv/lib/python3.13/site-packages/torch/include/ATen/native/cpu/UpSampleKernelAVXAntialias.h:  // TODO: Do we also need block 4 ???\n./.venv/lib/python3.13/site-packages/torch/include/ATen/native/cpu/LogSoftmaxKernelImpl.h:// TODO: find a way to have only one copy of divup.\n./.venv/lib/python3.13/site-packages/torch/include/ATen/native/DistributionTemplates.h:  // TODO: instead of variable name 'sigma', use 'gamma' or 'scale'\n./.venv/lib/python3.13/site-packages/torch/include/ATen/native/DistributionTemplates.h:  // TODO: Fix resize_as_. See pytorch/pytorch#11665.\n./.venv/lib/python3.13/site-packages/torch/include/ATen/native/transformers/cuda/mem_eff_attention/kernel_backward.h:  // TODO: Figure out why it's slower on the f32 kernels\n./.venv/lib/python3.13/site-packages/torch/include/ATen/native/transformers/cuda/mem_eff_attention/kernel_backward.h:  // TODO: Remove condition on `kOutputInRF` - this is needed to work\n./.venv/lib/python3.13/site-packages/torch/include/ATen/native/transformers/cuda/mem_eff_attention/kernel_backward.h:      // TODO: This must be terribly inefficient. There must be a better way\n./.venv/lib/python3.13/site-packages/torch/include/ATen/native/transformers/cuda/mem_eff_attention/kernel_backward.h:              // TODO: Otherwise we can get nans as we\n./.venv/lib/python3.13/site-packages/torch/include/ATen/native/transformers/cuda/mem_eff_attention/kernel_forward.h:          // MM1::Mma::drain_cp_asyncs(); # TODO figure out if this is needed for correctness\n./.venv/lib/python3.13/site-packages/torch/include/ATen/native/transformers/cuda/mem_eff_attention/gemm/mma_from_smem.h:  static int const kElementsPerAccess = 2; // TODO: Why 2?\n./.venv/lib/python3.13/site-packages/torch/include/ATen/native/transformers/cuda/mem_eff_attention/gemm/mma_from_smem.h:    // TODO: Optimize for each architecture\n./.venv/lib/python3.13/site-packages/torch/include/ATen/native/transformers/cuda/mem_eff_attention/gemm/mma_from_smem.h:    // TODO: Optimize for each architecture\n./.venv/lib/python3.13/site-packages/torch/include/ATen/native/transformers/cuda/mem_eff_attention/gemm_kernel_utils.h:// TODO: Maybe we could rely on Cutlass's DefaultGemm templates\n./.venv/lib/python3.13/site-packages/torch/include/ATen/native/UpSample.h:    // TODO: Our current linear mode impls use unbound indices\n./.venv/lib/python3.13/site-packages/torch/include/ATen/native/mps/OperationUtils.h:// TODO: Improve the overall design of MPSGraphCache.\n./.venv/lib/python3.13/site-packages/torch/include/ATen/native/mps/MPSGraphVenturaOps.h:// TODO: Remove me when moved to MacOS 13\n./.venv/lib/python3.13/site-packages/torch/include/ATen/native/SparseTensorUtils.h:// TODO: put this into the public API\n./.venv/lib/python3.13/site-packages/torch/include/ATen/native/SparseTensorUtils.h:// TODO: Expose this for real in ATen, some day?\n./.venv/lib/python3.13/site-packages/torch/include/ATen/native/CPUFallback.h:// TODO: update and add a usage example after https://github.com/pytorch/pytorch/pull/58092 lands.\n./.venv/lib/python3.13/site-packages/torch/include/ATen/native/CPUFallback.h:            // TODO: figure out how to make compiler happy without dynamic casts\n./.venv/lib/python3.13/site-packages/torch/include/ATen/native/CPUFallback.h:            // TODO: get std::forward<> to work\n./.venv/lib/python3.13/site-packages/torch/include/ATen/native/nested/NestedTensorUtils.h:// TODO: Figure out if we need a non-moving wrap_buffer()\n./.venv/lib/python3.13/site-packages/torch/include/ATen/native/nested/NestedTensorUtils.h:// TODO: Add static assert to verify lambda arguments match nested_node types\n./.venv/lib/python3.13/site-packages/torch/include/ATen/native/Resize.h:// TODO: make all operations that resize given outputs use this function\n./.venv/lib/python3.13/site-packages/torch/include/ATen/native/SharedReduceOps.h:  // TODO: remove this special case for HIP when issue is fixed:\n./.venv/lib/python3.13/site-packages/torch/include/ATen/native/SharedReduceOps.h:  // TODO: remove this special case for HIP when issue is fixed:\n./.venv/lib/python3.13/site-packages/torch/include/ATen/native/quantized/cudnn/utils.h:  // TODO: when and if cudnn enables padding in their operators, we can remove padding on our end and rename this to orig_weight_\n./.venv/lib/python3.13/site-packages/torch/include/ATen/native/quantized/cudnn/utils.h:// TODO: we can remove this function when cuDNN enables pass by value support for\n./.venv/lib/python3.13/site-packages/torch/include/ATen/native/quantized/cudnn/utils.h:// TODO: there is a table from input dtype to operator dtype, we can derive\n./.venv/lib/python3.13/site-packages/torch/include/ATen/native/quantized/cudnn/utils.h:// TODO: there is a table from input dtype to operator dtype, we can derive\n./.venv/lib/python3.13/site-packages/torch/include/ATen/native/quantized/cudnn/utils.h:// TODO: there is a table from input dtype to operator dtype, we can derive\n./.venv/lib/python3.13/site-packages/torch/include/ATen/native/quantized/cpu/OnednnUtils.h:  // TODO Support more OSs.\n./.venv/lib/python3.13/site-packages/torch/include/ATen/native/quantized/cpu/fbgemm_utils.h:// TODO: Remove functions below when ChannelsLast3d is ready.\n./.venv/lib/python3.13/site-packages/torch/include/ATen/native/quantized/AffineQuantizerBase.h:// TODO combine this with quantize_val once the numerics for ARM are aligned\n./.venv/lib/python3.13/site-packages/torch/include/ATen/native/ao_sparse/quantized/cpu/qnnpack_utils.h:// TODO: Refacto QnnpackUtils.h so as to separate code\n./.venv/lib/python3.13/site-packages/torch/include/ATen/native/cuda/ScanUtils.cuh:// TODO: The implementation of `tensor_kernel_scan_outer_dim` and\n./.venv/lib/python3.13/site-packages/torch/include/ATen/native/cuda/reduction_template.cuh://TODO this will need to be different for more generic reduction functions\n./.venv/lib/python3.13/site-packages/torch/include/ATen/native/cuda/reduction_template.cuh://TODO - multi-output reduction - we won't be able to use thrust::pair\n./.venv/lib/python3.13/site-packages/torch/include/ATen/native/cuda/ForeachFunctors.cuh:// TODO(crcrpar): Handle version bump in codegen.\n./.venv/lib/python3.13/site-packages/torch/include/ATen/native/cuda/Reduce.cuh:  //TODO for now arg_t is always opmath_t of the input, later we'll need to change it\n./.venv/lib/python3.13/site-packages/torch/include/ATen/native/cuda/Reduce.cuh:  //TODO - ReduceJitOp will probably need to be changed for reductions that need full functor,\n./.venv/lib/python3.13/site-packages/torch/include/ATen/native/cuda/Reduce.cuh://TODO this is 100 lines of almost-copy-paste, because we have to have different template args for this function\n./.venv/lib/python3.13/site-packages/torch/include/ATen/native/cuda/Reduce.cuh:  //TODO - this will be different for more complicated reductions, but for now reductions using\n./.venv/lib/python3.13/site-packages/torch/include/ATen/native/cuda/Reduce.cuh:      owned_buf_ptr.reset(new AccumulationBuffer(sizeof(out_scalar_t), //TODO\n./.venv/lib/python3.13/site-packages/torch/include/ATen/native/cuda/Reduce.cuh:                                                 output_memory_size * sizeof(out_scalar_t))); //TODO\n./.venv/lib/python3.13/site-packages/torch/include/ATen/native/cuda/Reduce.cuh:  //TODO - for now we support a single input, we may be able to relax this constraint\n./.venv/lib/python3.13/site-packages/torch/include/ATen/native/cuda/CUDALoops.cuh:  // TODO: Revisit this after CUDA 12.8 update.\n./.venv/lib/python3.13/site-packages/torch/include/ATen/native/cuda/vol2col.cuh:    // TODO: use LCM of stride and dilation to avoid unnecessary loops\n./.venv/lib/python3.13/site-packages/torch/include/ATen/native/cuda/CUDAJitLoops.cuh:  // TODO: Revisit this after CUDA 12.8 update.\n./.venv/lib/python3.13/site-packages/torch/include/ATen/native/cuda/CUDAJitLoops.cuh:  // TODO: Memory use can probably be optimized by re-using kernels across GPUs with\n./.venv/lib/python3.13/site-packages/torch/include/ATen/native/cuda/CUDAJitLoops.cuh:  constexpr int nOutputs = 1;  // TODO: Support more than 1 output\n./.venv/lib/python3.13/site-packages/torch/include/ATen/native/cuda/Loops.cuh:    // TODO: When all kernels that use gpu_kernel_with_scalars are\n./.venv/lib/python3.13/site-packages/torch/include/ATen/native/cuda/Loops.cuh:    // TODO: When all kernels that use gpu_kernel_with_scalars are\n./.venv/lib/python3.13/site-packages/torch/include/ATen/native/cuda/jit_utils.h:    // TODO(#146647): Can the code here be made generic for any scalartype?\n./.venv/lib/python3.13/site-packages/torch/include/ATen/native/cuda/im2col.cuh:  // TODO: use LCM of stride and dilation to avoid unnecessary loops\n./.venv/lib/python3.13/site-packages/torch/include/ATen/native/cuda/MultiTensorApply.cuh:// TODO(crcrpar): Add `n>5` for `low prec params & their higher prec copy`\n./.venv/lib/python3.13/site-packages/torch/include/ATen/native/cuda/UpSample.cuh:// TODO: Remove duplicate declaration.\n./.venv/lib/python3.13/site-packages/torch/include/ATen/native/cuda/UpSample.cuh:// TODO: Remove duplication with Upsample.h (CPU).\n./.venv/lib/python3.13/site-packages/torch/include/ATen/native/cuda/UpSample.cuh:/* TODO: move this to a common place */\n./.venv/lib/python3.13/site-packages/torch/include/ATen/native/cuda/UpSample.cuh:  /* TODO: result here is truncated to scalar_t,\n./.venv/lib/python3.13/site-packages/torch/include/ATen/native/cuda/Math.cuh:// TODO: elaborate in this comment on the structure of math.cuh\n./.venv/lib/python3.13/site-packages/torch/include/ATen/native/cuda/Math.cuh:    // TODO: what's an \"inverted interval\"? Open on the left\n./.venv/lib/python3.13/site-packages/torch/include/ATen/native/cuda/Math.cuh:    // TODO: what's an \"inverted interval\"? Open on the left\n./.venv/lib/python3.13/site-packages/torch/include/ATen/native/cuda/Math.cuh:  // TODO: review if this is computing in double when given a float input\n./.venv/lib/python3.13/site-packages/torch/include/ATen/native/cuda/JitLoops.cuh:  // TODO: much of preamble is common to both jitted_gpu_kernel and gpu_kernel\n./.venv/lib/python3.13/site-packages/torch/include/ATen/native/cuda/JitLoops.cuh:// TODO: support runtime state capture similar to `jitted_gpu_kernel`.\n./.venv/lib/python3.13/site-packages/torch/include/ATen/native/cuda/JitLoops.cuh:    // TODO: When all kernels that use gpu_kernel_with_scalars are\n./.venv/lib/python3.13/site-packages/torch/include/ATen/native/DispatchStub.h:// TODO: CPU instruction set selection should be folded into whatever\n./.venv/lib/python3.13/site-packages/torch/include/ATen/native/DispatchStub.h:    // TODO: make this point at hip_dispatch_ptr\n./.venv/lib/python3.13/site-packages/torch/include/ATen/native/DispatchStub.h:// TODO: cut this over to HIP dispatch once we stop pretending that CUDA\n./.venv/lib/python3.13/site-packages/torch/include/ATen/native/ConvUtils.h:  // TODO: check that output->size() matches output_sizes\n./.venv/lib/python3.13/site-packages/torch/include/ATen/native/ConvUtils.h:  // TODO: check that weight matches output->sizes()\n./.venv/lib/python3.13/site-packages/torch/include/ATen/native/ConvUtils.h:  // TODO: Remove PYTORCH_MIOPEN_SUGGEST_NHWC once ROCm officially supports NHWC in MIOpen\n./.venv/lib/python3.13/site-packages/torch/include/ATen/native/utils/Factory.h:// TODO: Remove this function when at::native::empty() is modified to accept a\n./.venv/lib/python3.13/site-packages/torch/include/ATen/CPUFunctions.h:// TODO Undo all logic introduced for Note [Avoiding Include Cycles In Static Dispatch]\n./.venv/lib/python3.13/site-packages/torch/include/ATen/ATen.h:// TODO: try to remove this\n./.venv/lib/python3.13/site-packages/torch/include/ATen/TensorUtils.h:// TODO: Consider generalizing this into a call stack.\n./.venv/lib/python3.13/site-packages/torch/include/ATen/TracerMode.h:// [TODOs]\n./.venv/lib/python3.13/site-packages/torch/include/ATen/TracerMode.h:// TODO: move this from `at::` to `jit::torch::` after\n./.venv/lib/python3.13/site-packages/torch/jit/_recursive.py:# TODO: there should be a more principled way of doing this.\n./.venv/lib/python3.13/site-packages/torch/jit/_recursive.py:            # TODO: We should really error in this case, but its bc-breaking so\n./.venv/lib/python3.13/site-packages/torch/jit/_recursive.py:            # TODO: We should really error in this case, but its bc-breaking so\n./.venv/lib/python3.13/site-packages/torch/jit/_recursive.py:            # TODO: could add more detail here. For example, what the user should do\n./.venv/lib/python3.13/site-packages/torch/jit/_recursive.py:            # TODO: Why skip this? Because @torch.jit._overload_method will\n./.venv/lib/python3.13/site-packages/torch/jit/_recursive.py:        # TODO: we don't currently do this functions that are recursively\n./.venv/lib/python3.13/site-packages/torch/jit/_recursive.py:    (TODO add a link when the rules are published).\n./.venv/lib/python3.13/site-packages/torch/jit/_script.py:                # TODO: we don't have _concrete_type set after load(), and in general we lose constant information.\n./.venv/lib/python3.13/site-packages/torch/jit/_script.py:                # TODO: it's possible that the following is confusing:\n./.venv/lib/python3.13/site-packages/torch/jit/_script.py:    # TODO MAKE SURE THAT DISABLING WORKS\n./.venv/lib/python3.13/site-packages/torch/jit/frontend.py:    # TODO: proper overriding analysis when implementing class inheritance\n./.venv/lib/python3.13/site-packages/torch/jit/frontend.py:# TODO: more robust handling of recognizing ignore context manager\n./.venv/lib/python3.13/site-packages/torch/jit/frontend.py:        # TODO: add input, output validator\n./.venv/lib/python3.13/site-packages/torch/jit/frontend.py:            # TODO: try to recover the location of else:? Python doesn't give us useful\n./.venv/lib/python3.13/site-packages/torch/jit/_decompositions.py:# TODO: replace torch.sigmoid -> aten.sigmoid\n./.venv/lib/python3.13/site-packages/torch/jit/_check.py:        # TODO @ansley: add `Union` once landed\n./.venv/lib/python3.13/site-packages/torch/jit/annotations.py:        # TODO: this is hack to recognize NumberType\n./.venv/lib/python3.13/site-packages/torch/jit/annotations.py:        # TODO: Determine if the other cases need to be fixed as well\n./.venv/lib/python3.13/site-packages/torch/jit/annotations.py:    # TODO: Consider not exporting these during wildcard import (reserve\n./.venv/lib/python3.13/site-packages/torch/jit/_trace.py:            # TODO: figure out one liner to .clone() and set requires_grad\n./.venv/lib/python3.13/site-packages/torch/jit/_trace.py:    # TODO: In principle, we track device information in our trace, so it\n./.venv/lib/python3.13/site-packages/torch/jit/_trace.py:    # TODO: Consider adding a utility function to torch.jit to test\n./.venv/lib/python3.13/site-packages/torch/jit/_trace.py:        # TODO: I'm not sure if the clone here is necessary but it is safer\n./.venv/lib/python3.13/site-packages/torch/jit/_shape_functions.py:            # TODO: only assertion error is bound in C++ compilation right now\n./.venv/lib/python3.13/site-packages/torch/jit/_shape_functions.py:            # TODO: only assertion error is bound in C++ compilation right now\n./.venv/lib/python3.13/site-packages/torch/jit/_shape_functions.py:    # TODO: return self\n./.venv/lib/python3.13/site-packages/torch/jit/_shape_functions.py:        # TODO: handling of slice\n./.venv/lib/python3.13/site-packages/torch/jit/_shape_functions.py:        # TODO: handling of slice\n./.venv/lib/python3.13/site-packages/torch/jit/_shape_functions.py:    # TODO: look into rewriting with early return and getting loop unrolling to fire\n./.venv/lib/python3.13/site-packages/torch/jit/_shape_functions.py:    # TODO: assertions could be expanded with the error messages\n./.venv/lib/python3.13/site-packages/torch/jit/_shape_functions.py:        # TODO: return self\n./.venv/lib/python3.13/site-packages/torch/jit/_shape_functions.py:    # TODO: use slicing when slice optimization has landed\n./.venv/lib/python3.13/site-packages/torch/jit/_shape_functions.py:# TODO: migrate over all of symbolic_shape_registry_util.cpp\n./.venv/lib/python3.13/site-packages/torch/jit/_shape_functions.py:# quantized_conv_prepack TODO\n./.venv/lib/python3.13/site-packages/torch/jit/_builtins.py:    # TODO: add support for more ops\n./.venv/lib/python3.13/site-packages/torch/jit/_monkeytype_config.py:    TODO: To remove this check once Union support lands.\n./.venv/lib/python3.13/site-packages/torch/jit/_monkeytype_config.py:                    # TODO: To remove this check once Union suppport in TorchScript lands.\n./.venv/lib/python3.13/site-packages/torch/jit/_serialization.py:    # TODO: Pretty sure this approach loses ConstSequential status and such\n./.venv/lib/python3.13/site-packages/torch/_utils_internal.py:            return 1700  # TODO: placeholder, get actual value\n./.venv/lib/python3.13/site-packages/scipy/cluster/hierarchy.py:        # TODO ARRAY_API complex indexing not supported\n./.venv/lib/python3.13/site-packages/scipy/interpolate/_fitpack_impl.py:TODO: Make interfaces to the following fitpack functions:\n./.venv/lib/python3.13/site-packages/scipy/_lib/array_api_compat/cupy/_info.py:        # TODO: Does this depend on device?\n./.venv/lib/python3.13/site-packages/scipy/_lib/array_api_compat/cupy/_info.py:        # TODO: Does this depend on device?\n./.venv/lib/python3.13/site-packages/scipy/_lib/array_api_compat/dask/array/_aliases.py:    # TODO: respect device keyword?\n./.venv/lib/python3.13/site-packages/scipy/_lib/array_api_compat/dask/array/_aliases.py:    # TODO: respect device keyword?\n./.venv/lib/python3.13/site-packages/scipy/_lib/array_api_compat/dask/array/_aliases.py:    # TODO: respect device keyword?\n./.venv/lib/python3.13/site-packages/scipy/_lib/array_api_compat/dask/array/_aliases.py:    # TODO: This won't handle dask unknown shapes\n./.venv/lib/python3.13/site-packages/scipy/_lib/array_api_compat/dask/array/linalg.py:# TODO: use the QR wrapper once dask\n./.venv/lib/python3.13/site-packages/scipy/_lib/array_api_compat/dask/array/linalg.py:    # TODO: can't avoid computing U or V for dask\n./.venv/lib/python3.13/site-packages/scipy/_lib/array_api_compat/torch/_aliases.py:    # TODO: is the return type a list or a tuple\n./.venv/lib/python3.13/site-packages/scipy/_lib/array_api_compat/common/_aliases.py:    # TODO: import from typing (requires Python >=3.13)\n./.venv/lib/python3.13/site-packages/scipy/_lib/array_api_compat/common/_aliases.py:    # TODO: The standard is not clear about what should happen when x.ndim == 0.\n./.venv/lib/python3.13/site-packages/scipy/_lib/array_api_compat/common/_aliases.py:    # TODO: np.clip has other ufunc kwargs\n./.venv/lib/python3.13/site-packages/scipy/_lib/array_api_compat/common/_helpers.py:    # TODO: import from typing (requires Python >=3.13)\n./.venv/lib/python3.13/site-packages/scipy/_lib/array_api_compat/common/_helpers.py:    # TODO: Should we reject ndarray subclasses?\n./.venv/lib/python3.13/site-packages/scipy/_lib/array_api_compat/common/_helpers.py:    # TODO: Account for other backends.\n./.venv/lib/python3.13/site-packages/scipy/_lib/array_api_compat/common/_helpers.py:        # TODO: drop support for numpy<2 which didn't have __array_namespace__\n./.venv/lib/python3.13/site-packages/scipy/_lib/array_api_compat/common/_helpers.py:        # TODO: drop support for jax<0.4.32 which didn't have __array_namespace__\n./.venv/lib/python3.13/site-packages/scipy/_lib/array_api_compat/common/_helpers.py:            # TODO: Support Python scalars?\n./.venv/lib/python3.13/site-packages/scipy/_lib/array_api_compat/common/_helpers.py:        # TODO: What if our array is on the GPU already?\n./.venv/lib/python3.13/site-packages/scipy/_lib/array_api_extra/_lib/_utils/_typing.pyi:# TODO import from typing (requires Python >=3.12)\n./.venv/lib/python3.13/site-packages/scipy/_lib/array_api_extra/_lib/_utils/_typing.pyi:# TODO: use array-api-typing once it is available\n./.venv/lib/python3.13/site-packages/scipy/_lib/array_api_extra/_lib/_utils/_compat.pyi:# TODO import from typing (requires Python >=3.13)\n./.venv/lib/python3.13/site-packages/scipy/_lib/array_api_extra/_lib/_utils/_helpers.py:    # TODO import from typing (requires Python >=3.12 and >=3.13)\n./.venv/lib/python3.13/site-packages/scipy/_lib/array_api_extra/_lib/_utils/_helpers.py:    TODO this helper should be eventually removed once all the special cases\n./.venv/lib/python3.13/site-packages/scipy/_lib/array_api_extra/_lib/_at.py:    # TODO import from typing (requires Python >=3.11)\n./.venv/lib/python3.13/site-packages/scipy/_lib/array_api_extra/testing.py:    # TODO import override from typing (requires Python >=3.12)\n./.venv/lib/python3.13/site-packages/scipy/_lib/pyprima/cobyla/geometry.py:    TODO: Check whether it improves the performance if JDROP = NUM_VARS is allowed when\n./.venv/lib/python3.13/site-packages/scipy/fft/_pocketfft/tests/test_basic.py:# TODO: Is this test actually valuable? The behavior it's testing shouldn't be\n./.venv/lib/python3.13/site-packages/scipy/fft/_pocketfft/basic.py:    # TODO: Optimize for hermitian and real?\n./.venv/lib/python3.13/site-packages/scipy/fft/_pocketfft/basic.py:    # TODO: Optimize for hermitian and real?\n./.venv/lib/python3.13/site-packages/scipy/fft/tests/test_real_transforms.py:    # TODO write an array-agnostic pad\n./.venv/lib/python3.13/site-packages/scipy/signal/_ltisys.py:    # TODO: This could use some more work.\n./.venv/lib/python3.13/site-packages/scipy/signal/_filter_design.py:                and n_fft > 0):  # TODO: review threshold acc. to benchmark?\n./.venv/lib/python3.13/site-packages/scipy/signal/_filter_design.py:    # TODO in the near future:\n./.venv/lib/python3.13/site-packages/scipy/signal/_filter_design.py:# TODO: Make this a real public function scipy.misc.ff\n./.venv/lib/python3.13/site-packages/scipy/signal/tests/test_signaltools.py:    @skip_xp_backends(np_only=True, reason=\"TODO: convert this test\")\n./.venv/lib/python3.13/site-packages/scipy/signal/tests/test_signaltools.py:    @xfail_xp_backends(np_only=True, reason=\"TODO: swapaxes\")\n./.venv/lib/python3.13/site-packages/scipy/signal/tests/test_windows.py:    @xfail_xp_backends(np_only=True, reason='TODO: make resample array API ready')\n./.venv/lib/python3.13/site-packages/scipy/signal/tests/test_ltisys.py:        # TODO: add meaningful test where X0 is a list\n./.venv/lib/python3.13/site-packages/scipy/signal/tests/test_ltisys.py:        # TODO: add meaningful test where X0 is a list\n./.venv/lib/python3.13/site-packages/scipy/signal/tests/test_bsplines.py:    @skip_xp_backends(np_only=True, reason=\"TODO: convert this test\")\n./.venv/lib/python3.13/site-packages/scipy/signal/tests/test_bsplines.py:    @skip_xp_backends(np_only=True, reason=\"TODO: convert this test\")\n./.venv/lib/python3.13/site-packages/scipy/signal/tests/test_bsplines.py:    @skip_xp_backends(np_only=True, reason=\"TODO: convert this test\")\n./.venv/lib/python3.13/site-packages/scipy/signal/tests/test_filter_design.py:        # TODO: Why so inaccurate?  Is reference flawed?\n./.venv/lib/python3.13/site-packages/scipy/signal/tests/test_filter_design.py:        # TODO: Why so inaccurate?  Is reference flawed?\n./.venv/lib/python3.13/site-packages/scipy/signal/tests/test_filter_design.py:        # TODO: Why so inaccurate?  Is reference flawed?\n./.venv/lib/python3.13/site-packages/scipy/signal/tests/test_filter_design.py:        # TODO: Why so inaccurate?  Is reference flawed?\n./.venv/lib/python3.13/site-packages/scipy/spatial/transform/tests/test_rotation.py:    # TODO: Why are we using _as_euler_from_matrix here? As a sanity check? It is not\n./.venv/lib/python3.13/site-packages/scipy/spatial/transform/tests/test_rotation.py:    # TODO: Same as before: Remove _as_euler_from_matrix?\n./.venv/lib/python3.13/site-packages/scipy/spatial/transform/tests/test_rotation.py:    # TODO: Do we want to support this for all Array API frameworks?\n./.venv/lib/python3.13/site-packages/scipy/spatial/distance.pyi:# TODO: Use a single protocol with a parameter specification variable\n./.venv/lib/python3.13/site-packages/scipy/spatial/distance.pyi:# TODO: Add `metric`-specific overloads\n./.venv/lib/python3.13/site-packages/scipy/spatial/distance.pyi:# TODO: Wait for dtype support; the return type is\n./.venv/lib/python3.13/site-packages/scipy/spatial/distance.pyi:# TODO: Wait for dtype support; the return type is\n./.venv/lib/python3.13/site-packages/scipy/spatial/distance.pyi:# TODO: Add `metric`-specific overloads\n./.venv/lib/python3.13/site-packages/scipy/optimize/_optimize.py:    # TODO: add hessp (callable or FD) to ScalarFunction?\n./.venv/lib/python3.13/site-packages/scipy/optimize/_shgo_lib/_complex.py:            sign_det_A_11 = -1  # TODO: Choose another det of j instead?\n./.venv/lib/python3.13/site-packages/scipy/optimize/_shgo_lib/_complex.py:            # TODO: Unlikely to work in many cases\n./.venv/lib/python3.13/site-packages/scipy/optimize/_shgo_lib/_complex.py:            # TODO: Note that scipy might be faster to add as an optional\n./.venv/lib/python3.13/site-packages/scipy/optimize/_shgo_lib/_complex.py:            # TODO: Note if sign_det_A_j0 == then the point is coplanar to the\n./.venv/lib/python3.13/site-packages/scipy/optimize/_shgo_lib/_complex.py:        # TODO: Is checking the projection of one vertex against faces of other\n./.venv/lib/python3.13/site-packages/scipy/optimize/_shgo_lib/_complex.py:        # TODO: Literature seems to suggest using proj.T, but why is this\n./.venv/lib/python3.13/site-packages/scipy/optimize/_shgo_lib/_complex.py:        if np.linalg.det(proj) == 0.0:  # TODO: Replace with tolerance?\n./.venv/lib/python3.13/site-packages/scipy/optimize/_remove_redundancy.py:        v = U[:, -1]  # TODO: return these so user can eliminate from problem?\n./.venv/lib/python3.13/site-packages/scipy/optimize/_linprog_util.py:        if rr and A_eq.size > 0:  # TODO: Fast sparse rank check?\n./.venv/lib/python3.13/site-packages/scipy/optimize/_linprog_util.py:        try:  # TODO: use results of first SVD in _remove_redundancy_svd\n./.venv/lib/python3.13/site-packages/scipy/optimize/_shgo.py:        pass  # TODO\n./.venv/lib/python3.13/site-packages/scipy/optimize/_shgo.py:                    # self.n #TODO: Should always be self.n, this is\n./.venv/lib/python3.13/site-packages/scipy/optimize/_shgo.py:        # TODO: Only do this if global mode\n./.venv/lib/python3.13/site-packages/scipy/optimize/_shgo.py:            # TODO: Uncertain if n_prc needs to add len(self.LMC.xl_maps)\n./.venv/lib/python3.13/site-packages/scipy/optimize/_trustregion_constr/projections.py:    # TODO: revert this once the warning bug fix in sksparse is merged/released\n./.venv/lib/python3.13/site-packages/scipy/optimize/_trustregion_constr/projections.py:    # TODO: Use a symmetric indefinite factorization\n./.venv/lib/python3.13/site-packages/scipy/optimize/_trustregion_constr/tr_interior_point.py:        # TODO: Use more advanced strategies from [2]_\n./.venv/lib/python3.13/site-packages/scipy/optimize/_trustregion_constr/qp_subproblem.py:    # TODO: Use a symmetric indefinite factorization\n./.venv/lib/python3.13/site-packages/scipy/optimize/_chandrupatla.py:# TODO:\n./.venv/lib/python3.13/site-packages/scipy/optimize/_linprog_rs.py:    # TODO: test redundant row removal better\n./.venv/lib/python3.13/site-packages/scipy/optimize/_linprog_rs.py:    # TODO: make solve more efficient with BGLU? This could take a while.\n./.venv/lib/python3.13/site-packages/scipy/optimize/_linprog_rs.py:        # TODO: cythonize?\n./.venv/lib/python3.13/site-packages/scipy/optimize/tests/test_chandrupatla.py:        # # TODO: Test zero tolerance\n./.venv/lib/python3.13/site-packages/scipy/optimize/tests/test__shgo.py:        # TODO: Make default n higher for faster tests\n./.venv/lib/python3.13/site-packages/scipy/optimize/tests/test__shgo.py:        # TODO: This test doesn't cover anything new, it is unknown what the\n./.venv/lib/python3.13/site-packages/scipy/optimize/tests/test__remove_redundancy.py:# TODO: add tests for:\n./.venv/lib/python3.13/site-packages/scipy/optimize/tests/test_optimize.py:        # TODO this test should really be equivalent to factorized version\n./.venv/lib/python3.13/site-packages/scipy/optimize/_direct_py.py:    # TODO: fix disp argument\n./.venv/lib/python3.13/site-packages/scipy/optimize/_linprog_ip.py:                # TODO: revert this suppress_warning once the warning bug fix in\n./.venv/lib/python3.13/site-packages/scipy/differentiate/_differentiate.py:    # TODO (followup):\n./.venv/lib/python3.13/site-packages/scipy/integrate/_rules/_gauss_legendre.py:        # TODO: current converting to/from numpy\n./.venv/lib/python3.13/site-packages/scipy/integrate/_rules/_gauss_kronrod.py:        # TODO: nodes and weights are currently hard-coded for values 15 and 21, but in\n./.venv/lib/python3.13/site-packages/scipy/integrate/_rules/_genz_malik.py:        # TODO: Currently only support for degree 7 Genz-Malik cubature, should aim to\n./.venv/lib/python3.13/site-packages/scipy/integrate/_rules/_genz_malik.py:        # TODO: Currently only support for the degree 5 lower rule, in the future it\n./.venv/lib/python3.13/site-packages/scipy/stats/_correlation.py:# TODO:\n./.venv/lib/python3.13/site-packages/scipy/stats/_axis_nan_policy.py:# TODO: add support for `axis` tuples\n./.venv/lib/python3.13/site-packages/scipy/stats/_distribution_infrastructure.py:# TODO:\n./.venv/lib/python3.13/site-packages/scipy/stats/_distribution_infrastructure.py:    # TODO: properly avoid NaN when y is negative infinity\n./.venv/lib/python3.13/site-packages/scipy/stats/_distribution_infrastructure.py:    # TODO: silence warning with taking log of complex nan\n./.venv/lib/python3.13/site-packages/scipy/stats/_distribution_infrastructure.py:    # TODO: deal with x == y better\n./.venv/lib/python3.13/site-packages/scipy/stats/_distribution_infrastructure.py:    # TODO:\n./.venv/lib/python3.13/site-packages/scipy/stats/_distribution_infrastructure.py:        # TODO:\n./.venv/lib/python3.13/site-packages/scipy/stats/_distribution_infrastructure.py:    # TODO:\n./.venv/lib/python3.13/site-packages/scipy/stats/tests/test_continuous.py:    # TODO: add `supported` method and check here\n./.venv/lib/python3.13/site-packages/scipy/stats/tests/test_mstats_extras.py:    # Check that `var` keyword returns a value.  TODO: check whether returned\n./.venv/lib/python3.13/site-packages/scipy/stats/tests/test_mstats_extras.py:    # TODO: check that implementation is correct.\n./.venv/lib/python3.13/site-packages/scipy/stats/tests/test_mstats_basic.py:# TODO: for all ttest functions, add tests with masked array inputs\n./.venv/lib/python3.13/site-packages/scipy/stats/tests/test_stats.py:    # TODO: write these tests to handle missing values properly\n./.venv/lib/python3.13/site-packages/scipy/stats/tests/test_fast_gen_inversion.py:# TODO: add more distributions\n./.venv/lib/python3.13/site-packages/scipy/stats/tests/test_distributions.py:        # These are excluded by the filters below. TODO: Rewrite tests so that\n./.venv/lib/python3.13/site-packages/scipy/stats/tests/test_continuous_basic.py:        # TODO: multiple checks in this function are not robust, tweaking the\n./.venv/lib/python3.13/site-packages/scipy/stats/tests/test_morestats.py:    # TODO: add method \"pearsonr\" after fix overflow issue\n./.venv/lib/python3.13/site-packages/scipy/stats/tests/test_morestats.py:    # TODO: add method \"pearsonr\" after fix overflow issue\n./.venv/lib/python3.13/site-packages/scipy/stats/_qmc.py:        # TODO consider returning both the mean and the standard deviation\n./.venv/lib/python3.13/site-packages/scipy/stats/_morestats.py:    # TODO: calculate exact distribution considering ties\n./.venv/lib/python3.13/site-packages/scipy/stats/_page_trend_test.py:    if ranks.ndim != 2:  # TODO: relax this to accept 3d arrays?\n./.venv/lib/python3.13/site-packages/scipy/stats/_levy_stable/__init__.py:    # TODO: add more where possible with test coverage,\n./.venv/lib/python3.13/site-packages/scipy/stats/_levy_stable/__init__.py:    # TODO: add more where possible with test coverage,\n./.venv/lib/python3.13/site-packages/scipy/sparse/_index.py:                # TODO: make sparse matrix indexing work for sparray\n./.venv/lib/python3.13/site-packages/scipy/sparse/_index.py:            # TODO: handle this for nD (adjacent arrays stay, separated move to start)\n./.venv/lib/python3.13/site-packages/scipy/sparse/_bsr.py:                # TODO infer shape here\n./.venv/lib/python3.13/site-packages/scipy/sparse/_bsr.py:        # TODO eliminate zeros\n./.venv/lib/python3.13/site-packages/scipy/sparse/_dok.py:            # TODO implement resize across dimensions\n./.venv/lib/python3.13/site-packages/scipy/sparse/_construct.py:    # TODO: delete next 15 lines [combine with _eye()] once spmatrix removed\n./.venv/lib/python3.13/site-packages/scipy/sparse/_construct.py:    # TODO: delete next 10 lines and replace _sparse with _array when spmatrix removed\n./.venv/lib/python3.13/site-packages/scipy/sparse/_construct.py:    # TODO: delete next 8 lines and replace _sparse with _array when spmatrix removed\n./.venv/lib/python3.13/site-packages/scipy/sparse/_construct.py:    # TODO remove this if-structure when sparse matrices removed\n./.venv/lib/python3.13/site-packages/scipy/sparse/_compressed.py:        # TODO check for duplicates?\n./.venv/lib/python3.13/site-packages/scipy/sparse/_compressed.py:        # TODO: don't fall back to fancy indexing here\n./.venv/lib/python3.13/site-packages/scipy/sparse/_compressed.py:            # TODO: only sort where necessary\n./.venv/lib/python3.13/site-packages/scipy/sparse/tests/test_construct.py:#TODO check whether format=XXX is respected\n./.venv/lib/python3.13/site-packages/scipy/sparse/tests/test_base.py:# TODO test prune\n./.venv/lib/python3.13/site-packages/scipy/sparse/tests/test_base.py:# TODO test has_sorted_indices\n./.venv/lib/python3.13/site-packages/scipy/sparse/tests/test_base.py:        # TODO: properly handle this assertion on ppc64le\n./.venv/lib/python3.13/site-packages/scipy/sparse/tests/test_base.py:        # TODO check that NC has duplicates (which are not explicit zeros)\n./.venv/lib/python3.13/site-packages/scipy/sparse/tests/test_spfuncs.py:        #TODO expose through function\n./.venv/lib/python3.13/site-packages/scipy/sparse/_data.py:# TODO implement all relevant operations\n./.venv/lib/python3.13/site-packages/scipy/sparse/_base.py:            # TODO sparse broadcasting\n./.venv/lib/python3.13/site-packages/scipy/sparse/_csr.py:        # TODO: uncomment this once it's faster:\n./.venv/lib/python3.13/site-packages/scipy/sparse/linalg/_eigen/tests/test_svds.py:            # TODO: arpack crashes when v0=v0, which=\"SM\"\n./.venv/lib/python3.13/site-packages/scipy/sparse/linalg/_isolve/tests/test_iterative.py:# TODO check that method preserve shape and type\n./.venv/lib/python3.13/site-packages/scipy/sparse/linalg/_isolve/tests/test_iterative.py:# TODO test both preconditioner methods\n./.venv/lib/python3.13/site-packages/scipy/sparse/linalg/_isolve/tests/test_iterative.py:    # TODO: minres / tfqmr. It didn't historically use absolute tolerances, so\n./.venv/lib/python3.13/site-packages/scipy/sparse/linalg/_isolve/tests/test_iterative.py:        pytest.skip(\"TODO: Add atol to minres/tfqmr\")\n./.venv/lib/python3.13/site-packages/scipy/sparse/linalg/_isolve/minres.py:            break  # TODO check this\n./.venv/lib/python3.13/site-packages/scipy/sparse/linalg/tests/test_onenormest.py:        #TODO this test seems to give estimates that match the table,\n./.venv/lib/python3.13/site-packages/scipy/sparse/linalg/tests/test_onenormest.py:        #TODO even though no attempt has been made to deal with\n./.venv/lib/python3.13/site-packages/scipy/sparse/linalg/tests/test_onenormest.py:        #TODO complex numbers in the one-norm estimation.\n./.venv/lib/python3.13/site-packages/scipy/special/_support_alternative_backends.py:            # TODO use xpx.lazy_apply to add jax.jit support\n./.venv/lib/python3.13/site-packages/scipy/special/_lambertw.py:    # TODO: special expert should inspect this\n./.venv/lib/python3.13/site-packages/scipy/special/tests/test_basic.py:        # TODO: cannot use N itself yet; factorial uses `gamma(N+1)` resp. `(hi+lo)//2`\n./.venv/lib/python3.13/site-packages/scipy/special/tests/test_sf_error.py:    # TODO: special expert should correct\n./.venv/lib/python3.13/site-packages/scipy/io/_harwell_boeing/hb.py:# TODO:\n./.venv/lib/python3.13/site-packages/scipy/io/arff/_arffread.py:# TODO:\n./.venv/lib/python3.13/site-packages/scipy/io/arff/_arffread.py:        # TODO: this is where we are spending time (~80%). I think things\n./.venv/lib/python3.13/site-packages/scipy/io/_netcdf.py:# TODO:\n./.venv/lib/python3.13/site-packages/scipy/linalg/_matfuncs.py:    # TODO use a better error approximation\n./.venv/lib/python3.13/site-packages/scipy/linalg/_matfuncs_inv_ssq.py:#TODO renovate or move this class when scipy operators are more mature\n./.venv/lib/python3.13/site-packages/scipy/linalg/_matfuncs_inv_ssq.py:#TODO renovate or move this function when SciPy operators are more mature\n./.venv/lib/python3.13/site-packages/scipy/linalg/tests/test_lapack.py:                # TODO: Add a test for ONB?\n./.venv/lib/python3.13/site-packages/chromadb/base_types.py:Vector = NDArray[Union[np.int32, np.float32]]  # TODO: Specify that the vector is 1D\n./.venv/lib/python3.13/site-packages/chromadb/execution/executor/local.py:        # TODO: Fix typing\n./.venv/lib/python3.13/site-packages/chromadb/execution/executor/local.py:        # TODO: Fix typing\n./.venv/lib/python3.13/site-packages/chromadb/execution/executor/distributed.py:        # TODO: Fix typing\n./.venv/lib/python3.13/site-packages/chromadb/execution/executor/distributed.py:        # TODO: Fix typing\n./.venv/lib/python3.13/site-packages/chromadb/logservice/logservice.py:        # TODO: Fix this\n./.venv/lib/python3.13/site-packages/chromadb/server/fastapi/types.py:    # TODO: Pydantic doesn't bode well with recursive types so we use generic Dicts\n./.venv/lib/python3.13/site-packages/chromadb/server/fastapi/types.py:    # TODO: Make CollectionConfiguration a Pydantic model\n./.venv/lib/python3.13/site-packages/chromadb/test/test_api.py:# TODO: Define the dimensionality of these embeddingds in terms of the default record\n./.venv/lib/python3.13/site-packages/chromadb/test/property/test_embeddings.py:# TODO: Use SQL escaping correctly internally\n./.venv/lib/python3.13/site-packages/chromadb/test/property/invariants.py:    # TODO: this is a hack to get the space\n./.venv/lib/python3.13/site-packages/chromadb/test/property/invariants.py:    # TODO: ip and cosine are numerically unstable in HNSW.\n./.venv/lib/python3.13/site-packages/chromadb/test/property/test_restart_persist.py:    # TODO: This test is broken for rust bindings and should be fixed\n./.venv/lib/python3.13/site-packages/chromadb/test/property/test_add.py:            \"TODO @jai, come back and debug why CI runners fail with async + sync\"\n./.venv/lib/python3.13/site-packages/chromadb/test/property/test_add.py:            \"TODO @jai, come back and debug why CI runners fail with async + sync\"\n./.venv/lib/python3.13/site-packages/chromadb/test/property/test_add.py:            \"TODO @jai, come back and debug why CI runners fail with async + sync\"\n./.venv/lib/python3.13/site-packages/chromadb/test/property/test_add.py:    # TODO: Generative embedding functions\n./.venv/lib/python3.13/site-packages/chromadb/test/property/test_add.py:    # TODO: The type of add() is incorrect as it does not allow for metadatas\n./.venv/lib/python3.13/site-packages/chromadb/test/property/test_add.py:            \"TODO @jai, come back and debug why CI runners fail with async + sync\"\n./.venv/lib/python3.13/site-packages/chromadb/test/property/test_add.py:            \"TODO @jai, come back and debug why CI runners fail with async + sync\"\n./.venv/lib/python3.13/site-packages/chromadb/test/property/test_add.py:# TODO: This test fails right now because the ids are not sorted by the input order\n./.venv/lib/python3.13/site-packages/chromadb/test/property/test_add.py:            \"TODO @jai, come back and debug why CI runners fail with async + sync\"\n./.venv/lib/python3.13/site-packages/chromadb/test/property/test_add.py:            \"TODO @jai, come back and debug why CI runners fail with async + sync\"\n./.venv/lib/python3.13/site-packages/chromadb/test/property/test_add.py:    # TODO: We need to clean up the api types to support this typing\n./.venv/lib/python3.13/site-packages/chromadb/test/property/strategies.py:# TODO: support arbitrary text everywhere so we don't SQL-inject ourselves.\n./.venv/lib/python3.13/site-packages/chromadb/test/property/strategies.py:# TODO: support empty strings everywhere\n./.venv/lib/python3.13/site-packages/chromadb/test/property/strategies.py:)  # TODO: handle longs\n./.venv/lib/python3.13/site-packages/chromadb/test/property/strategies.py:)  # TODO: handle infinity and NAN\n./.venv/lib/python3.13/site-packages/chromadb/test/property/strategies.py:int_types = [np.int16, np.int32, np.int64]  # TODO: handle int types\n./.venv/lib/python3.13/site-packages/chromadb/test/property/strategies.py:# TODO: Use a hypothesis strategy while maintaining embedding uniqueness\n./.venv/lib/python3.13/site-packages/chromadb/test/property/strategies.py:            # TODO: pull the distance functions from a source of truth that lives not\n./.venv/lib/python3.13/site-packages/chromadb/test/property/strategies.py:    # TODO: Remove this once the distributed system supports $not_contains\n./.venv/lib/python3.13/site-packages/chromadb/test/test_multithreaded.py:    # TODO: batch_size and sync_threshold should be configurable\n./.venv/lib/python3.13/site-packages/chromadb/api/rust.py:# TODO(hammadb): Unify imports across types vs root __init__.py\n./.venv/lib/python3.13/site-packages/chromadb/api/rust.py:# TODO(hammadb): Propagate the types from the bindings into the Python API\n./.venv/lib/python3.13/site-packages/chromadb/api/rust.py:        # TODO: This event doesn't capture the get_or_create case appropriately\n./.venv/lib/python3.13/site-packages/chromadb/api/rust.py:        # TODO: Re-enable embedding function tracking in create_collection\n./.venv/lib/python3.13/site-packages/chromadb/api/rust.py:                # TODO: Fix this when posthog is pushed into Rust frontend\n./.venv/lib/python3.13/site-packages/chromadb/api/rust.py:    # TODO: Remove this if it's not planned to be used\n./.venv/lib/python3.13/site-packages/chromadb/api/collection_configuration.py:# TODO: make warnings prettier and add link to migration docs\n./.venv/lib/python3.13/site-packages/chromadb/api/collection_configuration.py:# TODO: make warnings prettier and add link to migration docs\n./.venv/lib/python3.13/site-packages/chromadb/api/collection_configuration.py:# TODO: make warnings prettier and add link to migration docs\n./.venv/lib/python3.13/site-packages/chromadb/api/collection_configuration.py:# TODO: make warnings prettier and add link to migration docs\n./.venv/lib/python3.13/site-packages/chromadb/api/collection_configuration.py:# TODO: make warnings prettier and add link to migration docs\n./.venv/lib/python3.13/site-packages/chromadb/api/types.py:        # TODO: Enforce this at the embedding function level\n./.venv/lib/python3.13/site-packages/chromadb/api/types.py:    # TODO: Validate URIs\n./.venv/lib/python3.13/site-packages/chromadb/api/types.py:# TODO: make warnings prettier and add link to migration docs\n./.venv/lib/python3.13/site-packages/chromadb/api/segment.py:    # TODO: Actually fix CollectionMetadata type to remove type: ignore flags. This is\n./.venv/lib/python3.13/site-packages/chromadb/api/segment.py:        # TODO: remove backwards compatibility in naming requirements\n./.venv/lib/python3.13/site-packages/chromadb/api/segment.py:        # TODO: Let sysdb create the collection directly from the model\n./.venv/lib/python3.13/site-packages/chromadb/api/segment.py:        # TODO: This event doesn't capture the get_or_create case appropriately\n./.venv/lib/python3.13/site-packages/chromadb/api/segment.py:        # TODO: Re-enable embedding function tracking in create_collection\n./.venv/lib/python3.13/site-packages/chromadb/api/segment.py:    # TODO: Actually fix CollectionMetadata type to remove type: ignore flags. This is\n./.venv/lib/python3.13/site-packages/chromadb/api/segment.py:        # TODO eventually we'll want to use OptionalArgument and Unspecified in the\n./.venv/lib/python3.13/site-packages/chromadb/api/segment.py:        # TODO: Replace with unified validation\n./.venv/lib/python3.13/site-packages/chromadb/api/segment.py:        # TODO: Replace with unified validation\n./.venv/lib/python3.13/site-packages/chromadb/api/segment.py:        # TODO: Replace with unified validation\n./.venv/lib/python3.13/site-packages/chromadb/api/segment.py:    # TODO: This could potentially cause race conditions in a distributed version of the\n./.venv/lib/python3.13/site-packages/chromadb/api/segment.py:    # TODO: promote collection -> topic to a base class method so that it can be\n./.venv/lib/python3.13/site-packages/chromadb/api/configuration.py:# TODO: move out of API\n./.venv/lib/python3.13/site-packages/chromadb/api/models/Collection.py:        # TODO: fixme\n./.venv/lib/python3.13/site-packages/chromadb/api/models/AsyncCollection.py:        # TODO: fixme\n./.venv/lib/python3.13/site-packages/chromadb/api/models/CollectionCommon.py:# TODO: We should rename the types in chromadb.types to be Models where\n./.venv/lib/python3.13/site-packages/chromadb/api/models/CollectionCommon.py:            # TODO: Handle URI updates.\n./.venv/lib/python3.13/site-packages/chromadb/api/shared_system_client.py:                    \"ephemeral\"  # TODO: support pathing and  multiple ephemeral clients\n./.venv/lib/python3.13/site-packages/chromadb/types.py:    # TODO: This throws away type information.\n./.venv/lib/python3.13/site-packages/chromadb/types.py:    # TODO: This doesn't check types.\n./.venv/lib/python3.13/site-packages/chromadb/segment/impl/manager/local.py:        # TODO: prototyping with distributed segment for now, but this should be a configurable option\n./.venv/lib/python3.13/site-packages/chromadb/segment/impl/vector/local_persistent_hnsw.py:        # TODO: This should really be stored in sqlite, the index itself, or a better\n./.venv/lib/python3.13/site-packages/chromadb/segment/impl/vector/local_hnsw.py:            # TODO: these casts are not correct, hnswlib returns np\n./.venv/lib/python3.13/site-packages/chromadb/segment/impl/vector/brute_force_index.py:            # TODO: It may be faster to use multi-index selection on the vectors array\n./.venv/lib/python3.13/site-packages/chromadb/segment/impl/distributed/segment_directory.py:            # TODO: For some reason the above can flake and the memberlist won't be populated\n./.venv/lib/python3.13/site-packages/chromadb/segment/impl/distributed/segment_directory.py:        # TODO: We may want to make this watch function a library function that can be used by other\n./.venv/lib/python3.13/site-packages/chromadb/config.py:    # TODO: Don't use concrete types here to avoid circular deps. Strings are\n./.venv/lib/python3.13/site-packages/chromadb/config.py:    # TODO this is the sysdb port. Should probably rename it.\n./.venv/lib/python3.13/site-packages/chromadb/telemetry/opentelemetry/__init__.py:            # TODO: we may eventually want to make this configurable.\n./.venv/lib/python3.13/site-packages/chromadb/telemetry/product/events.py:# TODO: Re-enable embedding function tracking in create_collection\n./.venv/lib/python3.13/site-packages/chromadb/db/impl/grpc/server.py:    # TODO: investigate generating types for the request and response objects\n./.venv/lib/python3.13/site-packages/chromadb/db/impl/grpc/server.py:                # TODO: IN SysDB SQlite we have technical debt where we\n./.venv/lib/python3.13/site-packages/chromadb/db/impl/grpc/client.py:        # TODO: break out coordinator_port into a separate setting?\n./.venv/lib/python3.13/site-packages/chromadb/db/impl/grpc/client.py:            # TODO: implement limit and offset in the gRPC service\n./.venv/lib/python3.13/site-packages/chromadb/db/mixins/sysdb.py:    # TODO: refactor to remove this dependency into a separate interface\n./.venv/lib/python3.13/site-packages/chromadb/db/mixins/sysdb.py:    # TODO(rohit): Investigate and remove this method completely.\n./.venv/lib/python3.13/site-packages/chromadb/db/mixins/sysdb.py:            # TODO: Update to use better semantics where it's possible to update\n./.venv/lib/python3.13/site-packages/chromadb/db/mixins/sysdb.py:        # TODO(Sanket): Implement this efficiently using a count query.\n./.venv/lib/python3.13/site-packages/chromadb/db/system.py:    # TODO: Investigate and remove this method, as segment creation is done as\n./.venv/lib/python3.13/site-packages/chromadb/proto/convert.py:# TODO: Unit tests for this file, handling optional states etc\n./.venv/lib/python3.13/site-packages/chromadb/proto/convert.py:        # TODO: full error\n./.venv/lib/python3.13/site-packages/onnxruntime/__init__.py:# TODO: thiagofc: Temporary experimental namespace for new PyTorch front-end\n./.venv/lib/python3.13/site-packages/onnxruntime/transformers/onnx_model_phi.py:# TODO: move to a separate file\n./.venv/lib/python3.13/site-packages/onnxruntime/transformers/profile_result_processor.py:            # TODO: shall we have a separated group for nodes with subgraph?\n./.venv/lib/python3.13/site-packages/onnxruntime/transformers/fusion_qordered_layernorm.py:        # TODO: We only support CuBlasLt order ORDER_ROW for now.\n./.venv/lib/python3.13/site-packages/onnxruntime/transformers/quantize_helper.py:        TODO: mix of in-place and return, but results are different\n./.venv/lib/python3.13/site-packages/onnxruntime/transformers/quantize_helper.py:        # TODO: inlcude external data in total model size.\n./.venv/lib/python3.13/site-packages/onnxruntime/transformers/fusion_embedlayer.py:        # TODO: Support roberta (position starts from 2 instead of 0) in EmbedLayerNormalization kernel\n./.venv/lib/python3.13/site-packages/onnxruntime/transformers/fusion_embedlayer.py:        # TODO: use other information (like initializer names) to identify different embedding weights automatically.\n./.venv/lib/python3.13/site-packages/onnxruntime/transformers/fusion_attention_unet.py:                # TODO: Support non-packed QKV\n./.venv/lib/python3.13/site-packages/onnxruntime/transformers/fusion_attention_unet.py:                # TODO: Support non-packed KV\n./.venv/lib/python3.13/site-packages/onnxruntime/transformers/fusion_attention_unet.py:                # TODO: Support non-packed QKV\n./.venv/lib/python3.13/site-packages/onnxruntime/transformers/fusion_attention_unet.py:                # TODO: Support non-packed QKV\n./.venv/lib/python3.13/site-packages/onnxruntime/transformers/fusion_skiplayernorm.py:                # TODO(tianleiwu): support subgraph in shape inference or add broadcasting in SkipLayerNormalization op.\n./.venv/lib/python3.13/site-packages/onnxruntime/transformers/fusion_skiplayernorm.py:                # TODO(tianleiwu): support broadcasting Skip shape (1, sequence_length, hidden_size) or (sequence_length, hidden_size)\n./.venv/lib/python3.13/site-packages/onnxruntime/transformers/convert_generation.py:        # TODO(tianleiwu): Use auto mixed precision for fp16 conversion: arguments.append('--auto_mixed_precision')\n./.venv/lib/python3.13/site-packages/onnxruntime/transformers/convert_generation.py:    # TODO(hasesh): Handle cases where the fp16 data is stored in the\n./.venv/lib/python3.13/site-packages/onnxruntime/transformers/convert_generation.py:    # TODO(tianleiwu): verify shapes of inputs and outputs.\n./.venv/lib/python3.13/site-packages/onnxruntime/transformers/convert_generation.py:    # TODO: encoder_hidden_states is optional\n./.venv/lib/python3.13/site-packages/onnxruntime/transformers/convert_generation.py:    # TODO(hasesh): Are there more permutations to try before returning ?\n./.venv/lib/python3.13/site-packages/onnxruntime/transformers/convert_generation.py:    # TODO(hasesh): Are there more permutations to try before returning ?\n./.venv/lib/python3.13/site-packages/onnxruntime/transformers/convert_generation.py:            # TODO(tianleiwu): investigate the following which causes error in inference\n./.venv/lib/python3.13/site-packages/onnxruntime/transformers/convert_generation.py:    # TODO(tianleiwu): move shared initializers from T5 encoder and decoder subgraphs to parent graph to save memory.\n./.venv/lib/python3.13/site-packages/onnxruntime/transformers/fusion_qordered_attention.py:            # TODO: Investigate why FusionAttention doesn't have such logic\n./.venv/lib/python3.13/site-packages/onnxruntime/transformers/fusion_qordered_attention.py:            # TODO: Make this fusion logic EP-agnostic ?\n./.venv/lib/python3.13/site-packages/onnxruntime/transformers/onnx_model_bert_keras.py:                # TODO: shall we add back the checking of children op types.\n./.venv/lib/python3.13/site-packages/onnxruntime/transformers/fusion_gpt_attention_no_past.py:        # TODO: detect num_heads from graph like FusionAttention\n./.venv/lib/python3.13/site-packages/onnxruntime/transformers/fusion_gpt_attention_no_past.py:        # (TODO) hasesh/tlwu: Investigate what fixes the following logic needs in order\n./.venv/lib/python3.13/site-packages/onnxruntime/transformers/fusion_qordered_matmul.py:        # TODO: Make this fusion logic EP-agnostic ?\n./.venv/lib/python3.13/site-packages/onnxruntime/transformers/fusion_gpt_attention.py:                # TODO: match mask path for GPT2LMHeadModel_BeamSearchStep.\n./.venv/lib/python3.13/site-packages/onnxruntime/transformers/fusion_qordered_gelu.py:        # TODO: We only support CuBlasLt order ORDER_ROW for now.\n./.venv/lib/python3.13/site-packages/onnxruntime/transformers/onnx_model.py:                    # TODO: match recursively if we encounter cases that the correct child is not the first matched.\n./.venv/lib/python3.13/site-packages/onnxruntime/transformers/onnx_model.py:                # TODO: support other ops such as `BeamSearch` that have subgraphs as op attributes\n./.venv/lib/python3.13/site-packages/onnxruntime/transformers/onnx_model.py:                # TODO: This for-loop logic currently assumes that Loop/Scan/If nodes will not be\n./.venv/lib/python3.13/site-packages/onnxruntime/transformers/onnx_model.py:        # TODO: support graph_topological_sort() in subgraphs\n./.venv/lib/python3.13/site-packages/onnxruntime/transformers/onnx_model_bert.py:            # TODO: merge ConstantOfShape -->Cast to ConstantOfShape (need update the data type of value)\n./.venv/lib/python3.13/site-packages/onnxruntime/transformers/fusion_mha_mmdit.py:        # TODO: MHA op support BNSH format to reduce the effort in fusion.\n./.venv/lib/python3.13/site-packages/onnxruntime/transformers/onnx_model_t5.py:        # TODO: Optimization opportunity: change RelativePositionBias op to support query_length != key_length.\n./.venv/lib/python3.13/site-packages/onnxruntime/transformers/onnx_model_t5.py:                # TODO: more validation on node attributes\n./.venv/lib/python3.13/site-packages/onnxruntime/transformers/fusion_layernorm.py:        # TODO: add epsilon attribute\n./.venv/lib/python3.13/site-packages/onnxruntime/transformers/benchmark_helper.py:    # TODO: Add more.\n./.venv/lib/python3.13/site-packages/onnxruntime/transformers/fusion_reshape.py:        # TODO(tlwu): Subgraph blocks pruning un-used nodes. Add code to remove un-used nodes safely.\n./.venv/lib/python3.13/site-packages/onnxruntime/transformers/models/phi2/convert_to_onnx.py:            )  # TODO: temp setting for paged attention\n./.venv/lib/python3.13/site-packages/onnxruntime/transformers/models/gpt2/benchmark_gpt2.py:    use_external_data_format = config.n_layer > 24  # TODO: find a way to check model size > 2GB\n./.venv/lib/python3.13/site-packages/onnxruntime/transformers/models/gpt2/gpt2_parity.py:                    )  # TODO: shall we use one-sided: less or greater according to \"top1_match_rate\"\n./.venv/lib/python3.13/site-packages/onnxruntime/transformers/models/stable_diffusion/diffusion_models.py:        use_vae=True,  # TODO: this has couple with output type of pipeline\n./.venv/lib/python3.13/site-packages/onnxruntime/transformers/models/stable_diffusion/diffusion_models.py:        # TODO: can we read from config instead\n./.venv/lib/python3.13/site-packages/onnxruntime/transformers/models/stable_diffusion/engine_builder_ort_cuda.py:            onnx_opset_version=onnx_opset_version,  # TODO: ArgMax-12 is not implemented in CUDA\n./.venv/lib/python3.13/site-packages/onnxruntime/transformers/models/stable_diffusion/engine_builder_ort_cuda.py:            use_cuda_graph=False,  # TODO: fix Runtime Error with cuda graph\n./.venv/lib/python3.13/site-packages/onnxruntime/transformers/models/stable_diffusion/engine_builder.py:        # TODO(tianleiwu): save custom model to a directory named by its original model.\n./.venv/lib/python3.13/site-packages/onnxruntime/transformers/models/stable_diffusion/engine_builder.py:        # TODO: When we support original VAE, we shall save custom VAE to another directory.\n./.venv/lib/python3.13/site-packages/onnxruntime/transformers/models/longformer/convert_to_onnx.py:# update this script with correct name of longformer_attention.cpython-*.so (search TODO below).\n./.venv/lib/python3.13/site-packages/onnxruntime/transformers/models/longformer/convert_to_onnx.py:# TODO: search the directory to find correct output filename of \"python setup.py install\" when python version is not 3.8\n./.venv/lib/python3.13/site-packages/onnxruntime/transformers/models/longformer/convert_to_onnx.py:    # TODO: The filtering value may be -10000.0 or -inf. Check the huggingface implementation.\n./.venv/lib/python3.13/site-packages/onnxruntime/transformers/models/longformer/convert_to_onnx.py:    # TODO: add postprocessing of ONNX model to calculate based on graph input: input_mask = (attention_mask - 1) * 10000.0\n./.venv/lib/python3.13/site-packages/onnxruntime/transformers/models/longformer/convert_to_onnx.py:    # TODO: add postprocessing of ONNX model to use graph input directly: global_mask = global_attention_mask\n./.venv/lib/python3.13/site-packages/onnxruntime/transformers/models/longformer/benchmark_longformer.py:                    \"engine\": \"torch\",  # TODO: test torchscript\n./.venv/lib/python3.13/site-packages/onnxruntime/transformers/models/sam2/sam2_demo.py:    # TODO: support batched prompt inputs\n./.venv/lib/python3.13/site-packages/onnxruntime/transformers/models/sam2/image_encoder.py:        # TODO: we should avoid this transpose since it will be transposed back to NCHW later.\n./.venv/lib/python3.13/site-packages/onnxruntime/tools/mobile_helpers/usability_checker.py:            # TODO: CoreML has a flag. NNAPI doesn't. Either should be able to support a subgraph when treated as a\n./.venv/lib/python3.13/site-packages/onnxruntime/tools/onnx_randomizer.py:        # TODO: need to find a better way to differentiate shape data and weights.\n./.venv/lib/python3.13/site-packages/onnxruntime/tools/pytorch_export_contrib_ops.py:    # TODO(justinchuby): Create a function to alert users when torch is not installed\n./.venv/lib/python3.13/site-packages/onnxruntime/tools/ort_format_model/types.py:            # TODO: Decide if we need to wrap the type in a std::vector. Issue is that the element type is internal\n./.venv/lib/python3.13/site-packages/onnxruntime/tools/ort_format_model/operator_type_usage_processors.py:    # TODO - review and add ML ops as needed\n./.venv/lib/python3.13/site-packages/onnxruntime/tools/ort_format_model/operator_type_usage_processors.py:    # TODO: Only apply required types to the global type list and ignore if it's model based per-op type reduction\n./.venv/lib/python3.13/site-packages/onnxruntime/tools/ort_format_model/operator_type_usage_processors.py:    TODO: Currently the type tracking is not specific to a version of the operator.\n./.venv/lib/python3.13/site-packages/onnxruntime/tools/symbolic_shape_infer.py:                assert not is_sequence(vi.type)  # TODO: handle loop accumulation in sequence_type\n./.venv/lib/python3.13/site-packages/onnxruntime/tools/symbolic_shape_infer.py:                    # TODO: Is overload_name needed?\n./.venv/lib/python3.13/site-packages/onnxruntime/quantization/qdq_quantizer.py:        TODO: Explain the reasoning for using this formula.\n./.venv/lib/python3.13/site-packages/onnxruntime/quantization/qdq_quantizer.py:            # TODO: Add support for dedicated_qdq_pair if/when needed.\n./.venv/lib/python3.13/site-packages/onnxruntime/quantization/static_quantize_runner.py:    # TODO: --calib_strided_minmax\"\n./.venv/lib/python3.13/site-packages/onnxruntime/quantization/static_quantize_runner.py:    # TODO: --calib_moving_average_constant\"\n./.venv/lib/python3.13/site-packages/onnxruntime/quantization/static_quantize_runner.py:    # TODO: --calib_max_intermediate_outputs\"\n./.venv/lib/python3.13/site-packages/onnxruntime/quantization/static_quantize_runner.py:    # TODO: Add arguments related to Smooth Quant\n./.venv/lib/python3.13/site-packages/onnxruntime/quantization/static_quantize_runner.py:    # TODO: Enhance the function to handle more real cases of json file\n./.venv/lib/python3.13/site-packages/onnxruntime/quantization/execution_providers/qnn/quant_config.py:            # TODO: Make bias input not per-channel. QNN needs it to be per-tensor, but quantizer\n./.venv/lib/python3.13/site-packages/onnxruntime/quantization/execution_providers/qnn/fusion_lpnorm.py:        clip_max = self.get_node_attribute(clip_node, \"max\")  # TODO: clip_max could be FLOAT_MAX\n./.venv/lib/python3.13/site-packages/onnxruntime/quantization/onnx_model.py:    # TODO:use OnnxModel.graph_topological_sort(self.model.graph) from transformers.onnx_model\n./.venv/lib/python3.13/site-packages/onnxruntime/quantization/calibrate.py:        # TODO: Currently we have different collect() for entropy and percentile method respectively.\n./.venv/lib/python3.13/site-packages/onnxruntime/quantization/operators/attention.py:        # TODO This is a temporary fix to stop exporting QAttention with qkv_hidden_sizes\n./.venv/lib/python3.13/site-packages/onnxruntime/quantization/operators/pad.py:                    # TODO: check quantize_inputs after sub graph is supported\n./.venv/lib/python3.13/site-packages/onnxruntime/quantization/operators/conv.py:            to=onnx_type,  # TODO: FLOAT ot FLOAT16\n./.venv/lib/python3.13/site-packages/onnxruntime/quantization/base_quantizer.py:            # TODO: This formula should be explained including why the scale is not estimated for the bias as well.\n./.venv/lib/python3.13/site-packages/onnxruntime/quantization/base_quantizer.py:        # TODO(adrianlizarraga): This function is now only used by onnx_quantizer.py, so move it there.\n./.venv/lib/python3.13/site-packages/onnxruntime/quantization/base_quantizer.py:        # TODO(adrianlizarraga): This function is now only used by onnx_quantizer.py, so move it there.\n./.venv/lib/python3.13/site-packages/onnxruntime/capi/_pybind_state.py:    # TODO, we may want to try to load the VC Runtime dlls instead of checking if the hardcoded file path\n./.venv/lib/python3.13/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:            self._model_bytes = path_or_bytes  # TODO: This is bad as we're holding the memory indefinitely\n./.venv/lib/python3.13/site-packages/onnxruntime/backend/backend.py:                    # is generating attribute error. TODO investigate the pipelines to\n./.venv/lib/python3.13/site-packages/sympy/solvers/diophantine/diophantine.py:    # TODO: pre-simplification: Not necessary but may simplify\n./.venv/lib/python3.13/site-packages/sympy/solvers/diophantine/diophantine.py:        # TODO: Fall back to diop_DN when k = 2\n./.venv/lib/python3.13/site-packages/sympy/solvers/ode/ode.py:    # TODO: Use solveset here\n./.venv/lib/python3.13/site-packages/sympy/solvers/ode/ode.py:            # TODO: Hint first order series should match only if d/e is analytic.\n./.venv/lib/python3.13/site-packages/sympy/solvers/ode/ode.py:    # TODO: if two solutions are solved for f(x), we still want to be\n./.venv/lib/python3.13/site-packages/sympy/solvers/ode/single.py:    # TODO: Add methods that can be used by many ODE solvers:\n./.venv/lib/python3.13/site-packages/sympy/solvers/solveset.py:        # TODO : We should not blindly recurse through all args of arbitrary expressions like this\n./.venv/lib/python3.13/site-packages/sympy/solvers/solveset.py:        # TODO Case: A-> function of symbol, can be extended here\n./.venv/lib/python3.13/site-packages/sympy/solvers/solvers.py:    # TODO: improve solution testing\n./.venv/lib/python3.13/site-packages/sympy/solvers/solvers.py:# TODO: option for calculating J numerically\n./.venv/lib/python3.13/site-packages/sympy/solvers/bivariate.py:    # TODO it would be good to pick the smallest divisible power\n./.venv/lib/python3.13/site-packages/sympy/solvers/tests/test_solveset.py:    # TODO: Is the above solution set definitely complete?\n./.venv/lib/python3.13/site-packages/sympy/solvers/tests/test_solveset.py:    # TODO: add more simple testcases when solveset returns\n./.venv/lib/python3.13/site-packages/sympy/solvers/tests/test_solvers.py:    # TODO: Investigate why currently solution [0] is preferred over [1].\n./.venv/lib/python3.13/site-packages/sympy/solvers/tests/test_polysys.py:    # TODO: does this really have to be so complicated?!\n./.venv/lib/python3.13/site-packages/sympy/solvers/pde.py:        # TODO : 'best' hint should be implemented when adequate\n./.venv/lib/python3.13/site-packages/sympy/solvers/pde.py:    # TODO : For now pde.py uses support offered by the ode_order function\n./.venv/lib/python3.13/site-packages/sympy/solvers/pde.py:    # TODO : For now homogeneous first order linear PDE's having\n./.venv/lib/python3.13/site-packages/sympy/solvers/pde.py:    # TODO : For now homogeneous first order linear PDE's having\n./.venv/lib/python3.13/site-packages/sympy/matrices/expressions/tests/test_derivatives.py:    # TODO: this is commented because it slows down the tests.\n./.venv/lib/python3.13/site-packages/sympy/matrices/expressions/tests/test_derivatives.py:    # TODO: find a way to represent a four-dimensional zero-array:\n./.venv/lib/python3.13/site-packages/sympy/matrices/expressions/tests/test_derivatives.py:    # TODO: TensorProduct is not supported\n./.venv/lib/python3.13/site-packages/sympy/matrices/expressions/tests/test_derivatives.py:    # TODO: no support for TensorProduct.\n./.venv/lib/python3.13/site-packages/sympy/matrices/expressions/tests/test_derivatives.py:    # TODO: restore this result (currently returning the transpose):\n./.venv/lib/python3.13/site-packages/sympy/matrices/expressions/tests/test_derivatives.py:    # TODO: restore (currently returning the transpose):\n./.venv/lib/python3.13/site-packages/sympy/matrices/expressions/tests/test_derivatives.py:    # TODO: not implemented\n./.venv/lib/python3.13/site-packages/sympy/matrices/expressions/tests/test_derivatives.py:    # TODO: wrong\n./.venv/lib/python3.13/site-packages/sympy/matrices/expressions/tests/test_derivatives.py:    # TODO: wrong\n./.venv/lib/python3.13/site-packages/sympy/matrices/expressions/hadamard.py:# TODO Implement algorithm for rewriting Hadamard product as diagonal matrix\n./.venv/lib/python3.13/site-packages/sympy/matrices/matrixbase.py:        TODO: Implement algorithm for sparse matrices (SFF),\n./.venv/lib/python3.13/site-packages/sympy/matrices/tests/test_commonmatrix.py:    # TODO: currently not working as ``_MinimalMatrix`` cannot be sympified:\n./.venv/lib/python3.13/site-packages/sympy/matrices/matrices.py:        TODO: Implement algorithm for sparse matrices (SFF),\n./.venv/lib/python3.13/site-packages/sympy/matrices/determinant.py:    TODO: Implement algorithm for sparse matrices (SFF),\n./.venv/lib/python3.13/site-packages/sympy/categories/diagram_drawing.py:                # prop is a Symbol.  TODO: Find out why.\n./.venv/lib/python3.13/site-packages/sympy/functions/elementary/_trigonometric_special.py:TODO\n./.venv/lib/python3.13/site-packages/sympy/functions/elementary/piecewise.py:                # TODO simplify hi <= upto\n./.venv/lib/python3.13/site-packages/sympy/functions/elementary/exponential.py:        # TODO new and probably slow\n./.venv/lib/python3.13/site-packages/sympy/functions/elementary/hyperbolic.py:        if arg.is_Add: # TODO, implement more if deep stuff here\n./.venv/lib/python3.13/site-packages/sympy/functions/elementary/hyperbolic.py:        if arg.is_Add: # TODO, implement more if deep stuff here\n./.venv/lib/python3.13/site-packages/sympy/functions/elementary/tests/test_complexes.py:    # TODO XXX why does abs(x)._eval_evalf() not fall back to global evalf?\n./.venv/lib/python3.13/site-packages/sympy/functions/elementary/tests/test_piecewise.py:    # TODO raise error if function is discontinuous at limit of\n./.venv/lib/python3.13/site-packages/sympy/functions/elementary/trigonometric.py:        if arg.is_Add:  # TODO, implement more if deep stuff here\n./.venv/lib/python3.13/site-packages/sympy/functions/elementary/trigonometric.py:            # TODO: Do this more efficiently for more than two terms\n./.venv/lib/python3.13/site-packages/sympy/functions/elementary/trigonometric.py:        if arg.is_Add:  # TODO: Do this more efficiently for more than two terms\n./.venv/lib/python3.13/site-packages/sympy/functions/elementary/trigonometric.py:    # TODO refactor into TrigonometricFunction common parts of\n./.venv/lib/python3.13/site-packages/sympy/functions/combinatorial/factorials.py:        # TODO: extend this to complex numbers?\n./.venv/lib/python3.13/site-packages/sympy/functions/combinatorial/numbers.py:    # TODO: make this a class like bell()\n./.venv/lib/python3.13/site-packages/sympy/functions/special/delta_functions.py:            # TODO\n./.venv/lib/python3.13/site-packages/sympy/functions/special/hyper.py:# TODO should __new__ accept **options?\n./.venv/lib/python3.13/site-packages/sympy/functions/special/hyper.py:# TODO should constructors should check if parameters are sensible?\n./.venv/lib/python3.13/site-packages/sympy/functions/special/hyper.py:        # TODO should we check convergence conditions?\n./.venv/lib/python3.13/site-packages/sympy/functions/special/hyper.py:        # TODO should we check convergence conditions?\n./.venv/lib/python3.13/site-packages/sympy/functions/special/hyper.py:    # TODO this can be nicer\n./.venv/lib/python3.13/site-packages/sympy/functions/special/gamma_functions.py:                # TODO n == 1 also can do some rational z\n./.venv/lib/python3.13/site-packages/sympy/functions/special/zeta_functions.py:            # TODO should something be polarified here?\n./.venv/lib/python3.13/site-packages/sympy/functions/special/zeta_functions.py:        # TODO use minpoly instead of ad-hoc methods when issue 5888 is fixed\n./.venv/lib/python3.13/site-packages/sympy/functions/special/zeta_functions.py:            # TODO reference?\n./.venv/lib/python3.13/site-packages/sympy/functions/special/error_functions.py:# TODO series expansions\n./.venv/lib/python3.13/site-packages/sympy/functions/special/error_functions.py:# TODO see the \"Note:\" in Ei\n./.venv/lib/python3.13/site-packages/sympy/functions/special/error_functions.py:        # TODO:\n./.venv/lib/python3.13/site-packages/sympy/functions/special/error_functions.py:            # TODO: is the series really correct?\n./.venv/lib/python3.13/site-packages/sympy/functions/special/bessel.py:# TODO\n./.venv/lib/python3.13/site-packages/sympy/functions/special/spherical_harmonics.py:        # TODO Add more simplififcation here\n./.venv/lib/python3.13/site-packages/sympy/functions/special/spherical_harmonics.py:        # TODO: Make sure n \\in N\n./.venv/lib/python3.13/site-packages/sympy/functions/special/spherical_harmonics.py:        # TODO: Assert |m| <= n ortherwise we should return 0\n./.venv/lib/python3.13/site-packages/sympy/functions/special/spherical_harmonics.py:        # TODO: Make sure n \\in N\n./.venv/lib/python3.13/site-packages/sympy/functions/special/spherical_harmonics.py:        # TODO: Assert |m| <= n ortherwise we should return 0\n./.venv/lib/python3.13/site-packages/sympy/functions/special/spherical_harmonics.py:        # TODO: Make sure theta \\in R and phi \\in R\n./.venv/lib/python3.13/site-packages/sympy/functions/special/spherical_harmonics.py:        # TODO: Handle deep and hints\n./.venv/lib/python3.13/site-packages/sympy/holonomic/holonomic.py:                    # TODO: Implement this case\n./.venv/lib/python3.13/site-packages/sympy/holonomic/holonomic.py:                # TODO: support for singular initial condition\n./.venv/lib/python3.13/site-packages/sympy/polys/matrices/normalforms.py:# TODO (future work):\n./.venv/lib/python3.13/site-packages/sympy/polys/matrices/rref.py:            # TODO: Add partial pivot support to the sparse implementations.\n./.venv/lib/python3.13/site-packages/sympy/polys/matrices/dense.py:    # TODO: Use a nontrivial pivoting strategy to control intermediate\n./.venv/lib/python3.13/site-packages/sympy/polys/matrices/dense.py:    # TODO: Use a non-trivial pivoting strategy. Even just row swapping makes a\n./.venv/lib/python3.13/site-packages/sympy/polys/matrices/_dfm.py:# TODO:\n./.venv/lib/python3.13/site-packages/sympy/polys/matrices/_dfm.py:        # TODO: Implement similar algorithms for DDM and SDM.\n./.venv/lib/python3.13/site-packages/sympy/polys/modulargcd.py:    # TODO: to improve performance, choose the main variable here\n./.venv/lib/python3.13/site-packages/sympy/polys/modulargcd.py:# TODO: add support for algebraic function fields\n./.venv/lib/python3.13/site-packages/sympy/polys/heuristicgcd.py:    # TODO: don't expose poly repr implementation details\n./.venv/lib/python3.13/site-packages/sympy/polys/domains/quotientring.py:# TODO\n./.venv/lib/python3.13/site-packages/sympy/polys/domains/quotientring.py:        # TODO optionally disable reduction?\n./.venv/lib/python3.13/site-packages/sympy/polys/domains/polynomialring.py:        # TODO: remove this\n./.venv/lib/python3.13/site-packages/sympy/polys/domains/domain.py:        # TODO: implement this in from_ methods\n./.venv/lib/python3.13/site-packages/sympy/polys/domains/domain.py:        else: # TODO: remove this branch\n./.venv/lib/python3.13/site-packages/sympy/polys/domains/fractionfield.py:        # TODO: remove this\n./.venv/lib/python3.13/site-packages/sympy/polys/ring_series.py:    # TODO Use _parallel_dict_from_expr instead of sring as sring is\n./.venv/lib/python3.13/site-packages/sympy/polys/distributedmodules.py:        # TODO better data structure!!!\n./.venv/lib/python3.13/site-packages/sympy/polys/distributedmodules.py:        # TODO apply the product criterion?\n./.venv/lib/python3.13/site-packages/sympy/polys/distributedmodules.py:        # TODO mergesort?\n./.venv/lib/python3.13/site-packages/sympy/polys/distributedmodules.py:    # (TODO again, better data structures)\n./.venv/lib/python3.13/site-packages/sympy/polys/tests/test_distributedmodules.py:# TODO test to_dict?\n./.venv/lib/python3.13/site-packages/sympy/polys/tests/test_heuristicgcd.py:    # TODO: assert heugcd(f, f.diff(x))[0] == g\n./.venv/lib/python3.13/site-packages/sympy/polys/polyutils.py:        # TODO: Integrate this into expand() itself\n./.venv/lib/python3.13/site-packages/sympy/polys/polyroots.py:    # TODO: This is fragile. Figure out how to make this independent of construct_domain().\n./.venv/lib/python3.13/site-packages/sympy/polys/rings.py:    # TODO: rewrite this so that it doesn't use expand() (see poly()).\n./.venv/lib/python3.13/site-packages/sympy/polys/rings.py:        # TODO: should AlgebraicField be a Composite domain?\n./.venv/lib/python3.13/site-packages/sympy/polys/rings.py:        elif len(self) <= 5: # TODO: use an actual density measure\n./.venv/lib/python3.13/site-packages/sympy/polys/rings.py:        else: # TODO: don't use dense representation (port PRS algorithms)\n./.venv/lib/python3.13/site-packages/sympy/polys/rings.py:    # TODO: following methods should point to polynomial\n./.venv/lib/python3.13/site-packages/sympy/polys/numberfields/modules.py:        # TODO:\n./.venv/lib/python3.13/site-packages/sympy/polys/numberfields/primes.py:    # TODO (future work):\n./.venv/lib/python3.13/site-packages/sympy/polys/numberfields/basis.py:        # TODO:\n./.venv/lib/python3.13/site-packages/sympy/polys/agca/modules.py:# TODO\n./.venv/lib/python3.13/site-packages/sympy/polys/agca/modules.py:        # TODO this can be done more efficiently\n./.venv/lib/python3.13/site-packages/sympy/polys/agca/ideals.py:    # TODO more\n./.venv/lib/python3.13/site-packages/sympy/calculus/util.py:    # TODO: handle piecewise defined functions\n./.venv/lib/python3.13/site-packages/sympy/calculus/util.py:    # TODO: handle transcendental functions\n./.venv/lib/python3.13/site-packages/sympy/calculus/util.py:    # TODO: handle multivariate functions\n./.venv/lib/python3.13/site-packages/sympy/calculus/accumulationbounds.py:        # TODO : Devise a better method for Union of AccumBounds\n./.venv/lib/python3.13/site-packages/sympy/geometry/ellipse.py:        # TODO: Replace solve with nonlinsolve, when nonlinsolve will be able to solve in real domain\n./.venv/lib/python3.13/site-packages/sympy/geometry/ellipse.py:        # TODO: Replace solve with solveset, when this line is tested\n./.venv/lib/python3.13/site-packages/sympy/geometry/ellipse.py:                # TODO: Replace solve with solveset, when these lines are tested\n./.venv/lib/python3.13/site-packages/sympy/geometry/ellipse.py:            # TODO: Replace solve with solveset, when this line is tested\n./.venv/lib/python3.13/site-packages/sympy/geometry/plane.py:                # TODO: Replace solve with solveset, when this line is tested\n./.venv/lib/python3.13/site-packages/sympy/core/exprtools.py:            # XXX TODO there should be a way to inspect what order the terms\n./.venv/lib/python3.13/site-packages/sympy/core/mul.py:            # TODO: Make non-commutative exponents not combine automatically\n./.venv/lib/python3.13/site-packages/sympy/core/mul.py:        # TODO: Should these be self.func?\n./.venv/lib/python3.13/site-packages/sympy/core/mul.py:        # TODO: Should this be self.func?\n./.venv/lib/python3.13/site-packages/sympy/core/facts.py:       TODO: write about\n./.venv/lib/python3.13/site-packages/sympy/core/facts.py:        \"\"\"process a -> b rule\"\"\"   # TODO write more?\n./.venv/lib/python3.13/site-packages/sympy/core/facts.py:       # TODO b | c\n./.venv/lib/python3.13/site-packages/sympy/core/symbol.py:    # TODO add check against another Wild\n./.venv/lib/python3.13/site-packages/sympy/core/numbers.py:# TODO: we should use the warnings module\n./.venv/lib/python3.13/site-packages/sympy/core/numbers.py:# TODO caching with decorator, but not to degrade performance\n./.venv/lib/python3.13/site-packages/sympy/core/numbers.py:                #TODO: this can probably be optimized more\n./.venv/lib/python3.13/site-packages/sympy/core/numbers.py:    # TODO make it decorator + bytecodehacks?\n./.venv/lib/python3.13/site-packages/sympy/core/tests/test_args.py:@SKIP(\"TODO: sympy.physics.quantum.shor: Cmod Not Implemented\")\n./.venv/lib/python3.13/site-packages/sympy/core/tests/test_function.py:    # TODO: Disable string inputs (https://github.com/sympy/sympy/issues/11003)\n./.venv/lib/python3.13/site-packages/sympy/core/tests/test_expr.py:    # TODO UndefinedFunction does not subclass Expr\n./.venv/lib/python3.13/site-packages/sympy/core/tests/test_facts.py:# TODO move me to appropriate place\n./.venv/lib/python3.13/site-packages/sympy/core/tests/test_basic.py:    # TODO UndefinedFunction does not subclass Expr\n./.venv/lib/python3.13/site-packages/sympy/core/tests/test_diff.py:    # TODO: assert diff(x**2, (x, n)) == x**(2-n)*ff(2, n)\n./.venv/lib/python3.13/site-packages/sympy/core/tests/test_assumptions.py:    # TODO Change to x.is_nonzero is None\n./.venv/lib/python3.13/site-packages/sympy/core/function.py:        # TODO: Look at nargs\n./.venv/lib/python3.13/site-packages/sympy/core/function.py:            #TODO: check if assumption of discontinuous derivatives exist\n./.venv/lib/python3.13/site-packages/sympy/core/function.py:        # TODO: deprecate?  YES, make this 'enumerated_variables' and\n./.venv/lib/python3.13/site-packages/sympy/core/function.py:        # TODO: support for `d^n`?\n./.venv/lib/python3.13/site-packages/sympy/core/expr.py:        # TODO: Smarter heuristics\n./.venv/lib/python3.13/site-packages/sympy/core/add.py:                seq.extend(o_args)  # TODO zerocopy?\n./.venv/lib/python3.13/site-packages/sympy/physics/units/dimensions.py:                # TODO: should this raise a warning?\n./.venv/lib/python3.13/site-packages/sympy/physics/units/dimensions.py:        #TODO: the inversion will fail if the system is inconsistent, for\n./.venv/lib/python3.13/site-packages/sympy/physics/units/tests/test_quantities.py:    # TODO: decide whether to allow such expression in the future\n./.venv/lib/python3.13/site-packages/sympy/physics/units/tests/test_quantities.py:    # TODO: Pow only support structural equality:\n./.venv/lib/python3.13/site-packages/sympy/physics/units/tests/test_quantities.py:    # TODO: need better simplification routine:\n./.venv/lib/python3.13/site-packages/sympy/physics/units/tests/test_quantities.py:    # TODO: need a better way to simplify expressions containing units:\n./.venv/lib/python3.13/site-packages/sympy/physics/units/tests/test_quantities.py:    # TODO: fix this, it should give `m` without `Abs`\n./.venv/lib/python3.13/site-packages/sympy/physics/mechanics/kane.py:    # TODO : Remove `new_method` after 1.1 has been released.\n./.venv/lib/python3.13/site-packages/sympy/physics/mechanics/tests/test_particle.py:    # TODO make the result not be system-dependent\n./.venv/lib/python3.13/site-packages/sympy/physics/optics/gaussopt.py:    #TODO A class Complex may be implemented. The BeamParameter may\n./.venv/lib/python3.13/site-packages/sympy/physics/optics/gaussopt.py:    #TODO add the other possible arguments\n./.venv/lib/python3.13/site-packages/sympy/physics/optics/gaussopt.py:#TODO\n./.venv/lib/python3.13/site-packages/sympy/physics/optics/gaussopt.py:#TODO\n./.venv/lib/python3.13/site-packages/sympy/physics/quantum/trace.py:    TODO: Handle condition such as symbols have subscripts/superscripts\n./.venv/lib/python3.13/site-packages/sympy/physics/quantum/trace.py:    # TODO: Need to handle printing\n./.venv/lib/python3.13/site-packages/sympy/physics/quantum/trace.py:        #TODO: Current version ignores the indices set for partial trace.\n./.venv/lib/python3.13/site-packages/sympy/physics/quantum/trace.py:        # TODO : improve this implementation\n./.venv/lib/python3.13/site-packages/sympy/physics/quantum/trace.py:    #TODO: Review if the permute method is needed\n./.venv/lib/python3.13/site-packages/sympy/physics/quantum/shor.py:    TODO: implement a decompose property that returns how to do this in terms\n./.venv/lib/python3.13/site-packages/sympy/physics/quantum/gate.py:            # TODO: This can be optimized to reduce the number of Qubit\n./.venv/lib/python3.13/site-packages/sympy/physics/quantum/represent.py:TODO:\n./.venv/lib/python3.13/site-packages/sympy/physics/quantum/represent.py:            #TODO: Add support for sets of operators\n./.venv/lib/python3.13/site-packages/sympy/physics/quantum/represent.py:    TODO (?): Support for Muls and other types of expressions?\n./.venv/lib/python3.13/site-packages/sympy/physics/quantum/qapply.py:    # TODO: don't expand the scalars in front of each Mul.\n./.venv/lib/python3.13/site-packages/sympy/physics/quantum/qapply.py:    # TODO: I may need to expand before returning the final result.\n./.venv/lib/python3.13/site-packages/sympy/physics/quantum/cartesian.py:TODO:\n./.venv/lib/python3.13/site-packages/sympy/physics/quantum/cg.py:#TODO:\n./.venv/lib/python3.13/site-packages/sympy/physics/quantum/cg.py:    #TODO: Improve simplification method\n./.venv/lib/python3.13/site-packages/sympy/physics/quantum/cg.py:    # TODO: Check for symmetries\n./.venv/lib/python3.13/site-packages/sympy/physics/quantum/tensorproduct.py:        # TODO: disallow nested TensorProducts.\n./.venv/lib/python3.13/site-packages/sympy/physics/quantum/matrixcache.py:        # TODO: explore different sparse formats. But sparse.kron will use\n./.venv/lib/python3.13/site-packages/sympy/physics/quantum/matrixutils.py:# TODO: Move this into sympy.matrices.\n./.venv/lib/python3.13/site-packages/sympy/physics/quantum/tests/test_trace.py:    #TODO: needed while testing reduced density operations, etc.\n./.venv/lib/python3.13/site-packages/sympy/physics/quantum/tests/test_density.py:    #TODO: test for invalid arguments\n./.venv/lib/python3.13/site-packages/sympy/physics/quantum/tests/test_cartesian.py:    # TODO: Add tests for representations\n./.venv/lib/python3.13/site-packages/sympy/physics/quantum/tests/test_printing.py:TODO:\n./.venv/lib/python3.13/site-packages/sympy/physics/quantum/tests/test_printing.py:    # TODO: Fix non-unicode pretty printing\n./.venv/lib/python3.13/site-packages/sympy/physics/quantum/operator.py:TODO:\n./.venv/lib/python3.13/site-packages/sympy/physics/quantum/operator.py:            # TODO: make sure the hilbert spaces of the bra and ket are\n./.venv/lib/python3.13/site-packages/sympy/physics/quantum/operator.py:        # TODO if operands are tensorproducts this may be will be handled\n./.venv/lib/python3.13/site-packages/sympy/physics/quantum/operatorset.py:TODO List:\n./.venv/lib/python3.13/site-packages/sympy/physics/quantum/spin.py:        # TODO: add methods for uncoupling operators\n./.venv/lib/python3.13/site-packages/sympy/physics/quantum/spin.py:    # TODO: move this to qapply_Mul\n./.venv/lib/python3.13/site-packages/sympy/physics/quantum/spin.py:        #TODO: use options to use different j values\n./.venv/lib/python3.13/site-packages/sympy/physics/quantum/spin.py:        # TODO: move evaluation up to represent function/implement elsewhere\n./.venv/lib/python3.13/site-packages/sympy/physics/quantum/spin.py:            # TODO: better way to get angles of rotation\n./.venv/lib/python3.13/site-packages/sympy/physics/quantum/spin.py:            # TODO: Need hilbert space fix, see issue 5732\n./.venv/lib/python3.13/site-packages/sympy/physics/quantum/density.py:    TODO: Density operator support for Qubits\n./.venv/lib/python3.13/site-packages/sympy/physics/vector/vector.py:        # TODO : Circular dependency if imported at top. Should move\n./.venv/lib/python3.13/site-packages/sympy/physics/vector/tests/test_printing.py:    # TODO : The unit vectors should print with subscripts but they just\n./.venv/lib/python3.13/site-packages/sympy/physics/vector/tests/test_printing.py:    # TODO : The pretty print division does not print correctly here:\n./.venv/lib/python3.13/site-packages/sympy/sets/setexpr.py:        # TODO: this could be implemented straight into `imageset`:\n./.venv/lib/python3.13/site-packages/sympy/sets/handlers/functions.py:    # TODO: handle functions with infinitely many solutions (eg, sin, tan)\n./.venv/lib/python3.13/site-packages/sympy/sets/handlers/functions.py:    # TODO: handle multivariate functions\n./.venv/lib/python3.13/site-packages/sympy/sets/handlers/power.py:        # TODO: handle unevaluated condition.\n./.venv/lib/python3.13/site-packages/sympy/sets/handlers/power.py:        # TODO: `s2 > s1` could be unevaluated.\n./.venv/lib/python3.13/site-packages/sympy/sets/handlers/power.py:    # TODO: add logic for open intervals?\n./.venv/lib/python3.13/site-packages/sympy/sets/handlers/mul.py:    # TODO: some intervals containing 0 and oo will fail as 0*oo returns nan.\n./.venv/lib/python3.13/site-packages/sympy/sets/handlers/mul.py:    # TODO: handle symbolic intervals\n./.venv/lib/python3.13/site-packages/sympy/sets/handlers/intersection.py:            # TODO: Design a technique to handle multiple-inverse\n./.venv/lib/python3.13/site-packages/sympy/sets/tests/test_setexpr.py:    # TODO: add support for more functions in the future:\n./.venv/lib/python3.13/site-packages/sympy/sets/tests/test_setexpr.py:    # TODO: some expressions cannot be calculated due to bugs (currently\n./.venv/lib/python3.13/site-packages/sympy/sets/sets.py:                    know its dimensions. TODO\n./.venv/lib/python3.13/site-packages/sympy/sets/sets.py:    # TODO: check subsets (`func` in `setv`)\n./.venv/lib/python3.13/site-packages/sympy/sets/sets.py:    # TODO: support more\n./.venv/lib/python3.13/site-packages/sympy/plotting/plot.py:        # TODO: _process_piecewise check goes here\n./.venv/lib/python3.13/site-packages/sympy/plotting/plot.py:# TODO: Add color arrays for plots.\n./.venv/lib/python3.13/site-packages/sympy/plotting/plot.py:# TODO: Add more plotting options for 3d plots.\n./.venv/lib/python3.13/site-packages/sympy/plotting/plot.py:# TODO: Adaptive sampling for 3D plots.\n./.venv/lib/python3.13/site-packages/sympy/plotting/experimental_lambdify.py:#TODO debugging output\n./.venv/lib/python3.13/site-packages/sympy/plotting/experimental_lambdify.py:    #TODO\n./.venv/lib/python3.13/site-packages/sympy/plotting/backends/matplotlibbackend/matplotlib.py:        # TODO The 3D stuff\n./.venv/lib/python3.13/site-packages/sympy/plotting/backends/matplotlibbackend/matplotlib.py:        #TODO after fixing https://github.com/ipython/ipython/issues/1255\n./.venv/lib/python3.13/site-packages/sympy/plotting/utils.py:    # TODO: prange check goes here\n./.venv/lib/python3.13/site-packages/sympy/plotting/series.py:                # TODO: set cse=True once this issue is solved:\n./.venv/lib/python3.13/site-packages/sympy/plotting/series.py:        # TODO: for now, I assume that numpy functions are going to succeed\n./.venv/lib/python3.13/site-packages/sympy/plotting/series.py:                # TODO: what if points[k][idx]==e or points[k][idx+1]==e?\n./.venv/lib/python3.13/site-packages/sympy/plotting/series.py:        # TODO: remove this\n./.venv/lib/python3.13/site-packages/sympy/plotting/series.py:        # TODO: remove this\n./.venv/lib/python3.13/site-packages/sympy/plotting/series.py:        # TODO: remove this\n./.venv/lib/python3.13/site-packages/sympy/plotting/series.py:        # TODO: remove this\n./.venv/lib/python3.13/site-packages/sympy/plotting/series.py:        # TODO: remove this\n./.venv/lib/python3.13/site-packages/sympy/tensor/tensor.py:        # TODO: add possibility of metric after (spinors)\n./.venv/lib/python3.13/site-packages/sympy/tensor/tensor.py:            # TODO: what is the part which is not a coeff?\n./.venv/lib/python3.13/site-packages/sympy/tensor/tensor.py:        # TODO: this could be optimized by only swapping the indices\n./.venv/lib/python3.13/site-packages/sympy/tensor/tensor.py:    # TODO: put this into TensExpr?\n./.venv/lib/python3.13/site-packages/sympy/tensor/tensor.py:    # TODO: put this into TensExpr?\n./.venv/lib/python3.13/site-packages/sympy/tensor/tensor.py:        # TODO: inefficient, this should be done at root level only:\n./.venv/lib/python3.13/site-packages/sympy/tensor/tensor.py:        # TODO: check data compatibility with properties of tensor.\n./.venv/lib/python3.13/site-packages/sympy/tensor/tensor.py:        # TODO: replace .args[0] with .name:\n./.venv/lib/python3.13/site-packages/sympy/tensor/tensor.py:            # TODO: if there is no metric present, the derivative should be zero?\n./.venv/lib/python3.13/site-packages/sympy/tensor/tensor.py:    # TODO: this method should be private\n./.venv/lib/python3.13/site-packages/sympy/tensor/tensor.py:    # TODO: should this method be renamed _from_components_free_dum ?\n./.venv/lib/python3.13/site-packages/sympy/tensor/tensor.py:        # TODO: inherit dummies from expr\n./.venv/lib/python3.13/site-packages/sympy/tensor/tensor.py:        # TODO: can be improved:\n./.venv/lib/python3.13/site-packages/sympy/tensor/tensor.py:    # TODO: add a dum_to_components_map ?\n./.venv/lib/python3.13/site-packages/sympy/tensor/tests/test_tensor.py:    # TODO: add check for *get_symmetric_group_sgs(0)\n./.venv/lib/python3.13/site-packages/sympy/tensor/indexed.py:#   TODO:  (some ideas for improvement)\n./.venv/lib/python3.13/site-packages/sympy/tensor/indexed.py:         broadcasting.  (TODO)\n./.venv/lib/python3.13/site-packages/sympy/tensor/array/ndim_array.py:        # TODO: add checks for dimensions for `value`?\n./.venv/lib/python3.13/site-packages/sympy/tensor/array/expressions/from_indexed_to_array.py:        # TODO: check that Kronecker delta is only contracted to one other element:\n./.venv/lib/python3.13/site-packages/sympy/tensor/array/expressions/array_expressions.py:        # TODO: swap args positions in order to simplify the expression:\n./.venv/lib/python3.13/site-packages/sympy/tensor/array/expressions/array_expressions.py:        # TODO: this should be in a function\n./.venv/lib/python3.13/site-packages/sympy/tensor/array/expressions/array_expressions.py:        # TODO: function in order to permute the args:\n./.venv/lib/python3.13/site-packages/sympy/tensor/array/expressions/array_expressions.py:        # TODO: add API for total rank and cumulative rank:\n./.venv/lib/python3.13/site-packages/sympy/tensor/array/expressions/array_expressions.py:        # TODO: add API for total rank and cumulative rank:\n./.venv/lib/python3.13/site-packages/sympy/tensor/array/expressions/array_expressions.py:        # TODO: check that `expr` has `.subranks`:\n./.venv/lib/python3.13/site-packages/sympy/tensor/array/expressions/tests/test_convert_array_to_matrix.py:    # TODO: this is returning a wrong result:\n./.venv/lib/python3.13/site-packages/sympy/tensor/array/expressions/tests/test_array_expressions.py:    # TODO: not yet supported:\n./.venv/lib/python3.13/site-packages/sympy/tensor/array/expressions/tests/test_array_expressions.py:    # TODO: not yet supported:\n./.venv/lib/python3.13/site-packages/sympy/tensor/array/expressions/tests/test_array_expressions.py:    # TODO: reverse operation starting with `PermuteDims` and getting down to `bb`...\n./.venv/lib/python3.13/site-packages/sympy/tensor/array/expressions/from_array_to_matrix.py:                # TODO: is this break necessary?\n./.venv/lib/python3.13/site-packages/sympy/tensor/array/expressions/from_array_to_matrix.py:        # TODO: this assumes that all arguments are matrices, it may not be the case:\n./.venv/lib/python3.13/site-packages/sympy/tensor/array/expressions/from_array_to_matrix.py:    # TODO: check if subremoved should be permuted as well...\n./.venv/lib/python3.13/site-packages/sympy/tensor/array/expressions/from_array_to_matrix.py:        # TODO: move this to ElementwiseApplyFunction\n./.venv/lib/python3.13/site-packages/sympy/tensor/array/array_derivatives.py:        # TODO: this could be done with multiple-dispatching:\n./.venv/lib/python3.13/site-packages/sympy/assumptions/refine.py:        # TODO: this will probably not work with Integral or Polynomial\n./.venv/lib/python3.13/site-packages/sympy/assumptions/handlers/order.py:    # TODO: This should be deducible from the nonzero handler\n./.venv/lib/python3.13/site-packages/sympy/assumptions/handlers/matrices.py:    # TODO: implement sathandlers system for the matrices.\n./.venv/lib/python3.13/site-packages/sympy/assumptions/handlers/matrices.py:    # TODO: implement sathandlers system for the matrices.\n./.venv/lib/python3.13/site-packages/sympy/assumptions/handlers/matrices.py:    # TODO: implement sathandlers system for the matrices.\n./.venv/lib/python3.13/site-packages/sympy/assumptions/satask.py:        # TODO: Run additional checks to see which combination of the\n./.venv/lib/python3.13/site-packages/sympy/assumptions/predicates/calculus.py:    # TODO: Add examples\n./.venv/lib/python3.13/site-packages/sympy/assumptions/predicates/common.py:    # TODO: Add examples\n./.venv/lib/python3.13/site-packages/sympy/assumptions/predicates/matrices.py:    # TODO: Add handlers to make these keys work with\n./.venv/lib/python3.13/site-packages/sympy/assumptions/predicates/sets.py:    # TODO: Add examples\n./.venv/lib/python3.13/site-packages/sympy/assumptions/predicates/sets.py:    # TODO: Add examples\n./.venv/lib/python3.13/site-packages/sympy/assumptions/predicates/sets.py:    # TODO: Add examples\n./.venv/lib/python3.13/site-packages/sympy/utilities/_compilation/__init__.py:TODO:\n./.venv/lib/python3.13/site-packages/sympy/utilities/tests/test_wester.py:    # TODO: Replace solve with solveset, as of now test fails for solveset\n./.venv/lib/python3.13/site-packages/sympy/utilities/tests/test_wester.py:    # TODO: Replace solve with solveset when it gives Lambert solution\n./.venv/lib/python3.13/site-packages/sympy/utilities/tests/test_wester.py:    # TODO: x = [-1, 2*(+/-asinh(1)*I + n*pi}, 3*(pi/6 + n*pi/3)]\n./.venv/lib/python3.13/site-packages/sympy/utilities/tests/test_wester.py:    # TODO: Replace solve with solveset, as of now test fails for solveset\n./.venv/lib/python3.13/site-packages/sympy/utilities/tests/test_wester.py:    # TODO: Replace solve with solveset, as of now test fails for solveset\n./.venv/lib/python3.13/site-packages/sympy/utilities/tests/test_wester.py:    # TODO: Replace solve with solveset, as of now test fails for solveset\n./.venv/lib/python3.13/site-packages/sympy/utilities/tests/test_wester.py:    # TODO: Replace solve with solveset, as of now test fails for solveset\n./.venv/lib/python3.13/site-packages/sympy/utilities/tests/test_wester.py:    # TODO: Replace solve with solveset, as of now test fails for solveset\n./.venv/lib/python3.13/site-packages/sympy/utilities/tests/test_wester.py:    # TODO: Replace solve with solveset, as of now test fails for solveset\n./.venv/lib/python3.13/site-packages/sympy/utilities/tests/test_wester.py:    # TODO: Replace solve with solveset which gives both [+/- current answer]\n./.venv/lib/python3.13/site-packages/sympy/utilities/tests/test_wester.py:    # TODO: Replace solve with solveset, as of now\n./.venv/lib/python3.13/site-packages/sympy/utilities/tests/test_wester.py:    # TODO: Replace solve with solveset, as of now\n./.venv/lib/python3.13/site-packages/sympy/utilities/tests/test_wester.py:    # TODO: Replace solve with solveset, as of now\n./.venv/lib/python3.13/site-packages/sympy/utilities/tests/test_wester.py:    # TODO: Replace solve with solveset, as of now\n./.venv/lib/python3.13/site-packages/sympy/utilities/tests/test_wester.py:    # TODO: Replace solve with solveset, as of now\n./.venv/lib/python3.13/site-packages/sympy/utilities/tests/test_wester.py:    # TODO: Replace solve with solveset, as of now\n./.venv/lib/python3.13/site-packages/sympy/utilities/tests/test_wester.py:    # TODO: Replace solve with solveset, current test fails for solveset\n./.venv/lib/python3.13/site-packages/sympy/utilities/tests/test_wester.py:    # TODO: Replace solve with solveset, when it works for solveset\n./.venv/lib/python3.13/site-packages/sympy/utilities/tests/test_pickling.py:    # TODO: fix pickling of Options class (see GroebnerBasis._options)\n./.venv/lib/python3.13/site-packages/sympy/utilities/tests/test_pickling.py:        check(c, exclude=[0, 1], check_attr=False) # TODO: Py3k\n./.venv/lib/python3.13/site-packages/sympy/utilities/tests/test_pickling.py:    # TODO: AssertionError: assert id(obj) not in self.memo\n./.venv/lib/python3.13/site-packages/sympy/utilities/tests/test_pickling.py:    # TODO: AssertionError: assert id(obj) not in self.memo\n./.venv/lib/python3.13/site-packages/sympy/utilities/tests/test_pickling.py:    # TODO: fix pickling of ModularInteger\n./.venv/lib/python3.13/site-packages/sympy/utilities/tests/test_pickling.py:    # TODO: fix pickling of RealElement\n./.venv/lib/python3.13/site-packages/sympy/utilities/tests/test_pickling.py:    # TODO: fix pickling of ComplexElement\n./.venv/lib/python3.13/site-packages/sympy/utilities/tests/test_pickling.py:    # TODO: fix pickling of ModularInteger\n./.venv/lib/python3.13/site-packages/sympy/utilities/tests/test_pickling.py:        # TODO: fix pickling of ModularInteger\n./.venv/lib/python3.13/site-packages/sympy/utilities/tests/test_pickling.py:    # TODO: fix pickling of RealElement\n./.venv/lib/python3.13/site-packages/sympy/utilities/tests/test_pickling.py:    # TODO: fix pickling of ComplexElement\n./.venv/lib/python3.13/site-packages/sympy/utilities/tests/test_pickling.py:    # TODO: AssertionError\n./.venv/lib/python3.13/site-packages/sympy/utilities/tests/test_pickling.py:    # TODO: AttributeError: 'PolyElement' object has no attribute 'ring'\n./.venv/lib/python3.13/site-packages/sympy/utilities/tests/test_pickling.py:    # TODO: Argh, Python is so naive. No lambdas nor inner function support in\n./.venv/lib/python3.13/site-packages/sympy/utilities/tests/test_pickling.py:    # TODO: TypeError: __init__() takes at least 3 arguments (1 given)\n./.venv/lib/python3.13/site-packages/sympy/utilities/tests/test_pickling.py:    # TODO: TypeError: can't pickle instancemethod objects\n./.venv/lib/python3.13/site-packages/sympy/utilities/tests/test_pickling.py:    # TODO: PicklingError: Can't pickle <function <lambda> at 0x38578c0>: it's not found as __main__.<lambda>\n./.venv/lib/python3.13/site-packages/sympy/utilities/tests/test_pickling.py:    # TODO: TypeError: __init__() takes at least 3 arguments (1 given)\n./.venv/lib/python3.13/site-packages/sympy/utilities/tests/test_pickling.py:    # TODO: fix pickling of `symbols' flag\n./.venv/lib/python3.13/site-packages/sympy/utilities/tests/test_pickling.py:# TODO: def test_pickling_polys_rootisolation():\n./.venv/lib/python3.13/site-packages/sympy/interactive/tests/test_ipython.py:# TODO: The code below could be made more granular with something like:\n./.venv/lib/python3.13/site-packages/sympy/interactive/tests/test_ipython.py:    # TODO: How can we test that the output of a SyntaxError is the original\n./.venv/lib/python3.13/site-packages/sympy/series/series_class.py:        TODO\n./.venv/lib/python3.13/site-packages/sympy/series/gruntz.py:    # TODO this should not be necessary\n./.venv/lib/python3.13/site-packages/sympy/series/tests/test_gruntz.py:    # TODO zeta function series\n./.venv/lib/python3.13/site-packages/sympy/series/tests/test_gruntz.py:    # TODO 8.35 - 8.37 (bessel, max-min)\n./.venv/lib/python3.13/site-packages/sympy/series/tests/test_order.py:    # TODO : A better output for Order(log(x) + 1/log(x))\n./.venv/lib/python3.13/site-packages/sympy/series/tests/test_formal.py:    f = x*exp(x)*sin(2*x)  # TODO: rsolve needs improvement\n./.venv/lib/python3.13/site-packages/sympy/vector/functions.py:        # TODO: This gets a random coordinate system in case of multiple ones:\n./.venv/lib/python3.13/site-packages/sympy/vector/functions.py:        # TODO : The following line introduces a performance issue\n./.venv/lib/python3.13/site-packages/sympy/vector/operators.py:        # TODO: is case of many coord systems, this gets a random one:\n./.venv/lib/python3.13/site-packages/sympy/vector/coordsysrect.py:            # TODO: trigsimp is needed here so that the matrix becomes\n./.venv/lib/python3.13/site-packages/sympy/concrete/summations.py:        # TODO\n./.venv/lib/python3.13/site-packages/sympy/concrete/tests/test_sums_products.py:    # TODO Implement matrix geometric series summation.\n./.venv/lib/python3.13/site-packages/sympy/codegen/rewriting.py:    # TODO: We should be able to support more than 2 elements\n./.venv/lib/python3.13/site-packages/sympy/codegen/tests/test_rewriting.py:def test_optims_numpy_TODO():\n./.venv/lib/python3.13/site-packages/sympy/codegen/tests/test_rewriting.py:    NUMBER_OF_DIGITS = 25   # TODO: this should ideally be automatically handled.\n./.venv/lib/python3.13/site-packages/sympy/printing/str.py:        #TODO : Handle indices\n./.venv/lib/python3.13/site-packages/sympy/printing/latex.py:    # TODO: merge this with the above, which requires a lot of test changes\n./.venv/lib/python3.13/site-packages/sympy/printing/latex.py:        # TODO should exp_polar be printed differently?\n./.venv/lib/python3.13/site-packages/sympy/printing/latex.py:                # TODO incorporate order\n./.venv/lib/python3.13/site-packages/sympy/printing/latex.py:        # TODO: This expression is potentially confusing,\n./.venv/lib/python3.13/site-packages/sympy/printing/latex.py:        # TODO nicer fractions for few generators...\n./.venv/lib/python3.13/site-packages/sympy/printing/latex.py:        # TODO nicer fractions for few generators...\n./.venv/lib/python3.13/site-packages/sympy/printing/latex.py:        # TODO: Handle indices\n./.venv/lib/python3.13/site-packages/sympy/printing/llvmjitcode.py:    # TODO - assumes all called functions take one double precision argument.\n./.venv/lib/python3.13/site-packages/sympy/printing/tensorflow.py:    # TODO: a better class structure would avoid this mess:\n./.venv/lib/python3.13/site-packages/sympy/printing/tensorflow.py:        # TODO: is this necessary?\n./.venv/lib/python3.13/site-packages/sympy/printing/tests/test_repr.py:    # TODO more tests\n./.venv/lib/python3.13/site-packages/sympy/printing/tests/test_aesaracode.py:@SKIP  # TODO - this is currently not checked but should be implemented\n./.venv/lib/python3.13/site-packages/sympy/printing/tests/test_aesaracode.py:    # TODO - matrix broadcasting?\n./.venv/lib/python3.13/site-packages/sympy/printing/tests/test_aesaracode.py:    # assert theq(aesara_code_(sy.Ne(x, y)), aet.neq(xt, yt))  # TODO - implement\n./.venv/lib/python3.13/site-packages/sympy/printing/tests/test_latex.py:    #TODO: Handle indices\n./.venv/lib/python3.13/site-packages/sympy/printing/tests/test_theanocode.py:@SKIP  # TODO - this is currently not checked but should be implemented\n./.venv/lib/python3.13/site-packages/sympy/printing/tests/test_theanocode.py:    # TODO - matrix broadcasting?\n./.venv/lib/python3.13/site-packages/sympy/printing/tests/test_theanocode.py:    # assert theq(theano_code_(sy.Ne(x, y)), tt.neq(xt, yt))  # TODO - implement\n./.venv/lib/python3.13/site-packages/sympy/printing/pretty/stringpict.py:TODO:\n./.venv/lib/python3.13/site-packages/sympy/printing/pretty/pretty.py:        # TODO should exp_polar be printed differently?\n./.venv/lib/python3.13/site-packages/sympy/printing/pretty/pretty.py:            #TODO: Move this code to prettyForm\n./.venv/lib/python3.13/site-packages/sympy/printing/pretty/pretty.py:        # TODO: the stuff to the left of the | and the stuff to the right of\n./.venv/lib/python3.13/site-packages/sympy/printing/pretty/pretty.py:        # TODO: copy-pasted from _print_Function: can we do better?\n./.venv/lib/python3.13/site-packages/sympy/printing/pretty/pretty.py:                # TODO incorporate order\n./.venv/lib/python3.13/site-packages/sympy/printing/pretty/pretty.py:        #TODO: Handle indices\n./.venv/lib/python3.13/site-packages/sympy/printing/pretty/tests/test_pretty.py:    # TODO: The \"x in N\" parts below should be centered independently of the\n./.venv/lib/python3.13/site-packages/sympy/printing/pretty/tests/test_pretty.py:    # TODO: add support for ASCII pretty.\n./.venv/lib/python3.13/site-packages/sympy/printing/pretty/tests/test_pretty.py:    # TODO: TBD polylog(s - 1, z)\n./.venv/lib/python3.13/site-packages/sympy/printing/pretty/pretty_symbology.py:# TODO: Make brackets adjust to height of contents\n./.venv/lib/python3.13/site-packages/sympy/printing/pretty/pretty_symbology.py:    # TODO robustify when no unicodedat available\n./.venv/lib/python3.13/site-packages/sympy/testing/runtests.py:        # TODO parse integers as well ?\n./.venv/lib/python3.13/site-packages/sympy/testing/runtests.py:        # TODO: Should these be protected?\n./.venv/lib/python3.13/site-packages/sympy/stats/crv_types.py:    .. TODO - What is the difference between these degrees of freedom?\n./.venv/lib/python3.13/site-packages/sympy/stats/crv_types.py:    .. TODO - what does the parameter mean?\n./.venv/lib/python3.13/site-packages/sympy/stats/matrix_distributions.py:    ### TODO: Add tests after adding matrix distributions in numpy_rv_map\n./.venv/lib/python3.13/site-packages/sympy/stats/random_matrix_models.py:        # TODO : Add support for Lie groups(as extensions of sympy.diffgeom)\n./.venv/lib/python3.13/site-packages/sympy/stats/joint_rv.py:            #TODO: Modify to support integration\n./.venv/lib/python3.13/site-packages/sympy/stats/frv.py:            #TODO: Implement the mechanism for handling queries for symbolic sized distributions.\n./.venv/lib/python3.13/site-packages/sympy/stats/drv.py:        # TODO: support discrete sets with non integer stepsizes\n./.venv/lib/python3.13/site-packages/sympy/stats/rv.py:        # TODO : Remove when lambdify accepts 'pymc' as module\n./.venv/lib/python3.13/site-packages/sympy/stats/rv.py:                # TODO: Replace the try-except block with only given_fn(*args)\n./.venv/lib/python3.13/site-packages/sympy/stats/rv.py:                    # TODO: Replace the try-except block with only given_fn(*args)\n./.venv/lib/python3.13/site-packages/sympy/stats/rv.py:            # TODO: Replace the try-except block with only fn(*args)\n./.venv/lib/python3.13/site-packages/sympy/stats/rv.py:            # TODO: do this for drv.py and frv.py if necessary.\n./.venv/lib/python3.13/site-packages/sympy/stats/rv.py:            # TODO: add more distributions here if there are more\n./.venv/lib/python3.13/site-packages/sympy/stats/tests/test_continuous_rv.py:    with ignore_warnings(UserWarning): ### TODO: Restore tests once warnings are removed\n./.venv/lib/python3.13/site-packages/sympy/stats/tests/test_continuous_rv.py:    with ignore_warnings(UserWarning): ### TODO: Restore tests once warnings are removed\n./.venv/lib/python3.13/site-packages/sympy/stats/tests/test_continuous_rv.py:    with ignore_warnings(UserWarning): ### TODO: Restore tests once warnings are removed\n./.venv/lib/python3.13/site-packages/sympy/stats/tests/test_finite_rv.py:    # TODO: Make iid method!\n./.venv/lib/python3.13/site-packages/sympy/stats/tests/test_compound_rv.py:    with ignore_warnings(UserWarning): ### TODO: Restore tests once warnings are removed\n./.venv/lib/python3.13/site-packages/sympy/stats/tests/test_mix.py:    with ignore_warnings(UserWarning): ### TODO: Restore tests once warnings are removed\n./.venv/lib/python3.13/site-packages/sympy/stats/tests/test_stochastic_process.py:        with ignore_warnings(UserWarning):  # TODO: Restore tests once warnings are removed\n./.venv/lib/python3.13/site-packages/sympy/stats/tests/test_stochastic_process.py:    with ignore_warnings(UserWarning): ### TODO: Restore tests once warnings are removed\n./.venv/lib/python3.13/site-packages/sympy/stats/tests/test_stochastic_process.py:    with ignore_warnings(UserWarning): ### TODO: Restore tests once warnings are removed\n./.venv/lib/python3.13/site-packages/sympy/stats/joint_rv_types.py:    #TODO: Add support for sets provided by the user\n./.venv/lib/python3.13/site-packages/sympy/logic/algorithms/lra_theory.py:TODO:\n./.venv/lib/python3.13/site-packages/sympy/diffgeom/diffgeom.py:# TODO you are a bit excessive in the use of Dummies\n./.venv/lib/python3.13/site-packages/sympy/diffgeom/diffgeom.py:# TODO dummy point, literal field\n./.venv/lib/python3.13/site-packages/sympy/diffgeom/diffgeom.py:# TODO too often one needs to call doit or simplify on the output, check the\n./.venv/lib/python3.13/site-packages/sympy/diffgeom/diffgeom.py:        # TODO: you need a real dummy function for the next line\n./.venv/lib/python3.13/site-packages/sympy/diffgeom/diffgeom.py:                    if c:  # TODO this is ugly - the Commutator can be Zero and\n./.venv/lib/python3.13/site-packages/sympy/diffgeom/diffgeom.py:    # TODO the calculation of signatures is slow\n./.venv/lib/python3.13/site-packages/sympy/diffgeom/diffgeom.py:    # TODO you do not need all these permutations (neither the prefactor)\n./.venv/lib/python3.13/site-packages/sympy/diffgeom/diffgeom.py:        # TODO: you need a real dummy function for the next line\n./.venv/lib/python3.13/site-packages/sympy/diffgeom/diffgeom.py:    # TODO Is this a good idea?\n./.venv/lib/python3.13/site-packages/sympy/diffgeom/diffgeom.py:    # TODO move some of this to class methods.\n./.venv/lib/python3.13/site-packages/sympy/diffgeom/diffgeom.py:    # TODO rewrite using the .as_blah_blah methods\n./.venv/lib/python3.13/site-packages/sympy/diffgeom/diffgeom.py:    # TODO move some of this to class methods.\n./.venv/lib/python3.13/site-packages/sympy/diffgeom/diffgeom.py:    # TODO rewrite using the .as_blah_blah methods\n./.venv/lib/python3.13/site-packages/sympy/diffgeom/tests/test_class_structure.py:    #TODO assert point.subs(x, 2) == Point(cs, [2, y])\n./.venv/lib/python3.13/site-packages/sympy/diffgeom/tests/test_class_structure.py:    #TODO assert point.free_symbols == set([x, y])\n./.venv/lib/python3.13/site-packages/sympy/diffgeom/tests/test_diffgeom.py:    #TODO assert m == R2_r.transform(R2_p, R2_p.transform(R2_r, [a, b])).applyfunc(simplify)\n./.venv/lib/python3.13/site-packages/sympy/diffgeom/tests/test_diffgeom.py:    #TODO assert m == R3_r.transform(R3_c, R3_c.transform(R3_r, m)).applyfunc(simplify)\n./.venv/lib/python3.13/site-packages/sympy/diffgeom/tests/test_diffgeom.py:    #TODO assert m == R3_r.transform(R3_s, R3_s.transform(R3_r, m)).applyfunc(simplify)\n./.venv/lib/python3.13/site-packages/sympy/diffgeom/tests/test_diffgeom.py:    #TODO assert m == R3_c.transform(R3_s, R3_s.transform(R3_c, m)).applyfunc(simplify)\n./.venv/lib/python3.13/site-packages/sympy/diffgeom/tests/test_diffgeom.py:        #TODO assert m == R3_r.coord_tuple_transform_to(R3_c, R3_c.coord_tuple_transform_to(R3_r, m)).applyfunc(simplify)\n./.venv/lib/python3.13/site-packages/sympy/diffgeom/tests/test_diffgeom.py:        #TODO assert m == R3_r.coord_tuple_transform_to(R3_s, R3_s.coord_tuple_transform_to(R3_r, m)).applyfunc(simplify)\n./.venv/lib/python3.13/site-packages/sympy/diffgeom/tests/test_diffgeom.py:        #TODO assert m == R3_c.coord_tuple_transform_to(R3_s, R3_s.coord_tuple_transform_to(R3_c, m)).applyfunc(simplify)\n./.venv/lib/python3.13/site-packages/sympy/diffgeom/tests/test_hyperbolic_space.py:    #TODO - it would be nice to have index contraction built-in\n./.venv/lib/python3.13/site-packages/sympy/integrals/transforms.py:        # TODO handle derivatives etc\n./.venv/lib/python3.13/site-packages/sympy/integrals/transforms.py:    # TODO\n./.venv/lib/python3.13/site-packages/sympy/integrals/integrals.py:        # check for regularity conditions (TODO), see issue 4215\n./.venv/lib/python3.13/site-packages/sympy/integrals/meijerint.py:    # TODO this needs more polar_lift (c/f entry for exp)\n./.venv/lib/python3.13/site-packages/sympy/integrals/meijerint.py:    # TODO can do sin^n, sinh^n by expansion ... where?\n./.venv/lib/python3.13/site-packages/sympy/integrals/meijerint.py:    # TODO can do t + a. but can also do by expansion... (XXX not really)\n./.venv/lib/python3.13/site-packages/sympy/integrals/meijerint.py:    # TODO these only hold for positive p, and can be made more general\n./.venv/lib/python3.13/site-packages/sympy/integrals/meijerint.py:    # TODO also it would be nice to derive them recursively ...\n./.venv/lib/python3.13/site-packages/sympy/integrals/meijerint.py:    # TODO log(x)/(x+a) and log(x)/(x-1) can also be done. should they\n./.venv/lib/python3.13/site-packages/sympy/integrals/meijerint.py:    # TODO further formulae in this section seem obscure\n./.venv/lib/python3.13/site-packages/sympy/integrals/meijerint.py:    # TODO\n./.venv/lib/python3.13/site-packages/sympy/integrals/meijerint.py:    # TODO exp(-x)*erf(I*x) does not work\n./.venv/lib/python3.13/site-packages/sympy/integrals/meijerint.py:    # TODO all of the following should be derivable\n./.venv/lib/python3.13/site-packages/sympy/integrals/meijerint.py:    # TODO many more formulas. should all be derivable\n./.venv/lib/python3.13/site-packages/sympy/integrals/meijerint.py:    # TODO many more formulas. should all be derivable\n./.venv/lib/python3.13/site-packages/sympy/integrals/meijerint.py:    # TODO should this be a method of meijerg?\n./.venv/lib/python3.13/site-packages/sympy/integrals/meijerint.py:    # TODO altered cases 4-7\n./.venv/lib/python3.13/site-packages/sympy/integrals/meijerint.py:    # TODO This leaves only one case from the three listed by Prudnikov.\n./.venv/lib/python3.13/site-packages/sympy/integrals/meijerint.py:    # XXX TODO we should reduce order first\n./.venv/lib/python3.13/site-packages/sympy/integrals/meijerint.py:    # TODO should we try both?\n./.venv/lib/python3.13/site-packages/sympy/integrals/meijerint.py:    # XXX TODO this is a testing *nightmare*\n./.venv/lib/python3.13/site-packages/sympy/integrals/meijerint.py:    # TODO\n./.venv/lib/python3.13/site-packages/sympy/integrals/trigonometry.py:# TODO sin(a*x)*cos(b*x) -> sin((a+b)x) + sin((a-b)x) ?\n./.venv/lib/python3.13/site-packages/sympy/integrals/manualintegrate.py:            # TODO: This is for future development, as currently\n./.venv/lib/python3.13/site-packages/sympy/integrals/rde.py:# TODO: Add messages to NonElementaryIntegralException errors\n./.venv/lib/python3.13/site-packages/sympy/integrals/rde.py:    # TODO: finish writing this and write tests\n./.venv/lib/python3.13/site-packages/sympy/integrals/rde.py:    # TODO: finish writing this and write tests\n./.venv/lib/python3.13/site-packages/sympy/integrals/rde.py:# TODO: better name for this function\n./.venv/lib/python3.13/site-packages/sympy/integrals/rde.py:            # TODO: Write a dummy function that does this idiom\n./.venv/lib/python3.13/site-packages/sympy/integrals/rde.py:        # TODO: Is this check necessary, and if so, what should it do if it fails?\n./.venv/lib/python3.13/site-packages/sympy/integrals/prde.py:    # TODO: Merge this with the very similar special_denom() in rde.py\n./.venv/lib/python3.13/site-packages/sympy/integrals/prde.py:                        # TODO: Add test\n./.venv/lib/python3.13/site-packages/sympy/integrals/prde.py:        # TODO: implement this\n./.venv/lib/python3.13/site-packages/sympy/integrals/prde.py:    # TODO: finish writing this and write tests\n./.venv/lib/python3.13/site-packages/sympy/integrals/prde.py:        # TODO: We treat this as 'no solution', until the structure\n./.venv/lib/python3.13/site-packages/sympy/integrals/prde.py:    # TODO: Write the full algorithm using the structure theorems.\n./.venv/lib/python3.13/site-packages/sympy/integrals/prde.py:        # TODO: This could be implemented more efficiently.\n./.venv/lib/python3.13/site-packages/sympy/integrals/prde.py:        # TODO: What should really be done in this case?\n./.venv/lib/python3.13/site-packages/sympy/integrals/prde.py:        # TODO: What should really be done in this case?\n./.venv/lib/python3.13/site-packages/sympy/integrals/prde.py:            # TODO: But maybe we can tell if they're not rational, like\n./.venv/lib/python3.13/site-packages/sympy/integrals/prde.py:    # TODO: finish writing this and write tests\n./.venv/lib/python3.13/site-packages/sympy/integrals/prde.py:        # TODO: we can use more efficient residue reduction from ratint()\n./.venv/lib/python3.13/site-packages/sympy/integrals/intpoly.py:        #  TODO : This part is quite hacky. Should be made more robust with\n./.venv/lib/python3.13/site-packages/sympy/integrals/intpoly.py:        #  TODO : respect to symbol names and scalable w.r.t higher dimensions.\n./.venv/lib/python3.13/site-packages/sympy/integrals/laplace.py:        # TODO: rules with sqrt(a*t) and sqrt(a/t) have stopped working after\n./.venv/lib/python3.13/site-packages/sympy/integrals/laplace.py:        # TODO\n./.venv/lib/python3.13/site-packages/sympy/integrals/laplace.py:        # TODO not implemented yet, but also not important\n./.venv/lib/python3.13/site-packages/sympy/integrals/tests/test_risch.py:    # TODO: Skip or make faster\n./.venv/lib/python3.13/site-packages/sympy/integrals/tests/test_risch.py:    # TODO: Add tests for integrate_hyperexponential() from the book\n./.venv/lib/python3.13/site-packages/sympy/integrals/tests/test_risch.py:    # TODO: Add a test where two different parts of the extension use a\n./.venv/lib/python3.13/site-packages/sympy/integrals/tests/test_integrals.py:    # TODO: Remove conds='none' below, let the assumption take care of it.\n./.venv/lib/python3.13/site-packages/sympy/integrals/tests/test_integrals.py:    # TODO: Remove conds='none' below, let the assumption take care of it.\n./.venv/lib/python3.13/site-packages/sympy/integrals/tests/test_integrals.py:    # TODO: How to test risch=False?\n./.venv/lib/python3.13/site-packages/sympy/integrals/tests/test_laplace.py:    # TODO: rules with sqrt(a*t) and sqrt(a/t) have stopped working after\n./.venv/lib/python3.13/site-packages/sympy/integrals/tests/test_laplace.py:    # TODO sinh/cosh shifted come out a mess. also delayed trig is a mess\n./.venv/lib/python3.13/site-packages/sympy/integrals/tests/test_laplace.py:    # TODO should this simplify further?\n./.venv/lib/python3.13/site-packages/sympy/integrals/tests/test_laplace.py:    # TODO can we make erf(t) work?\n./.venv/lib/python3.13/site-packages/sympy/integrals/tests/test_laplace.py:    # TODO LT of Si, Shi, Chi is a mess ...\n./.venv/lib/python3.13/site-packages/sympy/integrals/tests/test_meijerint.py:    # TODO what simplifications should be done automatically?\n./.venv/lib/python3.13/site-packages/sympy/integrals/tests/test_meijerint.py:    # TODO it would be nice to test the condition\n./.venv/lib/python3.13/site-packages/sympy/integrals/tests/test_meijerint.py:    # TODO more orthogonality integrals\n./.venv/lib/python3.13/site-packages/sympy/integrals/tests/test_meijerint.py:    # TODO can do higher powers, but come out as high order ... should they be\n./.venv/lib/python3.13/site-packages/sympy/integrals/tests/test_meijerint.py:    # TODO more besseli when tables are extended or recursive mellin works\n./.venv/lib/python3.13/site-packages/sympy/integrals/tests/test_meijerint.py:    # TODO how does besselj(0, a*x)*besselj(0, b*x) work?\n./.venv/lib/python3.13/site-packages/sympy/integrals/tests/test_meijerint.py:    # TODO how does besselj(0, x)**2*besselj(1, x)**2 work?\n./.venv/lib/python3.13/site-packages/sympy/integrals/tests/test_meijerint.py:    # TODO sin(x)*besselj(0, x) etc come out a mess\n./.venv/lib/python3.13/site-packages/sympy/integrals/tests/test_meijerint.py:    # TODO can x*log(x)*besselj(0, x) be done?\n./.venv/lib/python3.13/site-packages/sympy/integrals/tests/test_meijerint.py:    # TODO how does besselj(1, x)*besselj(0, x+a) work?\n./.venv/lib/python3.13/site-packages/sympy/integrals/tests/test_meijerint.py:    # TODO more indefinite integrals when struve functions etc are implemented\n./.venv/lib/python3.13/site-packages/sympy/integrals/tests/test_meijerint.py:    # TODO gammasimp cannot prove that the factor is unity\n./.venv/lib/python3.13/site-packages/sympy/integrals/tests/test_meijerint.py:    # TODO conditions are a mess\n./.venv/lib/python3.13/site-packages/sympy/integrals/tests/test_meijerint.py:    # TODO gamma, rayleigh\n./.venv/lib/python3.13/site-packages/sympy/integrals/tests/test_meijerint.py:    # TODO are there other distributions supported on (-oo, oo) that we can do?\n./.venv/lib/python3.13/site-packages/sympy/integrals/tests/test_meijerint.py:    # TODO maybe simplify the inequalities? when the simplification\n./.venv/lib/python3.13/site-packages/sympy/integrals/tests/test_meijerint.py:    # TODO FT(besselj(0,x)) - conditions are messy (but for acceptable reasons)\n./.venv/lib/python3.13/site-packages/sympy/integrals/tests/test_heurisch.py:    # TODO: it looks like this used to work just by coincindence and\n./.venv/lib/python3.13/site-packages/sympy/integrals/tests/test_heurisch.py:    # TODO: heurisch() is off by a constant: -3/4. Possibly different permutation\n./.venv/lib/python3.13/site-packages/sympy/integrals/tests/test_heurisch.py:# TODO: convert the rest of PMINT tests:\n./.venv/lib/python3.13/site-packages/sympy/integrals/tests/test_rde.py:    # TODO: add more tests here\n./.venv/lib/python3.13/site-packages/sympy/integrals/tests/test_rde.py:    # TODO: Add test for when the degree bound becomes larger after limited_integrate\n./.venv/lib/python3.13/site-packages/sympy/integrals/tests/test_rde.py:    # TODO: Add test for db == da - 1 case\n./.venv/lib/python3.13/site-packages/sympy/integrals/tests/test_rde.py:    # TODO: Add tests\n./.venv/lib/python3.13/site-packages/sympy/integrals/tests/test_rde.py:    # TODO: Add test for when the degree becomes larger after parametric_log_deriv()\n./.venv/lib/python3.13/site-packages/sympy/integrals/tests/test_rde.py:    # TODO: Add more exp tests, including tests that require is_deriv_in_field()\n./.venv/lib/python3.13/site-packages/sympy/integrals/tests/test_rde.py:    # TODO: Add more primitive tests, including tests that require is_deriv_in_field()\n./.venv/lib/python3.13/site-packages/sympy/integrals/tests/test_rde.py:    # TODO: Add more tests for rischDE, including ones from the text\n./.venv/lib/python3.13/site-packages/sympy/integrals/tests/test_transforms.py:    # TODO does not work with bneg, argument wrong. Needs changes to matching.\n./.venv/lib/python3.13/site-packages/sympy/integrals/tests/test_transforms.py:    # TODO we cannot currently do these (needs summation of 3F2(-1))\n./.venv/lib/python3.13/site-packages/sympy/integrals/tests/test_transforms.py:    # TODO we can't do any of these (delicate cancellation)\n./.venv/lib/python3.13/site-packages/sympy/integrals/tests/test_transforms.py:    # TODO bessely(a, x)*besselk(a, x) is a mess\n./.venv/lib/python3.13/site-packages/sympy/integrals/tests/test_transforms.py:    # TODO products of besselk are a mess\n./.venv/lib/python3.13/site-packages/sympy/integrals/tests/test_transforms.py:    # TODO exp(x/2)*besselk(a, x/2) [etc] cannot currently be done\n./.venv/lib/python3.13/site-packages/sympy/integrals/tests/test_transforms.py:    # TODO various strange products of special orders\n./.venv/lib/python3.13/site-packages/sympy/integrals/tests/test_transforms.py:    # TODO\n./.venv/lib/python3.13/site-packages/sympy/integrals/tests/test_transforms.py:    # TODO this comes out as an amazing mess, but simplifies nicely\n./.venv/lib/python3.13/site-packages/sympy/integrals/tests/test_transforms.py:    # TODO this can be further simplified!\n./.venv/lib/python3.13/site-packages/sympy/integrals/tests/test_transforms.py:    # TODO more\n./.venv/lib/python3.13/site-packages/sympy/integrals/tests/test_transforms.py:    # TODO for this to work with real a, need to expand abs(a*x) to abs(a)*abs(x)\n./.venv/lib/python3.13/site-packages/sympy/integrals/tests/test_transforms.py:    # TODO IFT is a *mess*\n./.venv/lib/python3.13/site-packages/sympy/integrals/tests/test_transforms.py:    # TODO IFT\n./.venv/lib/python3.13/site-packages/sympy/integrals/tests/test_transforms.py:    # TODO IFT without factoring comes out as meijer g\n./.venv/lib/python3.13/site-packages/sympy/integrals/tests/test_transforms.py:    # TODO IFT (comes out as meijer G)\n./.venv/lib/python3.13/site-packages/sympy/integrals/tests/test_transforms.py:    # TODO besselj(n, x), n an integer > 0 actually can be done...\n./.venv/lib/python3.13/site-packages/sympy/integrals/tests/test_transforms.py:    # TODO are there other common transforms (no distributions!)?\n./.venv/lib/python3.13/site-packages/sympy/integrals/tests/test_trigonometry.py:    # TODO: remove conds='none' below. For this to work we would have to rule\n./.venv/lib/python3.13/site-packages/sympy/integrals/tests/test_prde.py:    # TODO: when bound_degree() can handle this, test degree bound from that too\n./.venv/lib/python3.13/site-packages/sympy/integrals/tests/test_prde.py:    # TODO: Add test for deg(b) <= 0 with b small\n./.venv/lib/python3.13/site-packages/sympy/integrals/tests/test_prde.py:    # TODO: Add more tests\n./.venv/lib/python3.13/site-packages/sympy/integrals/tests/test_prde.py:    # TODO: Add more tests, including ones with exponentials\n./.venv/lib/python3.13/site-packages/sympy/integrals/heurisch.py:    # TODO: caching is significant factor for why permutations work at all. Change this.\n./.venv/lib/python3.13/site-packages/sympy/integrals/heurisch.py:        # TODO: Currently it's better to use symbolic expressions here instead\n./.venv/lib/python3.13/site-packages/sympy/integrals/heurisch.py:                # TODO: Non-polynomial expression. This should have been\n./.venv/lib/python3.13/site-packages/sympy/integrals/risch.py:        # TODO: This probably doesn't need to be completely recomputed at\n./.venv/lib/python3.13/site-packages/sympy/integrals/risch.py:                    # TODO: Would there ever be any benefit from just\n./.venv/lib/python3.13/site-packages/sympy/integrals/risch.py:            # TODO: Just put it in self.Tfuncs\n./.venv/lib/python3.13/site-packages/sympy/integrals/risch.py:                    # TODO: Add something to backsubs to put exp(const*p)\n./.venv/lib/python3.13/site-packages/sympy/integrals/risch.py:                        # TODO: give algebraic dependence in error string\n./.venv/lib/python3.13/site-packages/sympy/integrals/risch.py:    # TODO: Rewrite algorithms below to use this (?)\n./.venv/lib/python3.13/site-packages/sympy/integrals/risch.py:    # TODO: Pass through information about why the integral was nonelementary,\n./.venv/lib/python3.13/site-packages/sympy/integrals/risch.py:    # TODO: This should go in densetools.py.\n./.venv/lib/python3.13/site-packages/sympy/integrals/risch.py:    # TODO: Use this on the final result.  That way, we can avoid answers like\n./.venv/lib/python3.13/site-packages/sympy/integrals/risch.py:    # TODO: This algorithm appears to be faster in every case\n./.venv/lib/python3.13/site-packages/sympy/integrals/risch.py:    # TODO: Verify this and splitfactor() for multiple extensions\n./.venv/lib/python3.13/site-packages/sympy/integrals/risch.py:        # TODO also consider the complex roots which should\n./.venv/lib/python3.13/site-packages/sympy/integrals/risch.py:    # TODO: Use log_to_atan() from rationaltools.py\n./.venv/lib/python3.13/site-packages/sympy/integrals/risch.py:    # TODO: check what Lambda does with RootOf\n./.venv/lib/python3.13/site-packages/sympy/integrals/risch.py:    # TODO: verify that this is correct for multiple extensions\n./.venv/lib/python3.13/site-packages/sympy/integrals/risch.py:        # TODO: This does not do the right thing when b is False\n./.venv/lib/python3.13/site-packages/sympy/integrals/risch.py:    # TODO: Integral from k?\n./.venv/lib/python3.13/site-packages/sympy/integrals/risch.py:    # TODO: split out nonelementary integral\n./.venv/lib/python3.13/site-packages/sympy/integrals/risch.py:    # TODO: This is useful in and of itself, because isinstance(result,\n./.venv/lib/python3.13/site-packages/sympy/simplify/trigsimp.py:    # TODO\n./.venv/lib/python3.13/site-packages/sympy/simplify/simplify.py:    # TODO: Apply different strategies, considering expression pattern:\n./.venv/lib/python3.13/site-packages/sympy/simplify/simplify.py:        # TODO: see if x*log(a)+x*log(a)*log(b) -> x*log(a)*(1+log(b))?\n./.venv/lib/python3.13/site-packages/sympy/simplify/simplify.py:    # TODO\n./.venv/lib/python3.13/site-packages/sympy/simplify/hyperexpand.py:#   TODO work this out in detail.\n./.venv/lib/python3.13/site-packages/sympy/simplify/hyperexpand.py:    # TODO see if this can work as Mod(x, 1); this will require\n./.venv/lib/python3.13/site-packages/sympy/simplify/hyperexpand.py:    # TODO branching\n./.venv/lib/python3.13/site-packages/sympy/simplify/hyperexpand.py:        # TODO with symbolic parameters, it could be advantageous\n./.venv/lib/python3.13/site-packages/sympy/simplify/hyperexpand.py:    # TODO tons of more formulae\n./.venv/lib/python3.13/site-packages/sympy/simplify/hyperexpand.py:    # TODO\n./.venv/lib/python3.13/site-packages/sympy/simplify/hyperexpand.py:    # TODO for now, we use the following simple heuristic: inverse-shift\n./.venv/lib/python3.13/site-packages/sympy/simplify/hyperexpand.py:    # TODO the following would be possible:\n./.venv/lib/python3.13/site-packages/sympy/simplify/hyperexpand.py:    # TODO Also, we tend to create combinations of gamma functions that can be\n./.venv/lib/python3.13/site-packages/sympy/simplify/hyperexpand.py:    # TODO it would be helpful to give conditions under which the integral\n./.venv/lib/python3.13/site-packages/sympy/simplify/tests/test_hyperexpand.py:    # TODO [a+1, aRational(-1, 2)], [2*a]\n./.venv/lib/python3.13/site-packages/sympy/simplify/tests/test_hyperexpand.py:    # TODO hyperexpand(hyper([a], [2*a + 1], z))\n./.venv/lib/python3.13/site-packages/sympy/simplify/tests/test_hyperexpand.py:    # TODO [S.Half, a], [Rational(3, 2), a+1]\n./.venv/lib/python3.13/site-packages/sympy/simplify/tests/test_hyperexpand.py:    # TODO [a], [a - S.Half, 2*a]\n./.venv/lib/python3.13/site-packages/sympy/simplify/tests/test_hyperexpand.py:    # TODO polys\n./.venv/lib/python3.13/site-packages/sympy/simplify/tests/test_hyperexpand.py:    # TODO LOTS more\n./.venv/lib/python3.13/site-packages/sympy/simplify/tests/test_hyperexpand.py:    # TODO LOTS more\n./.venv/lib/python3.13/site-packages/sympy/simplify/gammasimp.py:                    # TODO is there a better heuristic?\n./.venv/lib/python3.13/site-packages/sympy/parsing/latex/lark/transformer.py:            # TODO: ANTLR refers to ISO 80000-2:2019. should we keep base 10 or base 2?\n./.venv/lib/python3.13/site-packages/sympy/parsing/latex/lark/grammar/greek_symbols.lark:// TODO: Shouold we include the uppercase variants for the symbols where the uppercase variant doesn't have a separate meaning?\n./.venv/lib/python3.13/site-packages/sympy/parsing/latex/lark/grammar/greek_symbols.lark:DELTA: \"\\\\delta\" // TODO: Should this be included? Delta usually denotes other things.\n./.venv/lib/python3.13/site-packages/sympy/parsing/latex/lark/grammar/greek_symbols.lark:// TODO: Should I add iota to the list?\n./.venv/lib/python3.13/site-packages/sympy/parsing/latex/lark/grammar/greek_symbols.lark:LAMBDA: \"\\\\lambda\" // TODO: What about the uppercase variant?\n./.venv/lib/python3.13/site-packages/sympy/parsing/latex/lark/grammar/greek_symbols.lark:// TODO: Should there be a separate note for transforming \\pi into sympy.pi?\n./.venv/lib/python3.13/site-packages/sympy/parsing/latex/lark/grammar/greek_symbols.lark:// TODO: What should we do about sigma?\n./.venv/lib/python3.13/site-packages/sympy/parsing/fortran/fortran_parser.py:            # TODO: Arithmetic Assignment\n./.venv/lib/python3.13/site-packages/sympy/parsing/fortran/fortran_parser.py:            # TODO: Integer Binary Operations\n./.venv/lib/python3.13/site-packages/sympy/parsing/fortran/fortran_parser.py:            # TODO:Numbers when the LFortran ASR is updated\n./.venv/lib/python3.13/site-packages/sympy/parsing/fortran/fortran_parser.py:            # TODO: Return statement, variable declaration\n./.venv/lib/python3.13/site-packages/sympy/parsing/c/c_parser.py:            #TODO: No string type in AST\n./.venv/lib/python3.13/site-packages/sympy/parsing/autolev/_listener_autolev_antlr.py:            # TODO: Currently only works with symbols. Make it work for dynamicsymbols.\n./.venv/lib/python3.13/site-packages/sympy/parsing/autolev/_listener_autolev_antlr.py:            # TODO** Parse block matrices\n./.venv/lib/python3.13/site-packages/sympy/combinatorics/coset_table.py:    # TODO: complete the docstring\n./.venv/lib/python3.13/site-packages/sympy/combinatorics/fp_groups.py:        # TODO: use |G:H| = |G|/|H| (currently H can't be made into a group)\n./.venv/lib/python3.13/site-packages/sympy/combinatorics/fp_groups.py:    # TODO:: Sims points out in [Sim94] that performance can be improved by\n./.venv/lib/python3.13/site-packages/sympy/combinatorics/fp_groups.py:            # TODO: this should support input of a list of general words\n./.venv/lib/python3.13/site-packages/sympy/combinatorics/tensor_can.py:    TODO: use baseswap in the case in which if it fails in finding a\n./.venv/lib/python3.13/site-packages/tiktoken/core.py:        # TODO: assess correctness for errors=\"ignore\" and errors=\"replace\"\n./.venv/lib/python3.13/site-packages/tiktoken/model.py:# TODO: these will likely be replaced by an API endpoint\n./workspace/README.MD:        core_loop.py (orchestrator, mind loop skeleton, TODO markers for each step)\n", "stderr": "grep: ./.venv/lib/python3.13/site-packages/mpmath/matrices/__pycache__/calculus.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/mpmath/matrices/__pycache__/matrices.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/mpmath/functions/__pycache__/bessel.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/mpmath/calculus/__pycache__/odes.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/mpmath/calculus/__pycache__/extrapolation.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/mpmath/calculus/__pycache__/differentiation.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/mpmath/libmp/__pycache__/gammazeta.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/mpmath/libmp/__pycache__/libmpf.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/mpmath/libmp/__pycache__/libintmath.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/mpmath/libmp/__pycache__/libhyper.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/mpmath/tests/__pycache__/torture.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/pygments/filters/__pycache__/__init__.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/pygments/lexers/__pycache__/objective.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/pygments/lexers/__pycache__/openscad.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/pygments/lexers/__pycache__/markup.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/pygments/lexers/__pycache__/modula2.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/pygments/lexers/__pycache__/_asy_builtins.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/pygments/lexers/__pycache__/testing.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/pygments/lexers/__pycache__/inferno.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/pygments/__pycache__/lexer.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/opentelemetry/sdk/metrics/_internal/__pycache__/__init__.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/opentelemetry/sdk/trace/__pycache__/__init__.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/triton/_C/libtriton.so: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/joblib/__pycache__/pool.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/joblib/__pycache__/numpy_pickle_utils.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/joblib/__pycache__/compressor.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/huggingface_hub/commands/__pycache__/delete_cache.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/huggingface_hub/serialization/__pycache__/_torch.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/huggingface_hub/inference/__pycache__/_common.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/huggingface_hub/__pycache__/_local_folder.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/huggingface_hub/__pycache__/keras_mixin.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/huggingface_hub/utils/__pycache__/_deprecation.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/transformers/utils/__pycache__/auto_docstring.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/transformers/models/lightglue/__pycache__/image_processing_lightglue.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/transformers/models/llama4/__pycache__/configuration_llama4.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/transformers/models/xlnet/__pycache__/modeling_tf_xlnet.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/transformers/models/superpoint/__pycache__/image_processing_superpoint.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/transformers/models/deepseek_v3/__pycache__/modeling_deepseek_v3.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/transformers/models/deepseek_v3/__pycache__/modular_deepseek_v3.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/transformers/models/unispeech_sat/__pycache__/modular_unispeech_sat.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/transformers/models/unispeech_sat/__pycache__/modeling_unispeech_sat.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/transformers/models/efficientloftr/__pycache__/image_processing_efficientloftr.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/transformers/models/mllama/__pycache__/processing_mllama.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/transformers/models/superglue/__pycache__/image_processing_superglue.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/transformers/models/bloom/__pycache__/modeling_bloom.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/transformers/models/unispeech/__pycache__/modular_unispeech.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/transformers/models/unispeech/__pycache__/modeling_unispeech.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/kubernetes/stream/__pycache__/ws_client.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/numpy/_core/__pycache__/getlimits.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/numpy/_core/__pycache__/_add_newdocs.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/numpy/f2py/tests/__pycache__/test_f2py2e.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/numpy/f2py/__pycache__/crackfortran.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/numpy/f2py/__pycache__/cfuncs.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/setuptools/config/_validate_pyproject/__pycache__/fastjsonschema_validations.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/setuptools/command/__pycache__/egg_info.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/setuptools/__pycache__/_normalization.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/oauthlib/oauth2/rfc6749/__pycache__/utils.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/hf_xet/hf_xet.abi3.so: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/nvidia/cudnn/lib/libcudnn_engines_runtime_compiled.so.9: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/nvidia/cudnn/lib/libcudnn_engines_precompiled.so.9: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/nvidia/cusparselt/lib/libcusparseLt.so.0: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/nvidia/cublas/lib/libcublasLt.so.12: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/nvidia/nvjitlink/lib/libnvJitLink.so.12: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/nvidia/cuda_nvrtc/lib/libnvrtc.so.12: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/nvidia/cuda_nvrtc/lib/libnvrtc.alt.so.12: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/pydantic/_internal/__pycache__/_core_metadata.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/pydantic/_internal/__pycache__/_generate_schema.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/pydantic/__pycache__/json_schema.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/sklearn/ensemble/_hist_gradient_boosting/__pycache__/gradient_boosting.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/sklearn/linear_model/__pycache__/_base.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/sklearn/metrics/__pycache__/pairwise.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/sklearn/compose/__pycache__/_column_transformer.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/sklearn/svm/tests/__pycache__/test_svm.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/sklearn/model_selection/__pycache__/_validation.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/sklearn/__pycache__/pipeline.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/sklearn/utils/__pycache__/estimator_checks.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/torchgen/api/__pycache__/lazy.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/torchgen/dest/__pycache__/register_dispatch_key.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/torchgen/dest/__pycache__/lazy_ir.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/torchgen/__pycache__/gen_aoti_c_shim.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/torchgen/__pycache__/gen_lazy_tensor.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/sentence_transformers/backend/__pycache__/utils.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/sentence_transformers/__pycache__/SentenceTransformer.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/tqdm/__pycache__/cli.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/chromadb_rust_bindings/chromadb_rust_bindings.abi3.so: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/pydantic_core/__pycache__/core_schema.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/pip/_internal/req/__pycache__/req_set.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/pip/_vendor/pygments/filters/__pycache__/__init__.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/pip/_vendor/pygments/__pycache__/lexer.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/pip/_vendor/distlib/__pycache__/locators.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/pip/_vendor/distlib/__pycache__/version.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/torch/_decomp/__pycache__/decompositions.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/torch/ao/ns/fx/__pycache__/graph_passes.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/torch/ao/ns/__pycache__/_numeric_suite_fx.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/torch/ao/nn/sparse/quantized/__pycache__/linear.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/torch/ao/quantization/backend_config/__pycache__/tensorrt.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/__pycache__/convert.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/__pycache__/prepare.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/torch/ao/quantization/fx/__pycache__/utils.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/torch/ao/quantization/__pycache__/quantize_fx.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/torch/ao/quantization/__pycache__/observer.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/torch/ao/pruning/sparsifier/__pycache__/base_sparsifier.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/torch/ao/pruning/_experimental/activation_sparsifier/__pycache__/activation_sparsifier.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/torch/lib/libtorch_cuda.so: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/torch/lib/libtorch_cpu.so: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/torch/_export/serde/__pycache__/serialize.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/torch/_export/__pycache__/utils.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/torch/_inductor/runtime/__pycache__/coordinate_descent_tuner.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/cuda/__pycache__/cuda_kernel.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/__pycache__/cpp_gemm_template.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/__pycache__/cpp_flex_attention_template.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/__pycache__/cpp_micro_gemm.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/torch/_inductor/codegen/__pycache__/triton.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/torch/_inductor/kernel/__pycache__/flex_attention.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/torch/_inductor/__pycache__/__init__.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/torch/_inductor/__pycache__/ir.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/torch/_inductor/__pycache__/cudagraph_trees.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/torch/_inductor/__pycache__/virtualized.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/torch/_inductor/__pycache__/codecache.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/torch/_inductor/__pycache__/comm_analysis.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/torch/_inductor/__pycache__/config.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/torch/_inductor/__pycache__/comms.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/torch/_inductor/__pycache__/async_compile.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/torch/_inductor/fx_passes/__pycache__/post_grad.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/torch/_inductor/fx_passes/__pycache__/mkldnn_fusion.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/torch/export/__pycache__/_swap.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/torch/export/__pycache__/_trace.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/torch/_dynamo/variables/__pycache__/distributed.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/torch/_dynamo/__pycache__/decorators.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/torch/_dynamo/__pycache__/codegen.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/torch/_dynamo/__pycache__/cache_size.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/torch/_dynamo/__pycache__/convert_frame.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/torch/_dynamo/__pycache__/utils.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/torch/_dynamo/__pycache__/precompile_context.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/torch/onnx/_internal/fx/__pycache__/patcher.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/torch/onnx/_internal/fx/__pycache__/_pass.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/torch/onnx/__pycache__/symbolic_opset15.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/torch/_library/__pycache__/utils.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/torch/nn/attention/__pycache__/flex_attention.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/torch/_lazy/__pycache__/computation.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/torch/fx/experimental/migrate_gradual_types/__pycache__/constraint_transformation.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/torch/fx/experimental/__pycache__/symbolic_shapes.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/torch/distributed/_tensor/__pycache__/api.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/torch/distributed/_tensor/__pycache__/placement_types.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/torch/distributed/optim/__pycache__/named_optimizer.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/torch/distributed/pipelining/__pycache__/schedules.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/torch/distributed/pipelining/__pycache__/stage.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/torch/distributed/tensor/_ops/__pycache__/_tensor_ops.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/torch/distributed/tensor/__pycache__/_op_schema.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/torch/distributed/tensor/__pycache__/placement_types.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/torch/distributed/_symmetric_memory/__pycache__/__init__.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/torch/distributed/checkpoint/__pycache__/planner_helpers.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/torch/distributed/checkpoint/__pycache__/storage.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/torch/distributed/__pycache__/distributed_c10d.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/torch/testing/_internal/__pycache__/common_device_type.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/torch/testing/_internal/__pycache__/common_methods_invocations.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/torch/utils/hipify/__pycache__/hipify_python.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/torch/utils/data/datapipes/__pycache__/gen_pyi.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/torch/utils/_sympy/__pycache__/reference.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/torch/jit/__pycache__/_recursive.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/torch/jit/__pycache__/_monkeytype_config.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/scipy/interpolate/__pycache__/_fitpack_impl.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/scipy/_lib/array_api_extra/_lib/_utils/__pycache__/_helpers.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/scipy/_lib/pyprima/cobyla/__pycache__/geometry.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/scipy/signal/tests/__pycache__/test_signaltools.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/scipy/signal/tests/__pycache__/test_bsplines.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/scipy/signal/tests/__pycache__/test_windows.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/scipy/sparse/linalg/_isolve/tests/__pycache__/test_iterative.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/chromadb/test/property/__pycache__/test_add.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/onnxruntime/transformers/__pycache__/quantize_helper.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/onnxruntime/tools/ort_format_model/__pycache__/operator_type_usage_processors.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/onnxruntime/quantization/__pycache__/qdq_quantizer.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/onnxruntime/capi/onnxruntime_pybind11_state.cpython-313-x86_64-linux-gnu.so: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/onnxruntime/capi/libonnxruntime.so.1.22.1: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/sympy/matrices/__pycache__/determinant.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/sympy/matrices/__pycache__/matrixbase.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/sympy/matrices/__pycache__/matrices.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/sympy/functions/elementary/__pycache__/_trigonometric_special.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/sympy/core/tests/__pycache__/test_args.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/sympy/core/__pycache__/facts.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/sympy/physics/quantum/tests/__pycache__/test_printing.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/sympy/physics/quantum/__pycache__/shor.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/sympy/physics/quantum/__pycache__/represent.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/sympy/physics/quantum/__pycache__/trace.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/sympy/physics/quantum/__pycache__/operator.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/sympy/physics/quantum/__pycache__/operatorset.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/sympy/physics/quantum/__pycache__/cartesian.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/sympy/physics/quantum/__pycache__/density.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/sympy/sets/__pycache__/sets.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/sympy/tensor/__pycache__/indexed.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/sympy/utilities/_compilation/__pycache__/__init__.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/sympy/series/__pycache__/series_class.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/sympy/codegen/tests/__pycache__/test_rewriting.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/sympy/printing/pretty/__pycache__/stringpict.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/sympy/stats/__pycache__/crv_types.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/sympy/logic/algorithms/__pycache__/lra_theory.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/sympy/combinatorics/__pycache__/coset_table.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/sympy/combinatorics/__pycache__/fp_groups.cpython-313.pyc: binary file matches\ngrep: ./.venv/lib/python3.13/site-packages/sympy/combinatorics/__pycache__/tensor_can.cpython-313.pyc: binary file matches\n", "returncode": 0}}
